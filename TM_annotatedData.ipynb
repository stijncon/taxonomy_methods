{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b719fb5-c640-4dcc-a94e-f4971978e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c83738-40d5-4997-9ba1-51425014e46a",
   "metadata": {},
   "source": [
    "# Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4cf57f-77b8-419b-9afb-a48d85227add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of methods paragraphs: 44853\n",
      "total number of methods sentences: 641892\n"
     ]
    }
   ],
   "source": [
    "# open the paragraphs file if you don't want to run the above again\n",
    "\n",
    "filename = r\"C:\\Users\\conix\\Documents\\Corpus\\methods_corpus\\methods_paragraph_corpus.pickle\"\n",
    "with open(filename, \"rb\") as f:\n",
    "    methods_paras = pickle.load(f)\n",
    "\n",
    "\n",
    "filename = r\"C:\\Users\\conix\\Documents\\Corpus\\methods_sentencesrawtext.pickle\"        \n",
    "with open(filename, \"rb\") as f:\n",
    "    methods_sentences = pickle.load(f)\n",
    "\n",
    "print(f'total number of methods paragraphs: {len(methods_paras)}')\n",
    "print(f'total number of methods sentences: {len(methods_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8baad22-945a-4911-8f47-2eccee332843",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a183f5-0d86-40e8-b23f-47b26b812998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a list of the columns from the dictionary with the classification\n",
    "\n",
    "\n",
    "def extract_strings(d):\n",
    "    strings = []\n",
    "    if isinstance(d, dict):\n",
    "        for key, value in d.items():\n",
    "            strings.append(key)\n",
    "            strings.extend(extract_strings(value))\n",
    "    elif isinstance(d, list):\n",
    "        for item in d:\n",
    "            strings.extend(extract_strings(item))\n",
    "    elif isinstance(d, str):\n",
    "        strings.append(d)\n",
    "    return strings\n",
    "\n",
    "# read json file with the annotations\n",
    "\n",
    "\n",
    "def read_jsonfile(filepath):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding line: {line}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {filepath} was not found.\")\n",
    "    return data\n",
    "\n",
    "# Function to get the labels from the jsonl\n",
    "\n",
    "\n",
    "def extract_terms_after_colons(data):\n",
    "    terms = []\n",
    "    \n",
    "    def traverse_dict(d):\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):\n",
    "                traverse_dict(value)  # Recursively traverse if value is a dict\n",
    "            else:\n",
    "                # Extract the part after ':::'\n",
    "                if isinstance(value, str) and ':::' in value:\n",
    "                    terms.append(value.split(':::')[-1])\n",
    "    \n",
    "    traverse_dict(data)\n",
    "    return terms\n",
    "\n",
    "# Function to update the df for higher categories (in place)\n",
    "\n",
    "\n",
    "def recursive_update(df, classif):\n",
    "    # Go through each item in the classification\n",
    "    for key, value in classif.items():\n",
    "        if isinstance(value, list):\n",
    "            # First process subcategories (go deeper into the hierarchy)\n",
    "            for v in value:\n",
    "                if isinstance(v, dict):\n",
    "                    # Recursively update subcategories first\n",
    "                    recursive_update(df, v)\n",
    "            \n",
    "            # After processing subcategories, update the current category\n",
    "            sub_categories = [v if isinstance(v, str) else list(v.keys())[0] for v in value]\n",
    "            df.loc[df[sub_categories].eq(1).any(axis=1), key] = 1\n",
    "    \n",
    "    # Return the DataFrame to allow for chaining if needed\n",
    "    return df\n",
    "\n",
    "# Function to turn the jsonl into a complete df\n",
    "\n",
    "\n",
    "def json_to_df(data, classif):\n",
    "    # Get the columns from the classification\n",
    "    all_strings = extract_strings(classif)\n",
    "    extra_columns = [\"id\", \"displayed_text\"]\n",
    "    columns = all_strings + extra_columns\n",
    "\n",
    "    # Create a list to hold all rows\n",
    "    rows = []\n",
    "\n",
    "    # Loop over the data and create a new row for each entry\n",
    "    for i in data:\n",
    "        row = {}  # Dictionary for the current row\n",
    "\n",
    "        # Extract labels\n",
    "        labels = extract_terms_after_colons(i['label_annotations'])\n",
    "\n",
    "        # Set values for all columns in all_strings\n",
    "        for cat in all_strings:\n",
    "            row[cat] = 1 if cat in labels else 0\n",
    "\n",
    "        # Add extra columns\n",
    "        row[\"displayed_text\"] = i[\"displayed_text\"]\n",
    "        row['id'] = i['id']\n",
    "\n",
    "        # Append the row to the list of rows\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    # Update higher categories using recursive update (ensure in-place modification)\n",
    "    df = recursive_update(df, classif)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aadddbd-a347-4766-9f83-333f6ded983c",
   "metadata": {},
   "source": [
    "# Our classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e899171-13aa-4b02-8642-30713c5804d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d33b16-e925-4eb5-91e1-b503c970f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms and classification used for the first batch of annotations\n",
    "# for the updated terms, see classifbelow\n",
    "classif_old = {\n",
    "    \"Phenotype\": [\n",
    "        {\n",
    "            \"Phen datatypes\": [\n",
    "                {\"MORPH\": \n",
    "               [ \"quant morph\",\"Interbreeding_morph\",\n",
    "                {\n",
    "                    \"qual morph\": [\n",
    "                        \"color_pattern\", \n",
    "                        \"Shape\", \n",
    "                        \"Texture\", \n",
    "                        \"Ultrastructural\", \n",
    "                        \n",
    "                    ]\n",
    "                }]},\n",
    "                {\n",
    "                    \"BEHAV\": [\n",
    "                        \"Acoustic data\", \n",
    "                        \"feeding\", \n",
    "                        \"Mating behavior\"\n",
    "                    ]\n",
    "                },\n",
    "                \"ECOLOGY\"\n",
    "                            ]\n",
    "        },\n",
    "        {\n",
    "            \"Phen processing\": [\n",
    "                \"IMAGING\",\n",
    "                \"SAMPLING\",\n",
    "                \"STORAGE\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Phen analysis\": [\n",
    "                \"phen_regression\",\n",
    "                \"phen_pylo\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"Genotype\": [\n",
    "        {\n",
    "            \"genot datatypes\": [\n",
    "                \"Nuclear DNA\",\n",
    "                \"Organellar DNA\",\n",
    "                \"Transcriptomic data\",\n",
    "                \"Proteomic data\",\n",
    "                \"Microsatellites\",\n",
    "                \"Whole genomes\",\n",
    "                \"Exomes\",\n",
    "                \"Genome-wide studies/SNPs\",\n",
    "                \"Epigenetic data\",\n",
    "                \"eDNA\",\n",
    "                {\n",
    "                    \"BIOCHEM\": [\n",
    "                        \"Chemotax\", \n",
    "                        \"Cytotax\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"gen processing\": [\n",
    "                {\"SEQUENCING\": [\"gen1\", \n",
    "                \"gen2\", \n",
    "                \"gen3\",]},\n",
    "                \"other\"\n",
    "                \n",
    "                \n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"gen analysis\": [\n",
    "                {\n",
    "                    \"GEN_NON_PHYLO\": [\n",
    "                        \"Distance\", \n",
    "                        \"haplowebs\", \n",
    "                        \"Fixed alt character states\", \n",
    "                        \"Clustering\", \n",
    "                        \"fen_Interbreeding\"\n",
    "                    ]\n",
    "                },\n",
    "                 \"PHYLO_SD\",\n",
    "                    \n",
    "                {\n",
    "                    \"PHYLO_TREE\": [\n",
    "                        \"Distance_based\", \n",
    "                        \"Character_based\",\n",
    "                         \"Consensus_supertree\",\n",
    "                    ]\n",
    "                },\n",
    "               \n",
    "                \"Other\",\n",
    "                \"ML_methods\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"Singletons\": [\n",
    "        \"Interbreeding\", \n",
    "        \"spec justification\", \n",
    "        \"Phylogenetic\", \n",
    "        \"Specimen storage location\", \n",
    "        \"sampling location\", \n",
    "        \"abbreviations & terms\", \n",
    "        \"nomenclature & history\",\n",
    "        \"BIOGEO\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c49c7f7-9bf0-4cc4-ac01-56d8f446ea3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ce61ae-701e-426f-82d0-3a57cdbd95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = {\n",
    "    \"PHENOTYPE\": [\n",
    "        {\n",
    "            \"Phen_data\": [\n",
    "                {\"MORPH\": \n",
    "               [ \"quant_morph\",\"interbr_morph\",\n",
    "                {\n",
    "                    \"qual_morph\": [\n",
    "                        \"color_pattern\", \n",
    "                        \"shape\", \n",
    "                        \"texture\", \n",
    "                        \"ultrastruct\", \n",
    "                        \n",
    "                    ]\n",
    "                }]},\n",
    "                {\n",
    "                    \"BEHAV\": [\n",
    "                        \"acoustic\", \n",
    "                        \"feeding\", \n",
    "                        \"mating\"\n",
    "                    ]\n",
    "                },\n",
    "                \"ECOLOGY\"\n",
    "                            ]\n",
    "        },\n",
    "        {\n",
    "            \"Phen_proc\": [\n",
    "                \"IMAGING\",\n",
    "                \"SAMPLING\",\n",
    "                \"STORAGE\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Phen_analysis\": [\n",
    "                \"phen_nonphylo\",\n",
    "                \"phen_pylo\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"GENOTYPE\": [\n",
    "        {\n",
    "            \"Gen_data\": [\n",
    "                \"nuclear\",\n",
    "                \"organellar\",\n",
    "                \"transcriptomic\",\n",
    "                \"proteomic\",\n",
    "                \"tandem_repeats\",\n",
    "                \"whole_genomes\",\n",
    "                \"exomes\",\n",
    "                \"genome_wide\",\n",
    "                \"epigenetic\",\n",
    "                \"eDNA\",\n",
    "                {\n",
    "                    \"BIOCHEM\": [\n",
    "                        \"chemotax\", \n",
    "                        \"cytotax\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Gen_proc\": [\n",
    "                {\"SEQUENCING\": [\"gen1\", \n",
    "                \"gen2\", \n",
    "                \"gen3\"]},\n",
    "                \"genproc_other\"\n",
    "                \n",
    "                \n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Gen_analysis\": [\n",
    "                {\n",
    "                    \"GEN_NON_PHYLO\": [\n",
    "                        \"distance\", \n",
    "                        \"haplowebs\", \n",
    "                        \"fixed_alt_states\", \n",
    "                        \"clustering\", \n",
    "                        \"gen_interbr\"\n",
    "                    ]\n",
    "                },\n",
    "                 \"PHYLO_SD\",\n",
    "                    \n",
    "                {\n",
    "                    \"PHYLO_TREE\": [\n",
    "                        \"distance_based\", \n",
    "                        \"character_based\",\n",
    "                         \"consensus_supertree\",\n",
    "                    ]\n",
    "                },\n",
    "                \"MACHINE_LEARNING\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"Singletons\": [\n",
    "        \"interbreeding\", \n",
    "        \"rank_just\", \n",
    "        \"phylogenetic\", \n",
    "        \"specimen_storage_loc\", \n",
    "        \"sampling_loc\", \n",
    "        \"abbrev_terms\", \n",
    "        \"nomenclat_history\",\n",
    "        \"biogeo\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7046d0-84ba-4c6a-93a1-96cf8e009eb9",
   "metadata": {},
   "source": [
    "# Annotate Data\n",
    "\n",
    "We annotate in potato, using the immigration framing template with an adapted config file (implementing our classification)\n",
    "\n",
    "Category retention:\n",
    "- keep only those that after batch 1 have at least 10 instances (i.e. 2.5%). Dropped: 'gen_interbr', 'tandem_repeats', 'haplowebs', 'fixed_alt_states', 'ultrastruct', 'machine_learning', 'chemotax', 'gen2','texture', 'cytotax', 'clustering', 'biochem', 'proteomic', 'feeding','mating', 'transcriptomic', 'whole_genomes', 'exomes', 'genome_wide', 'epigenetic', 'edna', 'gen3', 'genproc_other', 'consensus_supertree'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d6957-1321-46aa-aca2-2173614b0825",
   "metadata": {},
   "source": [
    "## load newly annotated data from jsonl\n",
    "\n",
    "This process includes reading the data and making sure that higher level categories are \"1\" if their leaf categories are \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da331527-3bd0-4518-a44f-56dfe1e41f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first batch, 403 done by me of which 100 also by Marlies\n",
    "batch1 = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\firstTry\\corrected_dfx.csv\", sep = \";\")\n",
    "batch1_double = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\firstTry\\checked_samples.csv\", sep = \";\")\n",
    "\n",
    "# add a column to show whether it has been double-checked by Marlies\n",
    "batch1['checked'] = np.where(batch1['id'].isin(batch1_double['id'].values), 1, 0)\n",
    "batch1 = batch1.drop(columns = ['Unnamed: 0','Phylo_singlelocus', 'Phylo_multilocus','Other', 'Revisions', 'bad sample', 'gen alaysis' ])\n",
    "# add categroy that was added\n",
    "batch1['SEQUENCING'] = np.where(((batch1['gen1'] == 1) |(batch1['gen2'] == 1) |(batch1['gen3'] == 1)),1,0)\n",
    "\n",
    "# Marlies batch 2 (300)\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\annotated_instances_marlies2_stijn.jsonl\"\n",
    "filepath2 = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\annotated_instances_marlies2_marlies.jsonl\"\n",
    "data = read_jsonfile(filepath)\n",
    "batch2_m = json_to_df(data, classif)\n",
    "# batch2_m['checked'] = 0\n",
    "data2 = read_jsonfile(filepath2)\n",
    "batch2_m2 = json_to_df(data2, classif)\n",
    "\n",
    "## Laura batch 2 (300)\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\laura2\\annotated_instances_laura2_stijn.jsonl\"\n",
    "filepath_laura =  r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\laura2\\annotated_instances_laura2_laura.jsonl\"\n",
    "data = read_jsonfile(filepath_laura)\n",
    "batch2_l = json_to_df(data, classif)\n",
    "batch2_l['checked'] = 0\n",
    "data = read_jsonfile(filepath)\n",
    "batch2_ls = json_to_df(data, classif)\n",
    "batch2_ls['checked'] = 0\n",
    "\n",
    "## Stijn batch 2 (100)\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\stijn2\\annotated_instances_STIJN2.tsv\"\n",
    "df = pd.read_csv(filepath, sep = '\\t')\n",
    "\n",
    "terms = extract_strings(classif)\n",
    "df.columns = ['user', 'id', 'displayed_text'] + [i.split(\":::\")[1] for i in df.columns[3:]]\n",
    "\n",
    "\n",
    "missing = [j for j in extract_strings(classif) if j not in df.columns]\n",
    "df[missing] = np.nan\n",
    "df[terms] = df[terms].notna().astype(int)\n",
    "batch2_s = recursive_update(df, classif)\n",
    "batch2_s = batch2_s.drop(columns = 'user')\n",
    "batch2_s['checked'] = 0\n",
    "\n",
    "# active learning\n",
    "\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\fourthTry\\active_learning_annotated1.jsonl\"\n",
    "data = read_jsonfile(filepath)\n",
    "batch4_1 = json_to_df(data, classif)\n",
    "batch4_1['checked'] = 0\n",
    "\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\fourthTry\\active_learning_annotated2.jsonl\"\n",
    "data = read_jsonfile(filepath)\n",
    "batch4_2 = json_to_df(data, classif)\n",
    "batch4_2['checked'] = 0\n",
    "\n",
    "# laura batch 3\n",
    "\n",
    "filepath_laura =  r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\thirdTry\\laura\\annotated_instances_l3.jsonl\"\n",
    "data = read_jsonfile(filepath_laura)\n",
    "batch3_l = json_to_df(data, classif)\n",
    "batch3_l['checked'] = 0\n",
    "\n",
    "# targeted, domain knowledge based sampling\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\fifthTry\\targeted_samples.jsonl\"\n",
    "data = read_jsonfile(filepath)\n",
    "batch5 = json_to_df(data, classif)\n",
    "batch5['checked'] = 0\n",
    "\n",
    "# jhoe\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\jhoe2\\annotated_instances_jhoe.jsonl\"\n",
    "data = read_jsonfile(filepath)\n",
    "batch2_j = json_to_df(data, classif)\n",
    "batch2_j['checked'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b36e21-5d5c-4268-9e37-0d8a30177c46",
   "metadata": {},
   "source": [
    "## Check Brennan Prediger for batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5420a89-54a3-4d4b-90d1-91d8fed46962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont forget to map column names\n",
    "\n",
    "batch1 = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\firstTry\\corrected_dfx.csv\", sep = \";\")\n",
    "\n",
    "\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\firstTry\\annotated_instances_marlies.jsonl\"\n",
    "data = read_jsonfile(filepath)\n",
    "m = json_to_df(data, classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "964e3162-7d81-4e3f-86dc-26fd040ff36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_id = m.id.values\n",
    "\n",
    "overlap_s = batch1.loc[batch1.id.isin(m_id)]\n",
    "overlap_m = m.loc[m.id.isin(overlap_s.id.values)]\n",
    "cols = ['PHENOTYPE', 'Phen_data', 'MORPH', 'quant_morph', 'interbr_morph',\n",
    "       'qual_morph', 'color_pattern', 'shape', 'texture', 'ultrastruct',\n",
    "       'BEHAV', 'acoustic', 'feeding', 'mating', 'ECOLOGY', 'Phen_proc',\n",
    "       'IMAGING', 'SAMPLING', 'STORAGE', 'Phen_analysis', 'phen_nonphylo',\n",
    "       'phen_pylo', 'GENOTYPE', 'Gen_data', 'nuclear', 'organellar',\n",
    "       'transcriptomic', 'proteomic', 'tandem_repeats', 'whole_genomes',\n",
    "       'exomes', 'genome_wide', 'epigenetic', 'eDNA', 'BIOCHEM', 'chemotax',\n",
    "       'cytotax', 'Gen_proc',  'gen1', 'gen2', 'gen3',\n",
    "       'genproc_other', 'Gen_analysis', 'GEN_NON_PHYLO', 'distance',\n",
    "       'haplowebs', 'fixed_alt_states', 'clustering', 'gen_interbr',\n",
    "       'PHYLO_SD', 'PHYLO_TREE', 'distance_based', 'character_based',\n",
    "       'consensus_supertree', 'MACHINE_LEARNING', 'Singletons',\n",
    "       'interbreeding', 'rank_just', 'phylogenetic', 'specimen_storage_loc',\n",
    "       'sampling_loc', 'abbrev_terms', 'nomenclat_history', 'biogeo', 'id']\n",
    "      \n",
    "overlap_s = overlap_s[cols].set_index('id').sort_index()\n",
    "overlap_m = overlap_m[cols].set_index('id').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3230dd8a-78a0-48cf-b6d3-b899bdb3724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed agreement (p_o) = 0.8828125\n",
      "Brennan–Prediger’s Kappa = 0.765625\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "A = overlap_s.values\n",
    "B = overlap_m.values\n",
    "\n",
    "# 1) Compute p_o\n",
    "matches = np.sum(A == B)\n",
    "total_cells = A.size  # or A.shape[0] * A.shape[1]\n",
    "p_o = matches / total_cells\n",
    "\n",
    "# 2) For 2 categories, p_c = 1/2\n",
    "#    So Brennan–Prediger’s kappa = 2 * p_o - 1\n",
    "kappa_bp = 2 * p_o - 1\n",
    "\n",
    "print(\"Observed agreement (p_o) =\", p_o)\n",
    "print(\"Brennan–Prediger’s Kappa =\", kappa_bp)\n",
    "print(len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "189d2742-21bd-4089-b226-fc0ed4d5bf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed agreement (p_o) = 0.9714285714285714\n",
      "Brennan–Prediger’s Kappa = 0.9428571428571428\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "# Marlies batch 2 (300)\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\annotated_instances_marlies2_stijn.jsonl\"\n",
    "filepath2 = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\annotated_instances_marlies2_marlies.jsonl\"\n",
    "data = read_jsonfile(filepath)\n",
    "batch2_m = json_to_df(data, classif).set_index('id')\n",
    "# batch2_m['checked'] = 0\n",
    "data2 = read_jsonfile(filepath2)\n",
    "batch2_m2 = json_to_df(data2, classif).set_index('id')\n",
    "\n",
    "m_id = batch2_m2.index\n",
    "s = batch2_m.loc[batch2_m.index.isin(m_id)]\n",
    "m = batch2_m2.loc[batch2_m2.index.isin(s.index)]\n",
    "cols = [i for i in s.columns if i != 'displayed_text']\n",
    "s = s[cols].sort_index()\n",
    "m = m[cols].sort_index()\n",
    "\n",
    "\n",
    "A =s.values\n",
    "B = m.values\n",
    "\n",
    "# 1) Compute p_o\n",
    "matches = np.sum(A == B)\n",
    "total_cells = A.size  # or A.shape[0] * A.shape[1]\n",
    "p_o = matches / total_cells\n",
    "\n",
    "# 2) For 2 categories, p_c = 1/2\n",
    "#    So Brennan–Prediger’s kappa = 2 * p_o - 1\n",
    "kappa_bp = 2 * p_o - 1\n",
    "\n",
    "print(\"Observed agreement (p_o) =\", p_o)\n",
    "print(\"Brennan–Prediger’s Kappa =\", kappa_bp)\n",
    "print(len(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16790eb8-5d6b-42a2-9d34-57473f956265",
   "metadata": {},
   "source": [
    "## map diffferences\n",
    "\n",
    "We made small changes to the classification (i.e. drop categories) after initial annotations. As a part of that, we changed the names in the annotation interface. We map these here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b8df8dc-0b58-4b02-b2bc-6e100af58da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map classification 1 and 2, and straighten differences\n",
    "\n",
    "mapping_dct = {\n",
    "    'Phenotype': 'PHENOTYPE',\n",
    "    'Phen datatypes': 'Phen_data',\n",
    "    'MORPH': 'MORPH',\n",
    "    'quant morph': 'quant_morph',\n",
    "    'qual morph': 'qual_morph',\n",
    "    'color_pattern': 'color_pattern',\n",
    "    'Shape': 'shape',\n",
    "    'Texture': 'texture',\n",
    "    'Ultrastructural': 'ultrastruct',\n",
    "    'Interbreeding_morph': 'interbr_morph',\n",
    "    'BEHAV': 'BEHAV',\n",
    "    'Acoustic data': 'acoustic',\n",
    "    'feeding': 'feeding',\n",
    "    'Mating behavior': 'mating',\n",
    "    'ECOLOGY': 'ECOLOGY',\n",
    "    'Phen processing': 'Phen_proc',\n",
    "    'IMAGING': 'IMAGING',\n",
    "    'SAMPLING': 'SAMPLING',\n",
    "    'STORAGE': 'STORAGE',\n",
    "    'Phen analysis': 'Phen_analysis',\n",
    "    'phen_regression': 'phen_nonphylo',\n",
    "    'Phen_pylo': 'phen_pylo',\n",
    "    'Genotype': 'GENOTYPE',\n",
    "    'genot datatypes': 'Gen_data',\n",
    "    'Nuclear DNA': 'nuclear',\n",
    "    'Organellar DNA': 'organellar',\n",
    "    'Transcriptomic data': 'transcriptomic',\n",
    "    'Proteomic data': 'proteomic',\n",
    "    'Microsatellites': 'tandem_repeats',\n",
    "    'Whole genomes': 'whole_genomes',\n",
    "    'Exomes': 'exomes',\n",
    "    'Genome-wide studies/SNPs': 'genome_wide',\n",
    "    'Epigenetic data': 'epigenetic',\n",
    "    'eDNA': 'eDNA',\n",
    "    'BIOCHEM': 'BIOCHEM',\n",
    "    'Chemotax': 'chemotax',\n",
    "    'Cytotax': 'cytotax',\n",
    "    'gen processing': 'Gen_proc',\n",
    "    'SEQUENCING':'SEQUENCING',\n",
    "    'gen1': 'gen1',\n",
    "    'gen2': 'gen2',\n",
    "    'gen3': 'gen3',\n",
    "    'other': 'genproc_other',\n",
    "    'gen analysis': 'Gen_analysis',\n",
    "    'GEN_NON_PHYLO': 'GEN_NON_PHYLO',\n",
    "    'Distance': 'distance',\n",
    "    'haplowebs': 'haplowebs',\n",
    "    'Fixed alt character states': 'fixed_alt_states',\n",
    "    'Clustering': 'clustering',\n",
    "    'fen_Interbreeding': 'gen_interbr',\n",
    "    'PHYLO_SD': 'PHYLO_SD',\n",
    "    'PHYLO_TREE': 'PHYLO_TREE',\n",
    "    'Distance_based': 'distance_based',\n",
    "    'Character_based': 'character_based',\n",
    "    'Consensus_supertree': 'consensus_supertree',\n",
    "    \n",
    "    'ML_methods': 'MACHINE_LEARNING',\n",
    "    'Singletons': 'Singletons',\n",
    "    'Interbreeding': 'interbreeding',\n",
    "    'spec justification': 'rank_just',\n",
    "    'Phylogenetic': 'phylogenetic',\n",
    "    'Specimen storage location': 'specimen_storage_loc',\n",
    "    'sampling location': 'sampling_loc',\n",
    "    'abbreviations & terms': 'abbrev_terms',\n",
    "    'nomenclature & history': 'nomenclat_history',\n",
    "    'BIOGEO': 'biogeo',\n",
    "    'id': 'id',\n",
    "    'displayed_text': 'displayed_text',\n",
    "    'checked': 'checked'\n",
    "}\n",
    "\n",
    "batch1 = batch1.rename(columns = mapping_dct)\n",
    "batch1 = batch1.loc[:, ~batch1.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7044e62f-09c5-4c07-a8e9-9f94a5506cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch5.to_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\fifthTry\\batch5.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617b3632-4037-453f-ab76-2f8f11b94e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch2_j.to_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\jhoe2\\batch2_j.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08318f-df60-4eca-ba7a-e545e925df28",
   "metadata": {},
   "source": [
    "## make 1 file out of all the separate annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17868826-d444-42a3-8719-6e73bbb2f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate them\n",
    "\n",
    "batch1 =  pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\firstTry\\batch1_FullFinal.csv\", sep = \";\")\n",
    "batch2_m = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\marlies_batch2_checked.csv\", sep = \";\")\n",
    "batch2_l = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\laura2\\corrected_df_laura2_DONE.csv\", sep = \";\")\n",
    "batch3_l = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\thirdTry\\laura\\batch3_l.csv\", sep = \";\")\n",
    "batch2_s = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\stijn2\\stijn2.csv\", sep = \";\")\n",
    "batch2_j = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\jhoe2\\batch2_j.csv\", sep = \";\")\n",
    "batch4_1 = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\fourthTry\\batch4.csv\", sep = \";\")\n",
    "batch4_2 = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\fourthTry\\batch4_2.csv\", sep = \";\")\n",
    "batch5 = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\fifthTry\\batch5.csv\", sep = \";\")\n",
    "\n",
    "\n",
    "batch2_l['checked'] = 1\n",
    "\n",
    "batch1['batch'] = 'batch1'\n",
    "batch2_m['batch'] = 'batch2_m'\n",
    "batch2_s['batch'] = 'batch2_s'\n",
    "batch2_l['batch'] = 'batch2_l'\n",
    "batch2_j['batch'] = 'batch2_j'\n",
    "batch3_l['batch'] = 'batch3_l'\n",
    "batch4_1['batch'] = \"batch4_1\"\n",
    "batch4_2['batch'] = \"batch4_2\"\n",
    "batch5['batch'] = \"batch5\"\n",
    "\n",
    "dfs = [batch1, batch2_m, batch2_s, batch2_l, batch2_j, batch3_l, batch4_1, batch4_2,batch5]\n",
    "\n",
    "# for df in dfs:\n",
    "#     df = df.astype('object')  # Specify type conversion\n",
    "for idx, i in enumerate(dfs):\n",
    "    i.columns = [j.lower() for j in i.columns]\n",
    "    try:\n",
    "        dfs[idx] = i.drop(columns = ['unnamed: 0', 'terms_abbrev'], errors = 'ignore')\n",
    "    except KeyError as e:\n",
    "        print('column not there')\n",
    "        \n",
    "combined_df = pd.concat(dfs, axis=0, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "379d83b2-c22a-472e-be72-8315bdebc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistakes that came up in diagnosing gpt\n",
    "\n",
    "\n",
    "combined_df.loc[combined_df.ecology == 1].iloc[0]['ecology'] = 0\n",
    "\n",
    "mistakes_dct = {\n",
    "    \"./Corpus/Zootaxa/2/zootaxa_2496_1_3.json_0\": [\"rank_just\", 1],\n",
    "    \"./Corpus/Zootaxa/4/6/zootaxa_4619_2_6.json_0\": [\"rank_just\", 1],\n",
    "    \"./Corpus/Zootaxa/4/7/zootaxa_4751_2_11.json_1\": [\"rank_just\", 1],\n",
    "    \"./Corpus/Zootaxa/4/6/zootaxa_4651_2_6.json_0\": [\"rank_just\", 1],\n",
    "    \"./Corpus/Zootaxa/4/7/zootaxa_4731_4_1.json_2\": [\"rank_just\", 1],\n",
    "    \"./Corpus/Zootaxa/3/6/zootaxa_3681_4_1.json_3\": [\"rank_just\", 1],\n",
    "    \"./Corpus/Zootaxa/3/1/zootaxa_3162_1_3.json_1\": [\"rank_just\", 1],\n",
    "    \"./Corpus/Pensoft/journal_of_hymenoptera_research-25-035.json_0\": [\"rank_just\", 1],\n",
    "    \"./Corpus/Pensoft/zookeys-835-087.json_0\": [\"rank_just\", 1],\n",
    "    \"./Corpus/Zootaxa/4/2/zootaxa_4263_1_3.json_0\": [\"rank_just\", 1],\n",
    "    \"./Corpus/Pensoft/phytokeys-47-059.json_0\": [\"biogeo\", 1],\n",
    "    \"./Corpus/Zootaxa/4/0/zootaxa_4061_4_1.json_1\": [\"biogeo\", 1],\n",
    "    \"./Corpus/Zootaxa/4/5/zootaxa_4554_2_8.json_0\": [\"biogeo\", 1],\n",
    "    \"./Corpus/Zootaxa/4/1/zootaxa_4132_2_2.json_1\": [\"biogeo\", 0],\n",
    "    \"./Corpus/Zootaxa/4/4/zootaxa_4433_1_4.json_4\": [\"biogeo\", 0],\n",
    "    \"./Corpus/Zootaxa/1/zootaxa_1278_1_1.json_1\": [\"biogeo\", 0],\n",
    "    \"./Corpus/EJT/10_5852_ejt_2012_13.json_21\": [\"biogeo\", 0],\n",
    "    \"./Corpus/Pensoft/phytokeys-47-059.json_0\": [\"sampling\", 0],\n",
    "    \"./Corpus/Zootaxa/4/8/zootaxa_4809_3_2.json_1\": [\"color_pattern\", 0],\n",
    "    \"./Corpus/Pensoft/zookeys-913-089.json_0\": [\"color_pattern\", 1],\n",
    "    \"./Corpus/Pensoft/zookeys-315-055.json_0\": [\"sequencing\", 1],\n",
    "    \"./Corpus/Zootaxa/4/3/zootaxa_4303_4_8.json_1\": [\"sequencing\", 1],\n",
    "    \"./Corpus/Zootaxa/4/3/zootaxa_4312_3_3.json_0\": [\"sequencing\", 1],\n",
    "    \"./Corpus/Zootaxa/4/3/zootaxa_4374_4_5.json_2\": [\"sequencing\", 1],\n",
    "    \"./Corpus/Zootaxa/4/7/zootaxa_4751_2_11.json_1\": [\"sequencing\", 1],\n",
    "    \"./Corpus/Zootaxa/4/5/zootaxa_4543_2_8.json_0\": [\"sequencing\", 1],\n",
    "    \"./Corpus/Zootaxa/4/6/zootaxa_4656_3_11.json_1\": [\"sequencing\", 1],\n",
    "    \"./Corpus/Zootaxa/4/7/zootaxa_4766_3_2.json_2\": [\"sequencing\", 1],\n",
    "    \"./Corpus/Zootaxa/4/1/zootaxa_4147_4_9.json_0\": [\"phen_nonphylo\", 0],\n",
    "    \"./Corpus/Zootaxa/4/8/zootaxa_4822_4_4.json_1\": [\"phen_nonphylo\", 0],\n",
    "    \"./Corpus/Zootaxa/4/9/zootaxa_4949_3_1.json_6\": [\"phen_nonphylo\", 0],\n",
    "}\n",
    "\n",
    "for k, v in mistakes_dct.items():\n",
    "    combined_df.loc[(combined_df.id == k), v[0]] = v[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24a6f7-3c5f-4a94-a191-b650a8aca1fc",
   "metadata": {},
   "source": [
    "- all phylogenetic methods should have 'phylogenetic'\n",
    "- all haplotype should have 'genetic distance'\n",
    "- all interbr_morph should be interbeeding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db8d8122-0d2b-4404-bb4d-91e35eaeb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to complete labels\n",
    "\n",
    "combined_df.loc[(combined_df.interbr_morph == 1) | (combined_df.mating == 1) | (combined_df.gen_interbr == 1), 'interbreeding'] = 1\n",
    "combined_df.loc[(combined_df.haplowebs == 1) | (combined_df.distance_based == 1), 'distance' ] = 1\n",
    "combined_df.loc[(combined_df.phen_pylo == 1) | (combined_df.phylo_sd == 1) | (combined_df.phylo_tree == 1), 'phylogenetic'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dfeded0-ce6c-4359-b44f-aa34d88cdadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phenotype</th>\n",
       "      <th>phen_data</th>\n",
       "      <th>morph</th>\n",
       "      <th>quant_morph</th>\n",
       "      <th>qual_morph</th>\n",
       "      <th>color_pattern</th>\n",
       "      <th>shape</th>\n",
       "      <th>texture</th>\n",
       "      <th>ultrastruct</th>\n",
       "      <th>interbr_morph</th>\n",
       "      <th>...</th>\n",
       "      <th>specimen_storage_loc</th>\n",
       "      <th>sampling_loc</th>\n",
       "      <th>abbrev_terms</th>\n",
       "      <th>nomenclat_history</th>\n",
       "      <th>biogeo</th>\n",
       "      <th>id</th>\n",
       "      <th>displayed_text</th>\n",
       "      <th>checked</th>\n",
       "      <th>sequencing</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/EJT/10_5852_ejt_2021_735_1243.json_0</td>\n",
       "      <td>The material examined was collected in fragmen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/Zootaxa/1/zootaxa_1920_1_5.json_0</td>\n",
       "      <td>Invertebrate samples\\nwere collected using a h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/Zootaxa/4/7/zootaxa_4729_2_8.json_0</td>\n",
       "      <td>The nymphs were collected in the stream by han...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/Pensoft/phytokeys-47-059.json_0</td>\n",
       "      <td>We verified both the endemic status and the di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/Pensoft/zookeys-315-055.json_0</td>\n",
       "      <td>During each cruise, specimens were sorted onbo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>batch1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   phenotype  phen_data  morph  quant_morph  qual_morph  color_pattern  shape  \\\n",
       "0          1          0      0            0           0              0      0   \n",
       "1          1          0      0            0           0              0      0   \n",
       "2          1          0      0            0           0              0      0   \n",
       "3          1          0      0            0           0              0      0   \n",
       "4          1          0      0            0           0              0      0   \n",
       "\n",
       "   texture  ultrastruct  interbr_morph  ...  specimen_storage_loc  \\\n",
       "0        0            0              0  ...                     0   \n",
       "1        0            0              0  ...                     1   \n",
       "2        0            0              0  ...                     0   \n",
       "3        0            0              0  ...                     0   \n",
       "4        0            0              0  ...                     0   \n",
       "\n",
       "   sampling_loc  abbrev_terms  nomenclat_history  biogeo  \\\n",
       "0             1             1                  0       0   \n",
       "1             0             0                  0       0   \n",
       "2             0             0                  0       0   \n",
       "3             0             0                  0       0   \n",
       "4             0             0                  0       0   \n",
       "\n",
       "                                              id  \\\n",
       "0  ./Corpus/EJT/10_5852_ejt_2021_735_1243.json_0   \n",
       "1     ./Corpus/Zootaxa/1/zootaxa_1920_1_5.json_0   \n",
       "2   ./Corpus/Zootaxa/4/7/zootaxa_4729_2_8.json_0   \n",
       "3       ./Corpus/Pensoft/phytokeys-47-059.json_0   \n",
       "4        ./Corpus/Pensoft/zookeys-315-055.json_0   \n",
       "\n",
       "                                      displayed_text  checked  sequencing  \\\n",
       "0  The material examined was collected in fragmen...        0           0   \n",
       "1  Invertebrate samples\\nwere collected using a h...        0           0   \n",
       "2  The nymphs were collected in the stream by han...        0           0   \n",
       "3  We verified both the endemic status and the di...        0           0   \n",
       "4  During each cruise, specimens were sorted onbo...        1           1   \n",
       "\n",
       "    batch  \n",
       "0  batch1  \n",
       "1  batch1  \n",
       "2  batch1  \n",
       "3  batch1  \n",
       "4  batch1  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recursive updating using the hierarchy\n",
    "\n",
    "def lowercase_nested(data):\n",
    "    \"\"\"Recursively converts all strings in a nested structure (dict, list) to lowercase.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        # Recursively handle dictionaries\n",
    "        return {k.lower(): lowercase_nested(v) if isinstance(k, str) else k for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        # Recursively handle lists\n",
    "        return [lowercase_nested(item) for item in data]\n",
    "    elif isinstance(data, str):\n",
    "        # Convert strings to lowercase\n",
    "        return data.lower()\n",
    "    else:\n",
    "        # Leave other data types unchanged\n",
    "        return data\n",
    "\n",
    "\n",
    "lc_classif = lowercase_nested(classif)\n",
    "recursive_update(combined_df, lc_classif).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40e794c7-9a32-42c0-aa7f-62c4aa0bff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation_mistake\n",
      "1    0.702469\n",
      "0    0.297531\n",
      "Name: count, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ground truth</th>\n",
       "      <th>predicted</th>\n",
       "      <th>correct</th>\n",
       "      <th>label</th>\n",
       "      <th>old_label</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation_mistake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./Corpus/EJT/10_5852_ejt_2016_194.json_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>genetic and molecular features</td>\n",
       "      <td>genotype</td>\n",
       "      <td>European Journal of Taxonomy 194: 1â€“16 (2016...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./Corpus/EJT/10_5852_ejt_2016_194.json_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>phylogenetic tree reconstruction methods</td>\n",
       "      <td>phylo_tree</td>\n",
       "      <td>European Journal of Taxonomy 194: 1â€“16 (2016...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./Corpus/EJT/10_5852_ejt_2016_194.json_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>analysis of molecular data</td>\n",
       "      <td>gen_analysis</td>\n",
       "      <td>European Journal of Taxonomy 194: 1â€“16 (2016...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./Corpus/EJT/10_5852_ejt_2017_271.json_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sequencing</td>\n",
       "      <td>sequencing</td>\n",
       "      <td>The BCM criterion is similar to BM, but the qu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./Corpus/EJT/10_5852_ejt_2017_271.json_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>biogeographical methods</td>\n",
       "      <td>biogeo</td>\n",
       "      <td>The BCM criterion is similar to BM, but the qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  ground truth  predicted  correct  \\\n",
       "0  ./Corpus/EJT/10_5852_ejt_2016_194.json_1             0          1        0   \n",
       "1  ./Corpus/EJT/10_5852_ejt_2016_194.json_1             0          1        0   \n",
       "2  ./Corpus/EJT/10_5852_ejt_2016_194.json_1             0          1        0   \n",
       "3  ./Corpus/EJT/10_5852_ejt_2017_271.json_1             0          1        1   \n",
       "4  ./Corpus/EJT/10_5852_ejt_2017_271.json_1             1          0        1   \n",
       "\n",
       "                                      label     old_label  \\\n",
       "0            genetic and molecular features      genotype   \n",
       "1  phylogenetic tree reconstruction methods    phylo_tree   \n",
       "2                analysis of molecular data  gen_analysis   \n",
       "3                                sequencing    sequencing   \n",
       "4                   biogeographical methods        biogeo   \n",
       "\n",
       "                                                text  annotation_mistake  \n",
       "0  European Journal of Taxonomy 194: 1â€“16 (2016...                   0  \n",
       "1  European Journal of Taxonomy 194: 1â€“16 (2016...                   0  \n",
       "2  European Journal of Taxonomy 194: 1â€“16 (2016...                   0  \n",
       "3  The BCM criterion is similar to BM, but the qu...                   1  \n",
       "4  The BCM criterion is similar to BM, but the qu...                   0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mistakes selected by comparing regression and gpt, corrected\n",
    "# see TM_IdentifyAnnotationMistakes\n",
    "\n",
    "corrections = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\data\\overlapping_mistakes_correctedLauraStijn_csv_TEST_SET.csv\", sep = \";\")\n",
    "\n",
    "# check how many mistakes were due to annotator mistakes\n",
    "corrections['annotation_mistake'] = np.where(corrections['correct'] == corrections['predicted'], 1, 0)\n",
    "print(corrections['annotation_mistake'].value_counts() / len(corrections))\n",
    "\n",
    "\n",
    "corrections.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea91df9d-51d8-4b3a-80ff-04e3d1db7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fix the mistakes in combined\n",
    "\n",
    "for _, row in corrections.iterrows():\n",
    "    _id        = row['id']\n",
    "    col_to_fix = row['old_label']\n",
    "    new_value  = row['correct']\n",
    "    combined_df.loc[combined_df['id'] == _id, col_to_fix] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6159e239-baf2-409f-8400-13e0f04d2b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1845"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d9de872-e433-4b4e-af7b-1b06b97beae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file\n",
    "# this is the data we use for training the models\n",
    "\n",
    "combined_df.to_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\testData13052025_after_corrections.csv\")\n",
    "combined_df = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\testData13052025_after_corrections.csv\", index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd714b-ff3b-4e51-ab5b-588b831227cc",
   "metadata": {},
   "source": [
    "# Data from active learning\n",
    "\n",
    "Get training sampls for sparse categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c24d39c-8e26-474a-86b1-b9504245a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired = ['PHENOTYPE',\n",
    " 'Phen_data',\n",
    " 'MORPH',\n",
    " 'biogeo',\n",
    " 'color_pattern',\n",
    " 'Phen_proc',\n",
    " 'IMAGING',\n",
    " 'quant_morph',\n",
    " 'STORAGE',\n",
    " 'SAMPLING',\n",
    " 'GENOTYPE',\n",
    " 'interbr_morph',\n",
    " 'Gen_data',\n",
    " 'Gen_analysis',\n",
    " 'Gen_proc',\n",
    " 'SEQUENCING',\n",
    " 'gen1',\n",
    " 'organellar',\n",
    " 'PHYLO_TREE',\n",
    " 'Phen_analysis',\n",
    " 'character_based',\n",
    " 'phen_nonphylo',\n",
    " 'GEN_NON_PHYLO',\n",
    " 'ECOLOGY',\n",
    " 'distance',\n",
    " 'nuclear',\n",
    " 'BEHAV',\n",
    " 'phylogenetic',\n",
    " 'rank_just',\n",
    " 'phen_pylo',\n",
    " 'distance_based',\n",
    " 'acoustic',\n",
    " 'PHYLO_SD',\n",
    " 'interbreeding',\n",
    " ]\n",
    "\n",
    "desired = [i.lower() for i in desired]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "316a47d0-cff8-4f2d-a4b3-9d76b7c7be37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fea0192-6a88-4ed5-9a03-46cb8fce9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# original data\n",
    "df = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\testData20012025.csv\")\n",
    "df = df.drop(columns = 'Unnamed: 0')\n",
    "df.head(2)\n",
    "\n",
    "\n",
    "# preprocess\n",
    "\n",
    "# Extract features and labels\n",
    "X = df['displayed_text']  # Features (text field)\n",
    "y = df[desired]# df.iloc[:, :-4]  # Assuming last 3 columns are metadata, adjust as needed\n",
    "\n",
    "# Remove columns with only one class (all 0s or all 1s)\n",
    "non_constant_columns = [col for col in y.columns if y[col].nunique() > 1]\n",
    "y = y[non_constant_columns]\n",
    "\n",
    "# vectorize text\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=2808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ebbcdef-c2f8-4694-a647-12848cff1914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rank_just         47\n",
       "phen_pylo         42\n",
       "distance_based    27\n",
       "acoustic          29\n",
       "phylo_sd          23\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = combined_df[desired].sum() < 58\n",
    "combined_df[desired].sum()[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb7006a3-c01e-4522-8fbf-008668375dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rank_just', 'phen_pylo', 'distance_based', 'acoustic', 'phylo_sd'], dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[desired].sum()[mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5be6221-4194-43ee-a851-618b6893a27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rank_just', 'phen_pylo', 'distance_based', 'acoustic', 'phylo_sd'], dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse = combined_df[desired].sum()[mask].index\n",
    "sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89fa2c7f-cae9-4792-8ae0-d08388f00e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load a classifier\n",
    "import joblib\n",
    "\n",
    "base_filepath = r\"C:\\Users\\conix\\Documents\\Corpus\\classifier_models\"\n",
    "\n",
    "# Load the vectorizer\n",
    "vectorizer_filepath = os.path.join(base_filepath, \"vectorizer.pkl\")\n",
    "vectorizer = joblib.load(vectorizer_filepath)\n",
    "\n",
    "# Load each model dynamically based on label names\n",
    "loaded_models = {}\n",
    "for label in y.columns:  # Ensure this matches the original label set\n",
    "    model_filepath = os.path.join(base_filepath, f\"model_{label}.pkl\")\n",
    "    loaded_models[label] = joblib.load(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "180e48ed-47b3-41fa-aa11-e7347af280f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples_dct = {key:value for key,value in methods_paras.items() if key not in combined_df.id.values}\n",
    "new_models_dict = {key:value for key,value in loaded_models.items() if key in sparse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "740e713f-fec8-4453-a175-1ca4a3e9cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New samples\n",
    "import random\n",
    "\n",
    "new_sample_keys = random.sample(list(new_samples_dct.keys()), 5000)\n",
    "new_samples = [new_samples_dct[i] for i in new_sample_keys]\n",
    "\n",
    "\n",
    "# Preprocess using the vectorizer\n",
    "new_samples_vectorized = vectorizer.transform(new_samples)\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for label, model in new_models_dict.items():\n",
    "    predictions[label] = model.predict(new_samples_vectorized)\n",
    "    probabilities[label] = model.predict_proba(new_samples_vectorized) \n",
    "\n",
    "# Convert probabilities to a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "probabilities_df = pd.DataFrame({label: prob[:, 1] for label, prob in probabilities.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7c3ff21-dbf7-4f90-9f43-e05ace6579e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check where the model is most uncertain by looking at the difference in pron between the two classes\n",
    "# smaller is more uncertain\n",
    "\n",
    "uncertainty = probabilities_df.apply(lambda x: abs(x - 0.5), axis=1)\n",
    "most_uncertain_indices = uncertainty.sum(axis=1).nsmallest(n=10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "66d01157-2a80-4ce2-be13-e30a311a9f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "sample_indices = {}\n",
    "for i in uncertainty.columns:\n",
    "\n",
    "    #sample the most informative for each\n",
    "    n = 8\n",
    "    # get indices of n most uncertain samples\n",
    "    indices = uncertainty.loc[:,i].sort_values()[:n].index.values\n",
    "    sample_indices[i] = indices\n",
    "\n",
    "indices = list(set([item for sublist in sample_indices.values() for item in sublist]))\n",
    "\n",
    "for i in [(0.25,4), (0.17,3), (0.12,2), (0.1,2)]:\n",
    "    extra = list(uncertainty[(uncertainty <i[0]).sum(axis=1) >= i[1]].index.values)\n",
    "    indices.extend(extra)  \n",
    "\n",
    "indices = list(set(indices))\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "725dcc7b-f259-49ba-854f-547af7223c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataset to annotate\n",
    "\n",
    "samples_to_annotate = {new_sample_keys[i]:new_samples[i] for i in indices}\n",
    "\n",
    "\n",
    "output_file_path = r'C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\fourthTry\\active_learning_samples2.json'\n",
    "\n",
    "\n",
    "# Write the random key-value pairs to the output file\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for key, text in samples_to_annotate.items():\n",
    "        entry = {\n",
    "            \"id\": key,\n",
    "            \"text\": text,\n",
    "            \"annotations\": []\n",
    "        }\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ca422-da55-4ccb-901b-4462cc1dd5bc",
   "metadata": {},
   "source": [
    "# Data from domain knowledge searches (for diversity)\n",
    "\n",
    "Sparse categories: phylogenetic species delimitation (25),  distance based tree methods (33), acoustic data (38),  phylogenetic analysis methods for phenotypic data (54), rank_justification (56), behav (mating, feeding), nuclear genes (78), genetic distance (96).\n",
    "\n",
    "\n",
    "Categories that the classifier doesn't perform well for: \n",
    "\n",
    "- phen_nonphylo: bad recall because somewhat diverse -- it gets the common things (like PCA), but misses rarer things. This is a very heterogeneous category after all.\n",
    "- ecology: extremely heterogeneous, hard to solve by extra sampling\n",
    "- nuclear: **heterogeneous, but probably not so much, and sample is small. SO SAMPLE MORE.**\n",
    "- behav: heterogeneous because all behaviour apart from acoustic is rare. Sample some more feeding and mating because sample is low. SO SAMPLE MORE.\n",
    "- rank_just: very heterogeneous and very low sample. Try to sample more, but likely not very effective.\n",
    "- phen_pylo: **low sample, but terms often repeat so try SAMPLING MORE.**\n",
    "- distance_based: **sample is small but terms repeat so SAMPLE MORE.**\n",
    "- acoustic: **sample is small but terms repeat so SAMPLE MORE.**\n",
    "- phylo_sd: **sample is small but terms repeat so SAMPLE MORE.**\n",
    "- biogeo: very diverse, hard to solve by extra sampling.\n",
    "- color_pattern: very diverse, hard to solve by extra sampling.\n",
    "- sampling: very diverse, hard to solve by extra sampling.\n",
    "- phen_analysis: **sample is small but terms repeat so SAMPLE MORE.**\n",
    "\n",
    "We try targeted sampling for those, to improve the classifier. We don't use the active learning approach because we are scared that this will narrow the sample too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb937214-c7b9-47c6-8606-8a474f314491",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f7e4a2-a8dd-40ca-b7a8-85667c7c3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search terms\n",
    "\n",
    "search_dct = {\n",
    "    \"phylo_sd\": [            \n",
    "        r\"\\bGMYC\\b\",                             \n",
    "        r\"\\bGeneralized Mixed Yule Coalescent\\b\",\n",
    "        r\"\\bPTP\\b\",                              \n",
    "        r\"\\bPoisson Tree Processes\\b\",   \n",
    "        r\"\\bBPP\\b\",                             \n",
    "        r\"\\bBayesian Phylogenetics and Phylogeography\\b\",\n",
    "        r\"\\bcoalescent-based delimitation\\b\",                            \n",
    "        r\"\\bspecies tree(?:s)\\?b\",             \n",
    "        r\"\\bmultispecies coalescent\\b\",              \n",
    "    ],\n",
    "    \"distance_based\": [\n",
    "        r\"\\bdistance-based (?:phylogenetic )?tree inference\\b\", \n",
    "        r\"\\bdistance-based tree\\b\",                      \n",
    "        r\"\\bneighbor-joining\\b\",                       \n",
    "        r\"\\bUPGMA\\b\",                                   \n",
    "        r\"\\bgenetic distance tree\\b\",                     \n",
    "        r\"\\bFitch-Margoliash\\b\",   \n",
    "        r\"\\bNJ\\b\",             \n",
    "    ],\n",
    "    \"acoustic\": [\n",
    "        r\"\\bacoustic\\b\",\n",
    "        r\"\\bcall\\b\",\n",
    "        r\"\\bvocal(?:ization|ize|s|izing)?\\b\",\n",
    "        r\"\\bmicrophone\\b\",\n",
    "        r\"\\boscillogram(?:s)?\\b\",\n",
    "        r\"\\bspectral analys(?:is|es)?\\b\",\n",
    "        r\"\\bnote duration(?:s)?\\b\",\n",
    "        r\"\\bspectrogram(?:s)?\\b\",\n",
    "        r\"\\bfrequency analys(?:is|es)?\\b\",\n",
    "        r\"\\btimbre\\b\",\n",
    "        r\"\\bpitch\\b\",\n",
    "        r\"\\bamplitude\\b\",\n",
    "        r\"\\bwavelength\\b\",\n",
    "        r\"\\bdB(?: SPL)?\\b\",\n",
    "        r\"\\bsignal-to-noise ratio\\b\",\n",
    "        r\"\\bacoustic spectrum\\b\",\n",
    "        r\"\\brecorder(?:s)?\\b\",\n",
    "        r\"\\bsonograph(?:y|s)?\\b\",\n",
    "        r\"\\brhythmic\\b\",\n",
    "        r\"\\bchirp(?:s)?\\b\",\n",
    "        r\"\\btrill(?:s)?\\b\",\n",
    "        r\"\\bwhistle(?:s)?\\b\",\n",
    "        r\"\\bpulse(?:s)?\\b\",\n",
    "        r\"\\bclick(?:s)?\\b\",\n",
    "        r\"\\becolocation\\b\",\n",
    "        r\"\\bsonar\\b\"\n",
    "    ],\n",
    "    \"phen_pylo\": [\n",
    "        r\"\\bmatrix\\b\",                \n",
    "        r\"\\bcladistic(?:s)?\\b\",       \n",
    "        r\"\\bhomoplasy\\b\",             \n",
    "        r\"\\bautapomorph(?:y|ies)\\b\",  \n",
    "        r\"\\bplesiomorph(?:y|ies)\\b\",  \n",
    "        r\"\\bbremer\\b\",\n",
    "        r\"\\bretention\\b\",\n",
    "        r\"\\bmultistate\\b\",\n",
    "    ],\n",
    "    \"rank_just\": [\n",
    "        r\"\\bspecies concept\\b\",\n",
    "        r\"\\belevated to\\b\",\n",
    "        r\"\\new ranked\\b\",\n",
    "        r\"\\bcriteria\\b\",\n",
    "        r\"\\bsufficiently different\\b\",\n",
    "        r\"\\bspecies status\\b\",\n",
    "        r\"\\bshould be recognized as\\b\",\n",
    "        r\"\\bshould be recognised as\\b\",\n",
    "        r\"\\bphylogenetically distinct\\b\",\n",
    "        r\"\\bcombined evidence\\b\",\n",
    "        r\"\\bshould be ranked as\\b\",\n",
    "    \n",
    "    ],\n",
    "    \"nuclear\": [\n",
    "        r\"\\bnuclear\\b\",             \n",
    "        r\"\\b18s\\b\",         \n",
    "        r\"\\b28s\\b\",         \n",
    "        r\"\\brag1\\b\",                      \n",
    "        r\"\\brag2\\b\",                     \n",
    "        r\"\\bits(?:[ _]region)?\\b\",       \n",
    "        r\"\\bef1(?:-alpha)?\\b\",            \n",
    "        r\"\\bh3\\b\",                        \n",
    "        r\"\\bh4\\b\",                       \n",
    "        r\"\\btub(?:ulin)?\\b\",             \n",
    "        r\"\\bgapdh\\b\",                     \n",
    "        r\"\\brpb1\\b\",                      \n",
    "        r\"\\brpb2\\b\",                      \n",
    "        r\"\\brna polymerase\\b\",            \n",
    "    ],\n",
    "    \"phen_nonphylo\": [\n",
    "        r\"\\blandmark\\b\",\n",
    "        r\"\\bregression\\b\",\n",
    "        r\"\\bANOVA\\b\", \n",
    "        r\"\\bMANOVA\\b\",  \n",
    "        r\"\\bANCOVA\\b\",  \n",
    "        r\"\\bt-test\\b\",\n",
    "        r\"\\bpaired t-test\\b\",\n",
    "        r\"\\bwhitney\\b\",\n",
    "        r\"\\bwilcoxon\\b\",\n",
    "        r\"\\bchi-square\\b\",\n",
    "        r\"\\bKruskal-Wallis\\b\",\n",
    "        r\"\\bSpearman(?:'s)?\\b\",\n",
    "        r\"\\bPearson(?:'s)?\\b\",\n",
    "        r\"\\bmorphometric analys(?:is|es)\\b\",\n",
    "        r\"\\bgeometric morphometrics\\b\",\n",
    "        r\"\\bmorphospace\\b\"\n",
    "    ],\n",
    "    \"behav\": [\n",
    "        r\"\\bmating\\b\",\n",
    "        r\"\\bcourtship\\b\",\n",
    "        r\"\\bbreeding\\b\",\n",
    "        r\"\\bseasonal breeding\\b\",\n",
    "        r\"\\bmonogam(?:y|ous)\\b\",\n",
    "        r\"\\bmate guarding\\b\",\n",
    "        r\"\\bfeeding\\b\",\n",
    "        r\"\\bdiet\\b\",\n",
    "        r\"\\bforaging\\b\",\n",
    "        r\"\\bprey\\b\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "selection_dct = {'phylo_sd': 30, 'distance_based': 15, 'acoustic': 15, 'phen_pylo': 30, 'rank_just': 30, 'nuclear': 15, 'phen_nonphylo': 30, 'behav':20}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f0692-ca70-419c-951a-bd4c0ff4384e",
   "metadata": {},
   "source": [
    "## search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b49280-83bf-4fa7-b1af-5a5bfc91d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def select_matching_texts(search_dct, text_dct, selection_dct):\n",
    "    \"\"\"\n",
    "    Selects a specified number of randomly chosen text pieces per category from text_dct\n",
    "    based on regex patterns in search_dct.\n",
    "\n",
    "    Args:\n",
    "    - search_dct (dict): Dictionary mapping categories to lists of regex patterns.\n",
    "    - text_dct (dict): Dictionary mapping unique IDs to text pieces.\n",
    "    - selection_dct (dict): Dictionary mapping categories to the number of texts to select.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with unique text IDs as keys and text pieces as values.\n",
    "    \"\"\"\n",
    "    matched_texts = {key: [] for key in search_dct}  # Store matches per category\n",
    "    \n",
    "    # Iterate through text pieces and categorize them based on regex matches\n",
    "    for text_id, text in text_dct.items():\n",
    "        for category, patterns in search_dct.items():\n",
    "            if any(re.search(pattern, text, re.IGNORECASE) for pattern in patterns):\n",
    "                matched_texts[category].append((text_id, text))\n",
    "\n",
    "    # Select the requested number of random texts per category\n",
    "    selected_texts = {}\n",
    "    total_selected = 0\n",
    "\n",
    "    for category, num_to_select in selection_dct.items():\n",
    "        if category in matched_texts:\n",
    "            available_texts = matched_texts[category]\n",
    "            random.shuffle(available_texts)  # Shuffle for randomness\n",
    "            \n",
    "            # Select the required number of texts (or all if fewer than requested)\n",
    "            selected_pairs = available_texts[:num_to_select]\n",
    "            \n",
    "            # Store in final dictionary\n",
    "            for text_id, text in selected_pairs:\n",
    "                selected_texts[text_id] = text\n",
    "                total_selected += 1\n",
    "\n",
    "    return selected_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac0ab496-9b9a-446f-aeef-5aea62de5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples_dct = {key:value for key,value in methods_paras.items() if key not in combined_df.id.values}\n",
    "texts = select_matching_texts(search_dct, new_samples_dct, selection_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e1a709-f8a5-4c89-8433-4fec6db31974",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = r'C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\fourthTry\\diversity_sampling1.json'\n",
    "\n",
    "\n",
    "# Write the random key-value pairs to the output file\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for key, text in texts.items():\n",
    "        entry = {\n",
    "            \"id\": key,\n",
    "            \"text\": text,\n",
    "            \"annotations\": []\n",
    "        }\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
