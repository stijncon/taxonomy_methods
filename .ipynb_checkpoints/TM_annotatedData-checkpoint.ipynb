{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b719fb5-c640-4dcc-a94e-f4971978e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c83738-40d5-4997-9ba1-51425014e46a",
   "metadata": {},
   "source": [
    "# Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4cf57f-77b8-419b-9afb-a48d85227add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of methods paragraphs: 44853\n",
      "total number of methods sentences: 641892\n"
     ]
    }
   ],
   "source": [
    "# open the paragraphs file if you don't want to run the above again\n",
    "\n",
    "filename = r\"C:\\Users\\conix\\Documents\\Corpus\\methods_corpus\\methods_paragraph_corpus.pickle\"\n",
    "with open(filename, \"rb\") as f:\n",
    "    methods_paras = pickle.load(f)\n",
    "\n",
    "\n",
    "filename = r\"C:\\Users\\conix\\Documents\\Corpus\\methods_sentencesrawtext.pickle\"        \n",
    "with open(filename, \"rb\") as f:\n",
    "    methods_sentences = pickle.load(f)\n",
    "\n",
    "print(f'total number of methods paragraphs: {len(methods_paras)}')\n",
    "print(f'total number of methods sentences: {len(methods_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8baad22-945a-4911-8f47-2eccee332843",
   "metadata": {},
   "source": [
    "# Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a183f5-0d86-40e8-b23f-47b26b812998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a list of the columns from the dictionary with the classification\n",
    "\n",
    "\n",
    "def extract_strings(d):\n",
    "    strings = []\n",
    "    if isinstance(d, dict):\n",
    "        for key, value in d.items():\n",
    "            strings.append(key)\n",
    "            strings.extend(extract_strings(value))\n",
    "    elif isinstance(d, list):\n",
    "        for item in d:\n",
    "            strings.extend(extract_strings(item))\n",
    "    elif isinstance(d, str):\n",
    "        strings.append(d)\n",
    "    return strings\n",
    "\n",
    "# read json file with the annotations\n",
    "\n",
    "\n",
    "def read_jsonfile(filepath):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding line: {line}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {filepath} was not found.\")\n",
    "    return data\n",
    "\n",
    "# Function to get the labels from the jsonl\n",
    "\n",
    "\n",
    "def extract_terms_after_colons(data):\n",
    "    terms = []\n",
    "    \n",
    "    def traverse_dict(d):\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):\n",
    "                traverse_dict(value)  # Recursively traverse if value is a dict\n",
    "            else:\n",
    "                # Extract the part after ':::'\n",
    "                if isinstance(value, str) and ':::' in value:\n",
    "                    terms.append(value.split(':::')[-1])\n",
    "    \n",
    "    traverse_dict(data)\n",
    "    return terms\n",
    "\n",
    "# Function to update the df for higher categories (in place)\n",
    "\n",
    "\n",
    "def recursive_update(df, classif):\n",
    "    # Go through each item in the classification\n",
    "    for key, value in classif.items():\n",
    "        if isinstance(value, list):\n",
    "            # First process subcategories (go deeper into the hierarchy)\n",
    "            for v in value:\n",
    "                if isinstance(v, dict):\n",
    "                    # Recursively update subcategories first\n",
    "                    recursive_update(df, v)\n",
    "            \n",
    "            # After processing subcategories, update the current category\n",
    "            sub_categories = [v if isinstance(v, str) else list(v.keys())[0] for v in value]\n",
    "            df.loc[df[sub_categories].eq(1).any(axis=1), key] = 1\n",
    "    \n",
    "    # Return the DataFrame to allow for chaining if needed\n",
    "    return df\n",
    "\n",
    "# Function to turn the jsonl into a complete df\n",
    "\n",
    "\n",
    "def json_to_df(data, classif):\n",
    "    # Get the columns from the classification\n",
    "    all_strings = extract_strings(classif)\n",
    "    extra_columns = [\"id\", \"displayed_text\"]\n",
    "    columns = all_strings + extra_columns\n",
    "\n",
    "    # Create a list to hold all rows\n",
    "    rows = []\n",
    "\n",
    "    # Loop over the data and create a new row for each entry\n",
    "    for i in data:\n",
    "        row = {}  # Dictionary for the current row\n",
    "\n",
    "        # Extract labels\n",
    "        labels = extract_terms_after_colons(i['label_annotations'])\n",
    "\n",
    "        # Set values for all columns in all_strings\n",
    "        for cat in all_strings:\n",
    "            row[cat] = 1 if cat in labels else 0\n",
    "\n",
    "        # Add extra columns\n",
    "        row[\"displayed_text\"] = i[\"displayed_text\"]\n",
    "        row['id'] = i['id']\n",
    "\n",
    "        # Append the row to the list of rows\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    # Update higher categories using recursive update (ensure in-place modification)\n",
    "    df = recursive_update(df, classif)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aadddbd-a347-4766-9f83-333f6ded983c",
   "metadata": {},
   "source": [
    "# Our classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e899171-13aa-4b02-8642-30713c5804d9",
   "metadata": {},
   "source": [
    "## Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d33b16-e925-4eb5-91e1-b503c970f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms and classification used for the first batch of annotations\n",
    "# for the updated terms, see classifbelow\n",
    "classif_old = {\n",
    "    \"Phenotype\": [\n",
    "        {\n",
    "            \"Phen datatypes\": [\n",
    "                {\"MORPH\": \n",
    "               [ \"quant morph\",\"Interbreeding_morph\",\n",
    "                {\n",
    "                    \"qual morph\": [\n",
    "                        \"color_pattern\", \n",
    "                        \"Shape\", \n",
    "                        \"Texture\", \n",
    "                        \"Ultrastructural\", \n",
    "                        \n",
    "                    ]\n",
    "                }]},\n",
    "                {\n",
    "                    \"BEHAV\": [\n",
    "                        \"Acoustic data\", \n",
    "                        \"feeding\", \n",
    "                        \"Mating behavior\"\n",
    "                    ]\n",
    "                },\n",
    "                \"ECOLOGY\"\n",
    "                            ]\n",
    "        },\n",
    "        {\n",
    "            \"Phen processing\": [\n",
    "                \"IMAGING\",\n",
    "                \"SAMPLING\",\n",
    "                \"STORAGE\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Phen analysis\": [\n",
    "                \"phen_regression\",\n",
    "                \"phen_pylo\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"Genotype\": [\n",
    "        {\n",
    "            \"genot datatypes\": [\n",
    "                \"Nuclear DNA\",\n",
    "                \"Organellar DNA\",\n",
    "                \"Transcriptomic data\",\n",
    "                \"Proteomic data\",\n",
    "                \"Microsatellites\",\n",
    "                \"Whole genomes\",\n",
    "                \"Exomes\",\n",
    "                \"Genome-wide studies/SNPs\",\n",
    "                \"Epigenetic data\",\n",
    "                \"eDNA\",\n",
    "                {\n",
    "                    \"BIOCHEM\": [\n",
    "                        \"Chemotax\", \n",
    "                        \"Cytotax\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"gen processing\": [\n",
    "                {\"SEQUENCING\": [\"gen1\", \n",
    "                \"gen2\", \n",
    "                \"gen3\",]},\n",
    "                \"other\"\n",
    "                \n",
    "                \n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"gen analysis\": [\n",
    "                {\n",
    "                    \"GEN_NON_PHYLO\": [\n",
    "                        \"Distance\", \n",
    "                        \"haplowebs\", \n",
    "                        \"Fixed alt character states\", \n",
    "                        \"Clustering\", \n",
    "                        \"fen_Interbreeding\"\n",
    "                    ]\n",
    "                },\n",
    "                 \"PHYLO_SD\",\n",
    "                    \n",
    "                {\n",
    "                    \"PHYLO_TREE\": [\n",
    "                        \"Distance_based\", \n",
    "                        \"Character_based\",\n",
    "                         \"Consensus_supertree\",\n",
    "                    ]\n",
    "                },\n",
    "               \n",
    "                \"Other\",\n",
    "                \"ML_methods\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"Singletons\": [\n",
    "        \"Interbreeding\", \n",
    "        \"spec justification\", \n",
    "        \"Phylogenetic\", \n",
    "        \"Specimen storage location\", \n",
    "        \"sampling location\", \n",
    "        \"abbreviations & terms\", \n",
    "        \"nomenclature & history\",\n",
    "        \"BIOGEO\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c49c7f7-9bf0-4cc4-ac01-56d8f446ea3a",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ce61ae-701e-426f-82d0-3a57cdbd95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = {\n",
    "    \"PHENOTYPE\": [\n",
    "        {\n",
    "            \"Phen_data\": [\n",
    "                {\"MORPH\": \n",
    "               [ \"quant_morph\",\"interbr_morph\",\n",
    "                {\n",
    "                    \"qual_morph\": [\n",
    "                        \"color_pattern\", \n",
    "                        \"shape\", \n",
    "                        \"texture\", \n",
    "                        \"ultrastruct\", \n",
    "                        \n",
    "                    ]\n",
    "                }]},\n",
    "                {\n",
    "                    \"BEHAV\": [\n",
    "                        \"acoustic\", \n",
    "                        \"feeding\", \n",
    "                        \"mating\"\n",
    "                    ]\n",
    "                },\n",
    "                \"ECOLOGY\"\n",
    "                            ]\n",
    "        },\n",
    "        {\n",
    "            \"Phen_proc\": [\n",
    "                \"IMAGING\",\n",
    "                \"SAMPLING\",\n",
    "                \"STORAGE\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Phen_analysis\": [\n",
    "                \"phen_nonphylo\",\n",
    "                \"phen_pylo\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"GENOTYPE\": [\n",
    "        {\n",
    "            \"Gen_data\": [\n",
    "                \"nuclear\",\n",
    "                \"organellar\",\n",
    "                \"transcriptomic\",\n",
    "                \"proteomic\",\n",
    "                \"tandem_repeats\",\n",
    "                \"whole_genomes\",\n",
    "                \"exomes\",\n",
    "                \"genome_wide\",\n",
    "                \"epigenetic\",\n",
    "                \"eDNA\",\n",
    "                {\n",
    "                    \"BIOCHEM\": [\n",
    "                        \"chemotax\", \n",
    "                        \"cytotax\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Gen_proc\": [\n",
    "                {\"SEQUENCING\": [\"gen1\", \n",
    "                \"gen2\", \n",
    "                \"gen3\"]},\n",
    "                \"genproc_other\"\n",
    "                \n",
    "                \n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Gen_analysis\": [\n",
    "                {\n",
    "                    \"GEN_NON_PHYLO\": [\n",
    "                        \"distance\", \n",
    "                        \"haplowebs\", \n",
    "                        \"fixed_alt_states\", \n",
    "                        \"clustering\", \n",
    "                        \"gen_interbr\"\n",
    "                    ]\n",
    "                },\n",
    "                 \"PHYLO_SD\",\n",
    "                    \n",
    "                {\n",
    "                    \"PHYLO_TREE\": [\n",
    "                        \"distance_based\", \n",
    "                        \"character_based\",\n",
    "                         \"consensus_supertree\",\n",
    "                    ]\n",
    "                },\n",
    "                \"MACHINE_LEARNING\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"Singletons\": [\n",
    "        \"interbreeding\", \n",
    "        \"rank_just\", \n",
    "        \"phylogenetic\", \n",
    "        \"specimen_storage_loc\", \n",
    "        \"sampling_loc\", \n",
    "        \"abbrev_terms\", \n",
    "        \"nomenclat_history\",\n",
    "        \"biogeo\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7046d0-84ba-4c6a-93a1-96cf8e009eb9",
   "metadata": {},
   "source": [
    "# Annotate Data\n",
    "\n",
    "We annotate in potato, using the immigration framing template with an adapted config file (implementing our classification)\n",
    "\n",
    "To Do:\n",
    "\n",
    "- check whether I did not misinterpret wing venation as color and pattern\n",
    "- check ultrastructural data: SEM is not enough\n",
    "- check habitus: is not enough for qualitative morph\n",
    "\n",
    "What do we have now (17/01)\n",
    "\n",
    "- 403 annotated by stijn (corrected_dfx), of which 100 also rated by Marlies and compared with Stijn's (checked_samples)\n",
    "- 300 annotated by Stijn (annotated_instances_marlies2_stijn.jsonl), of which 100 also annotated by Marlies (annotated_instances_marlies2_marlies). Note: the 7 samples without any categories still need to be added somehow (if no categories, the output file doesnt have them). Comparison of the 100 shared ones is done (corrected_df_marlies2_DONE), and the entire batch 2 for Marlies can be found in marlies_batch2_checked.\n",
    "- 300 annotated instances by Laura and Stijn (annotated_instances_laura2_stijn.jsonl and annotated_instances_laura2_laura.jsonl). Samples without codes still need to be dealt with. The annotations have been compared and finalized (corrected_df_laura2_DONE).\n",
    "- 100 samples annotated by Stijn (annotated_instances_STIJN2.tsv)\n",
    "\n",
    "TO DO:\n",
    "- Joe samples\n",
    "- Compare Joe samples\n",
    "- Laura extra + compare\n",
    "- compare those 100 by Marlies\n",
    "\n",
    "Still need to do for all:\n",
    "\n",
    "- all phylogenetic methods should have 'phylogenetic'\n",
    "- all haplotype should have 'genetic distance'\n",
    "- all interbr_morph should be interbeeding\n",
    "- Make sure I do the hierarchy updating again at the end for all the ones that have been corrected\n",
    "- Does acoustic data always assume nonphylo analysis?\n",
    "- Does gen_analysis imply gen_data?\n",
    "- does SPEC_DELIMITATION always involve phylogenetic tree inference? (or at least, if there is GMYC)?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d6957-1321-46aa-aca2-2173614b0825",
   "metadata": {},
   "source": [
    "## load newly annotated data from jsonl\n",
    "\n",
    "This process includes reading the data and making sure that higher level categories are \"1\" if their leaf categories are \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da331527-3bd0-4518-a44f-56dfe1e41f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first batch, 403 done by me of which 100 also by Marlies\n",
    "batch1 = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\firstTry\\corrected_dfx.csv\", sep = \";\")\n",
    "batch1_double = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\firstTry\\checked_samples.csv\", sep = \";\")\n",
    "\n",
    "# add a column to show whether it has been double-checked by Marlies\n",
    "batch1['checked'] = np.where(batch1['id'].isin(batch1_double['id'].values), 1, 0)\n",
    "batch1 = batch1.drop(columns = ['Unnamed: 0','Phylo_singlelocus', 'Phylo_multilocus','Other', 'Revisions', 'bad sample', 'gen alaysis' ])\n",
    "# add categroy that was added\n",
    "batch1['SEQUENCING'] = np.where(((batch1['gen1'] == 1) |(batch1['gen2'] == 1) |(batch1['gen3'] == 1)),1,0)\n",
    "\n",
    "# Marlies batch 2 (300)\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\annotated_instances_marlies2_stijn.jsonl\"\n",
    "filepath2 = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\annotated_instances_marlies2_marlies.jsonl\"\n",
    "data = read_jsonfile(filepath)\n",
    "batch2_m = json_to_df(data, classif)\n",
    "# batch2_m['checked'] = 0\n",
    "data2 = read_jsonfile(filepath2)\n",
    "batch2_m2 = json_to_df(data2, classif)\n",
    "\n",
    "## Laura batch 2 (300)\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\laura2\\annotated_instances_laura2_stijn.jsonl\"\n",
    "filepath_laura =  r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\laura2\\annotated_instances_laura2_laura.jsonl\"\n",
    "data = read_jsonfile(filepath_laura)\n",
    "batch2_l = json_to_df(data, classif)\n",
    "batch2_l['checked'] = 0\n",
    "data = read_jsonfile(filepath)\n",
    "batch2_ls = json_to_df(data, classif)\n",
    "batch2_ls['checked'] = 0\n",
    "## Stijn batch 2 (100)\n",
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\stijn2\\annotated_instances_STIJN2.tsv\"\n",
    "df = pd.read_csv(filepath, sep = '\\t')\n",
    "\n",
    "terms = extract_strings(classif)\n",
    "df.columns = ['user', 'id', 'displayed_text'] + [i.split(\":::\")[1] for i in df.columns[3:]]\n",
    "\n",
    "\n",
    "missing = [j for j in extract_strings(classif) if j not in df.columns]\n",
    "df[missing] = np.nan\n",
    "df[terms] = df[terms].notna().astype(int)\n",
    "batch2_s = recursive_update(df, classif)\n",
    "batch2_s = batch2_s.drop(columns = 'user')\n",
    "batch2_s['checked'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "377b47a6-fbdd-421f-84e8-181c735a54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\stijn2\\annotated_instances_STIJN2.tsv\"\n",
    "df = pd.read_csv(filepath, sep = '\\t')\n",
    "\n",
    "terms = extract_strings(classif)\n",
    "df.columns = ['user', 'id', 'displayed_text'] + [i.split(\":::\")[1] for i in df.columns[3:]]\n",
    "\n",
    "\n",
    "missing = [j for j in extract_strings(classif) if j not in df.columns]\n",
    "df[missing] = np.nan\n",
    "df[terms] = df[terms].notna().astype(int)\n",
    "batch2_s = recursive_update(df, classif)\n",
    "batch2_s = batch2_s.drop(columns = 'user')\n",
    "batch2_s['checked'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aab73fac-ed2c-4433-9398-649ae4c177e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 74)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1_double.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c67929-3ca5-4d31-966b-b40141f6efba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16790eb8-5d6b-42a2-9d34-57473f956265",
   "metadata": {},
   "source": [
    "## map diffferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5b8df8dc-0b58-4b02-b2bc-6e100af58da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map classification 1 and 2, and straighten differences\n",
    "\n",
    "mapping_dct = {\n",
    "    'Phenotype': 'PHENOTYPE',\n",
    "    'Phen datatypes': 'Phen_data',\n",
    "    'MORPH': 'MORPH',\n",
    "    'quant morph': 'quant_morph',\n",
    "    'qual morph': 'qual_morph',\n",
    "    'color_pattern': 'color_pattern',\n",
    "    'Shape': 'shape',\n",
    "    'Texture': 'texture',\n",
    "    'Ultrastructural': 'ultrastruct',\n",
    "    'Interbreeding_morph': 'interbr_morph',\n",
    "    'BEHAV': 'BEHAV',\n",
    "    'Acoustic data': 'acoustic',\n",
    "    'feeding': 'feeding',\n",
    "    'Mating behavior': 'mating',\n",
    "    'ECOLOGY': 'ECOLOGY',\n",
    "    'Phen processing': 'Phen_proc',\n",
    "    'IMAGING': 'IMAGING',\n",
    "    'SAMPLING': 'SAMPLING',\n",
    "    'STORAGE': 'STORAGE',\n",
    "    'Phen analysis': 'Phen_analysis',\n",
    "    'phen_regression': 'phen_nonphylo',\n",
    "    'Phen_pylo': 'phen_pylo',\n",
    "    'Genotype': 'GENOTYPE',\n",
    "    'genot datatypes': 'Gen_data',\n",
    "    'Nuclear DNA': 'nuclear',\n",
    "    'Organellar DNA': 'organellar',\n",
    "    'Transcriptomic data': 'transcriptomic',\n",
    "    'Proteomic data': 'proteomic',\n",
    "    'Microsatellites': 'tandem_repeats',\n",
    "    'Whole genomes': 'whole_genomes',\n",
    "    'Exomes': 'exomes',\n",
    "    'Genome-wide studies/SNPs': 'genome_wide',\n",
    "    'Epigenetic data': 'epigenetic',\n",
    "    'eDNA': 'eDNA',\n",
    "    'BIOCHEM': 'BIOCHEM',\n",
    "    'Chemotax': 'chemotax',\n",
    "    'Cytotax': 'cytotax',\n",
    "    'gen processing': 'Gen_proc',\n",
    "    'SEQUENCING':'SEQUENCING',\n",
    "    'gen1': 'gen1',\n",
    "    'gen2': 'gen2',\n",
    "    'gen3': 'gen3',\n",
    "    'other': 'genproc_other',\n",
    "    'gen analysis': 'Gen_analysis',\n",
    "    'GEN_NON_PHYLO': 'GEN_NON_PHYLO',\n",
    "    'Distance': 'distance',\n",
    "    'haplowebs': 'haplowebs',\n",
    "    'Fixed alt character states': 'fixed_alt_states',\n",
    "    'Clustering': 'clustering',\n",
    "    'fen_Interbreeding': 'gen_interbr',\n",
    "    'PHYLO_SD': 'PHYLO_SD',\n",
    "    'PHYLO_TREE': 'PHYLO_TREE',\n",
    "    'Distance_based': 'distance_based',\n",
    "    'Character_based': 'character_based',\n",
    "    'Consensus_supertree': 'consensus_supertree',\n",
    "    \n",
    "    'ML_methods': 'MACHINE_LEARNING',\n",
    "    'Singletons': 'Singletons',\n",
    "    'Interbreeding': 'interbreeding',\n",
    "    'spec justification': 'rank_just',\n",
    "    'Phylogenetic': 'phylogenetic',\n",
    "    'Specimen storage location': 'specimen_storage_loc',\n",
    "    'sampling location': 'sampling_loc',\n",
    "    'abbreviations & terms': 'abbrev_terms',\n",
    "    'nomenclature & history': 'nomenclat_history',\n",
    "    'BIOGEO': 'biogeo',\n",
    "    'id': 'id',\n",
    "    'displayed_text': 'displayed_text',\n",
    "    'checked': 'checked'\n",
    "}\n",
    "\n",
    "batch1 = batch1.rename(columns = mapping_dct)\n",
    "batch1 = batch1.loc[:, ~batch1.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7044e62f-09c5-4c07-a8e9-9f94a5506cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1.to_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\firstTry\\batch1_FullFinal.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08318f-df60-4eca-ba7a-e545e925df28",
   "metadata": {},
   "source": [
    "## make 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "17868826-d444-42a3-8719-6e73bbb2f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate them\n",
    "\n",
    "batch1 =  pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\firstTry\\batch1_FullFinal.csv\", sep = \";\")\n",
    "batch2_m = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\marlies_batch2_checked.csv\", sep = \";\")\n",
    "batch2_l = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\laura2\\corrected_df_laura2_DONE.csv\", sep = \";\")\n",
    "batch2_s = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\stijn2\\stijn2.csv\", sep = \";\")\n",
    "\n",
    "batch2_l['checked'] = 1\n",
    "\n",
    "batch1['batch'] = 'batch1'\n",
    "batch2_m['batch'] = 'batch2_m'\n",
    "batch2_s['batch'] = 'batch2_s'\n",
    "batch2_l['batch'] = 'batch2_l'\n",
    "\n",
    "dfs = [batch1, batch2_m, batch2_s, batch2_l]\n",
    "\n",
    "# for df in dfs:\n",
    "#     df = df.astype('object')  # Specify type conversion\n",
    "for idx, i in enumerate(dfs):\n",
    "    i.columns = [j.lower() for j in i.columns]\n",
    "    try:\n",
    "        dfs[idx] = i.drop(columns = ['unnamed: 0', 'terms_abbrev'], errors = 'ignore')\n",
    "    except KeyError as e:\n",
    "        print('column not there')\n",
    "        \n",
    "combined_df = pd.concat(dfs, axis=0, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24a6f7-3c5f-4a94-a191-b650a8aca1fc",
   "metadata": {},
   "source": [
    "- all phylogenetic methods should have 'phylogenetic'\n",
    "- all haplotype should have 'genetic distance'\n",
    "- all interbr_morph should be interbeeding\n",
    "- Make sure I do the hierarchy updating again at the end for all the ones that have been corrected\n",
    "- Does acoustic data always assume nonphylo analysis?\n",
    "- Does gen_analysis imply gen_data?\n",
    "- does SPEC_DELIMITATION always involve phylogenetic tree inference? (or at least, if there is GMYC)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "db8d8122-0d2b-4404-bb4d-91e35eaeb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to complete labels\n",
    "\n",
    "combined_df.loc[(combined_df.interbr_morph == 1) | (combined_df.mating == 1) | (combined_df.gen_interbr == 1), 'interbreeding'] = 1\n",
    "combined_df.loc[(combined_df.haplowebs == 1) | (combined_df.distance_based == 1), 'distance' ] = 1\n",
    "combined_df.loc[(combined_df.phen_pylo == 1) | (combined_df.phylo_sd == 1) | (combined_df.phylo_tree == 1), 'phylogenetic'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3dfeded0-ce6c-4359-b44f-aa34d88cdadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phenotype</th>\n",
       "      <th>phen_data</th>\n",
       "      <th>morph</th>\n",
       "      <th>quant_morph</th>\n",
       "      <th>qual_morph</th>\n",
       "      <th>color_pattern</th>\n",
       "      <th>shape</th>\n",
       "      <th>texture</th>\n",
       "      <th>ultrastruct</th>\n",
       "      <th>interbr_morph</th>\n",
       "      <th>...</th>\n",
       "      <th>specimen_storage_loc</th>\n",
       "      <th>sampling_loc</th>\n",
       "      <th>abbrev_terms</th>\n",
       "      <th>nomenclat_history</th>\n",
       "      <th>biogeo</th>\n",
       "      <th>id</th>\n",
       "      <th>displayed_text</th>\n",
       "      <th>checked</th>\n",
       "      <th>sequencing</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/EJT/10_5852_ejt_2021_735_1243.json_0</td>\n",
       "      <td>The material examined was collected in fragmen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/Zootaxa/1/zootaxa_1920_1_5.json_0</td>\n",
       "      <td>Invertebrate samples\\nwere collected using a h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/Zootaxa/4/7/zootaxa_4729_2_8.json_0</td>\n",
       "      <td>The nymphs were collected in the stream by han...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/Pensoft/phytokeys-47-059.json_0</td>\n",
       "      <td>We verified both the endemic status and the di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/Pensoft/zookeys-315-055.json_0</td>\n",
       "      <td>During each cruise, specimens were sorted onbo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   phenotype  phen_data  morph  quant_morph  qual_morph  color_pattern  shape  \\\n",
       "0          1          0      0            0           0              0      0   \n",
       "1          1          0      0            0           0              0      0   \n",
       "2          1          0      0            0           0              0      0   \n",
       "3          1          0      0            0           0              0      0   \n",
       "4          1          0      0            0           0              0      0   \n",
       "\n",
       "   texture  ultrastruct  interbr_morph  ...  specimen_storage_loc  \\\n",
       "0        0            0              0  ...                     0   \n",
       "1        0            0              0  ...                     1   \n",
       "2        0            0              0  ...                     0   \n",
       "3        0            0              0  ...                     0   \n",
       "4        0            0              0  ...                     0   \n",
       "\n",
       "   sampling_loc  abbrev_terms  nomenclat_history  biogeo  \\\n",
       "0             1             1                  0       0   \n",
       "1             0             0                  0       0   \n",
       "2             0             0                  0       0   \n",
       "3             0             0                  0       0   \n",
       "4             0             0                  0       0   \n",
       "\n",
       "                                              id  \\\n",
       "0  ./Corpus/EJT/10_5852_ejt_2021_735_1243.json_0   \n",
       "1     ./Corpus/Zootaxa/1/zootaxa_1920_1_5.json_0   \n",
       "2   ./Corpus/Zootaxa/4/7/zootaxa_4729_2_8.json_0   \n",
       "3       ./Corpus/Pensoft/phytokeys-47-059.json_0   \n",
       "4        ./Corpus/Pensoft/zookeys-315-055.json_0   \n",
       "\n",
       "                                      displayed_text  checked  sequencing  \\\n",
       "0  The material examined was collected in fragmen...        0           0   \n",
       "1  Invertebrate samples\\nwere collected using a h...        0           0   \n",
       "2  The nymphs were collected in the stream by han...        0           0   \n",
       "3  We verified both the endemic status and the di...        0           0   \n",
       "4  During each cruise, specimens were sorted onbo...        1           0   \n",
       "\n",
       "    batch  \n",
       "0  batch1  \n",
       "1  batch1  \n",
       "2  batch1  \n",
       "3  batch1  \n",
       "4  batch1  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recursive updating using the hierarchy\n",
    "\n",
    "def lowercase_nested(data):\n",
    "    \"\"\"Recursively converts all strings in a nested structure (dict, list) to lowercase.\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        # Recursively handle dictionaries\n",
    "        return {k.lower(): lowercase_nested(v) if isinstance(k, str) else k for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        # Recursively handle lists\n",
    "        return [lowercase_nested(item) for item in data]\n",
    "    elif isinstance(data, str):\n",
    "        # Convert strings to lowercase\n",
    "        return data.lower()\n",
    "    else:\n",
    "        # Leave other data types unchanged\n",
    "        return data\n",
    "\n",
    "\n",
    "lc_classif = lowercase_nested(classif)\n",
    "recursive_update(combined_df, lc_classif).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d9de872-e433-4b4e-af7b-1b06b97beae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df.to_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\testData20012025.csv\")\n",
    "combined_df = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\testData20012025.csv\", index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec42497-c705-466e-996f-459580c11308",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## load newly annotated data from tsv\n",
    "\n",
    "This process includes reading the data and making sure that higher level categories are \"1\" if their leaf categories are \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9bd15b4d-5ea6-455d-86de-dd2e530c168d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepath1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mfilepath1\u001b[49m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m terms \u001b[38;5;241m=\u001b[39m extract_strings(classif)\n\u001b[0;32m      4\u001b[0m missing \u001b[38;5;241m=\u001b[39m [j \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m extract_strings(classif) \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:::\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m a[\u001b[38;5;241m3\u001b[39m:]]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filepath1' is not defined"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(filepath1, sep = '\\t')\n",
    "\n",
    "terms = extract_strings(classif)\n",
    "missing = [j for j in extract_strings(classif) if j not in [i.split(\":::\")[1] for i in a[3:]]]\n",
    "\n",
    "df1.columns = ['user', 'id', 'displayed_text'] + [i.split(\":::\")[1] for i in a[3:]]\n",
    "df1[missing] = np.nan\n",
    "df1[terms] = df1[terms].notna().astype(int)\n",
    "df1 = recursive_update(df1, classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "0176bf7d-a65e-42b6-8503-6a6439c0842c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Morphological terminology follows Goulet & Huber (1993), Viitasaari (2002), and Vikberg (2003). Body parts are measured in millimetres. The annuli of the\\nlancet are counted from the base towards the tip of the lancet starting with annulus 1. One reason is that they were\\nmissing in previous revisions (Kopelke 2007c: not even length of body is given). If possible (these are often more or less damaged), type\\nspecimens were also measured. The specimens chosen for measurement were well-developed specimens, and\\ntheir body size was near or above the mean. Reared specimens are often small and sometimes deformed due to\\nrearing conditions or because larvae were collected for rearing when too small. From the set of measurements\\nof one individual many different ratios or indices can be calculated. In small females the ovipositor and its\\nsheath is relatively longer when compared to the length of the hind femur or width of the head. Therefore the\\nvariation of ovipositor / head width index was studied in more detail, based on several females of different\\nsize. The biogeographical\\nprovinces of Finland, Norway and Sweden are according to Hämet-Ahti et al. ( '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch2_l.loc[batch2_l.id == './Corpus/Zootaxa/2/zootaxa_2620_1_1.json_1']['displayed_text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d010d6a-a2cc-4f25-b7a8-7678333abe2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Compare and fix annotations by multiple annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d587e7-a477-47dc-9dd1-25b03a97afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stijn = [i for i in batch2_m2.id.values if i not in batch2_m.id.values]\n",
    "remaining_marlies = [i for i in batch2_m2.id.values if i not in missing_stijn]\n",
    "\n",
    "df1 = batch2_m.loc[batch2_m.id.isin(remaining_marlies)]\n",
    "df2 = batch2_m2.loc[batch2_m2.id.isin(remaining_marlies + missing_stijn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a135ac2-449f-476c-b3b9-3cdf74e1415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good\n"
     ]
    }
   ],
   "source": [
    "# first make sure that the column names and shapes are identical\n",
    "if df1.shape == df2.shape:\n",
    "    print('all good')\n",
    "else:\n",
    "    print('shape mismatch')\n",
    "    print(df1.shape)\n",
    "    print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317c20e9-3b7e-482e-9682-93ac3c7d0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rows(ids, df):\n",
    "    new_rows = []\n",
    "    for i in ids:\n",
    "        new_row = {col: 0 for col in df.columns}\n",
    "        new_row['id'] = i\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f020f20b-8775-4718-b69b-b2c8c9f38ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = add_rows(missing_stijn, df1).sort_values(by='id').reset_index(drop=True).set_index('id')\n",
    "df2 = df2.sort_values(by='id').reset_index(drop=True).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fff7375-3c28-45b2-837c-a1fd1f8a8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should be rows that are empty for one, but not for the other\n",
    "# you don't always need this\n",
    "# so add them\n",
    "\n",
    "# missing_s = [i for i in batch2_l.id.values if i not in batch2_ls.id.values]\n",
    "# missing_l = [i for i in batch2_ls.id.values if i not in batch2_l.id.values]\n",
    "\n",
    "\n",
    "# df1 = add_rows(missing_s, batch2_ls).sort_values(by='id').reset_index(drop=True).set_index('id')\n",
    "# df2 = add_rows(missing_l, batch2_l).sort_values(by='id').reset_index(drop=True).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6651081f-153e-4218-8a42-9b5a812d0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an idea of how much disagreement there is\n",
    "shared = df1.compare(df2, result_names=('stijn','marlies'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3907ea28-cbee-45d5-bbce-959050b9bd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 72)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of samples to correct\n",
    "\n",
    "shared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2f867b9f-58f0-44ba-82bb-897dd3d99d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only load this when not at the start of the process\n",
    "\n",
    "df1 = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\corrected_df_marlies2.csv\", sep = \";\")\n",
    "# df1 = df1.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dd487ad0-36f5-4c23-8def-72b04847d5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>./Corpus/Zootaxa/9/zootaxa_960_1_1.json_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Phen_data</th>\n",
       "      <th>stijn</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marlies</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MORPH</th>\n",
       "      <th>stijn</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marlies</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ./Corpus/Zootaxa/9/zootaxa_960_1_1.json_0\n",
       "Phen_data stijn                                         1.0\n",
       "          marlies                                       0.0\n",
       "MORPH     stijn                                         1.0\n",
       "          marlies                                       0.0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first make sure that the column names and shapes are identical\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# the row you want to fix\n",
    "r = 76\n",
    "\n",
    "# get the id\n",
    "idx = shared.index[r]\n",
    "\n",
    "pd.DataFrame(shared.iloc[r,:]).dropna(axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d2195bf6-7bbf-4769-80c4-859c18b210c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The material of Lygistorrhina sanctaecatharinae was\\noriginally pinned, but was mounted on slides after treatment with KOH and dehydration. The data matrix (Appendix 1) was based on the one given by Grimaldi\\nand Blagoderov (2001), although some characters were excluded because we could not\\ncode the states, and some new characters were added. The character states for Matileola\\nyangi were taken from the original description (Papp 2003). The new taxa and Lygistorrhina sanctaecatharinae were coded from actual specimens. We did not try to find characters to resolve the phylogeny among the species of Lygistorrhina and Probolaeus. The data\\nmatrix for analysis was constructed and manipulated with the computer programme Winclada version 1.00.08 (Nixon 2002). Phylogenetic relationships were studied by parsimony\\nanalysis, using the computer programme NONA, version 2.0 (Goloboff 1999), together\\nwith Winclada, to search for the most parsimonious cladograms. The search parameters\\nused with NONA were ‘hold100000; hold/1000; mult*1000; max*; sswap*;. With these\\ncommands and settings, the programme makes a heuristic search and swaps branches with\\n‘tree bisection-reconnection’. The unsupported nodes were collapsed to accept only unambiguous support for the nodes in the strictest sense, i.e., only if all possible states between\\nthe ancestor and descendant node are different. Bremer support values were calculated by\\nNONA, using the following command sequence: hold 2000; sub 1; find*; hold 4000; sub\\n3; find*; hold 8000; sub 5; find*; hold 16000; sub 7; find*; hold 32000; sub10; find*;\\nbsupport;. The resulting cladograms and their strict consensus cladogram (Fig. The characters were equally\\nweighted in the analysis. Multistate characters were analyzed unordered. The character\\nstates were coded as (-) when the character involved was absent from a terminal and as (?) The programme used does not differentiate between these\\ncases. Selected species were used as terminals. The data matrix (Appendix 1) includes\\nmost described extant species of the family and our new undescribed species. To obtain the\\ncharacters, specimens of the latter were studied from slide mounts and most of the former\\nfrom the literature. Diadocidia of the Diadocidiidae was chosen as the outgroup. This\\nstudy is based mainly on males because females of these taxa are too poorly known. '"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the text\n",
    "df2.loc[idx]['displayed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3d3142f2-3376-49ea-abc0-934d891d9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose one df (e.g. dfx) to fix in place\n",
    "# change various column row values in place\n",
    "\n",
    "# df1.loc[idx, 'shape'] = 1\n",
    "# df1.loc[idx, 'sampling_loc'] = 1\n",
    "# df1.loc[idx, 'MORPH'] = 1\n",
    "# df1.loc[idx, 'STORAGE'] = 0\n",
    "# df1.loc[idx, 'displayed_text'] ='The Azores Autonomous Region is a Portuguese archipelago formed by nine volcanic islands, fortytwo identified seamounts, narrow shelves, steep island slopes, bathyal and abyssal plains and oceanic ridges, located\\nin the central North Atlantic Ocean (Morato et al. The archipelago is composed of three main groups of\\nislands intersected by the Mid-Atlantic Ridge (MAR), which divides the western group of islands (Flores and Corvo)\\nfrom the central (Terceira, Graciosa, S. Jorge, Pico and Faial) and eastern (S. Miguel and S. Maria) groups, over the\\nAzores plateau that rises from the adjacent abyssal plains from ca. '\n",
    "\n",
    "\n",
    "# use this to save the corrected df each time you make changes\n",
    "# df1.to_csv(r'C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\corrected_df_marlies2_DONE.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "772b186f-9d53-4ac5-ab55-cfd5e60005ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final.to_csv(r'C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\secondTry\\marlies2\\marlies_batch2_checked.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd714b-ff3b-4e51-ab5b-588b831227cc",
   "metadata": {},
   "source": [
    "# Data from active learning\n",
    "\n",
    "Get training sampls for sparse categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c24d39c-8e26-474a-86b1-b9504245a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired = ['PHENOTYPE',\n",
    " 'Phen_data',\n",
    " 'MORPH',\n",
    " 'Phen_proc',\n",
    " 'IMAGING',\n",
    " 'quant_morph',\n",
    " 'STORAGE',\n",
    " 'SAMPLING',\n",
    " 'GENOTYPE',\n",
    " 'interbr_morph',\n",
    " 'Gen_data',\n",
    " 'Gen_analysis',\n",
    " 'Gen_proc',\n",
    " 'SEQUENCING',\n",
    " 'organellar',\n",
    " 'PHYLO_TREE',\n",
    " 'Phen_analysis',\n",
    " 'character_based',\n",
    " 'color_pattern',\n",
    " 'phen_nonphylo',\n",
    " 'GEN_NON_PHYLO',\n",
    " 'ECOLOGY',\n",
    " 'distance',\n",
    " 'nuclear',\n",
    " 'BEHAV',\n",
    " 'phylogenetic',\n",
    " 'rank_just',\n",
    " 'phen_pylo',\n",
    " 'distance_based',\n",
    " 'acoustic',\n",
    " 'PHYLO_SD',\n",
    " 'interbreeding',\n",
    " ]\n",
    "\n",
    "desired = [i.lower() for i in desired]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fea0192-6a88-4ed5-9a03-46cb8fce9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# original data\n",
    "df = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\testData20012025.csv\")\n",
    "df = df.drop(columns = 'Unnamed: 0')\n",
    "df.head(2)\n",
    "\n",
    "\n",
    "# preprocess\n",
    "\n",
    "# Extract features and labels\n",
    "X = df['displayed_text']  # Features (text field)\n",
    "y = df[desired]# df.iloc[:, :-4]  # Assuming last 3 columns are metadata, adjust as needed\n",
    "\n",
    "# Remove columns with only one class (all 0s or all 1s)\n",
    "non_constant_columns = [col for col in y.columns if y[col].nunique() > 1]\n",
    "y = y[non_constant_columns]\n",
    "\n",
    "# vectorize text\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=2808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ebbcdef-c2f8-4694-a647-12848cff1914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nuclear           54\n",
       "behav             47\n",
       "rank_just         40\n",
       "phen_pylo         30\n",
       "distance_based    23\n",
       "acoustic          20\n",
       "phylo_sd          18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = combined_df[desired].sum() < 60\n",
    "combined_df[desired].sum()[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb7006a3-c01e-4522-8fbf-008668375dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nuclear', 'behav', 'rank_just', 'phen_pylo', 'distance_based',\n",
       "       'acoustic', 'phylo_sd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[desired].sum()[mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5be6221-4194-43ee-a851-618b6893a27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nuclear', 'behav', 'rank_just', 'phen_pylo', 'distance_based',\n",
       "       'acoustic', 'phylo_sd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse = combined_df[desired].sum()[mask].index\n",
    "sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89fa2c7f-cae9-4792-8ae0-d08388f00e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load a classifier\n",
    "import joblib\n",
    "\n",
    "base_filepath = r\"C:\\Users\\conix\\Documents\\Corpus\\classifier_models\"\n",
    "\n",
    "# Load the vectorizer\n",
    "vectorizer_filepath = os.path.join(base_filepath, \"vectorizer.pkl\")\n",
    "vectorizer = joblib.load(vectorizer_filepath)\n",
    "\n",
    "# Load each model dynamically based on label names\n",
    "loaded_models = {}\n",
    "for label in y.columns:  # Ensure this matches the original label set\n",
    "    model_filepath = os.path.join(base_filepath, f\"model_{label}.pkl\")\n",
    "    loaded_models[label] = joblib.load(model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "180e48ed-47b3-41fa-aa11-e7347af280f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples_dct = {key:value for key,value in methods_paras.items() if key not in combined_df.id.values}\n",
    "new_models_dict = {key:value for key,value in loaded_models.items() if key in sparse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "740e713f-fec8-4453-a175-1ca4a3e9cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New samples\n",
    "import random\n",
    "\n",
    "new_sample_keys = random.sample(list(new_samples_dct.keys()), 5000)\n",
    "new_samples = [new_samples_dct[i] for i in new_sample_keys]\n",
    "\n",
    "\n",
    "# Preprocess using the vectorizer\n",
    "new_samples_vectorized = vectorizer.transform(new_samples)\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for label, model in new_models_dict.items():\n",
    "    predictions[label] = model.predict(new_samples_vectorized)\n",
    "    probabilities[label] = model.predict_proba(new_samples_vectorized) \n",
    "\n",
    "# Convert probabilities to a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "probabilities_df = pd.DataFrame({label: prob[:, 1] for label, prob in probabilities.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7c3ff21-dbf7-4f90-9f43-e05ace6579e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check where the model is most uncertain by looking at the difference in pron between the two classes\n",
    "# smaller is more uncertain\n",
    "\n",
    "uncertainty = probabilities_df.apply(lambda x: abs(x - 0.5), axis=1)\n",
    "most_uncertain_indices = uncertainty.sum(axis=1).nsmallest(n=10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66d01157-2a80-4ce2-be13-e30a311a9f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "sample_indices = {}\n",
    "for i in uncertainty.columns:\n",
    "\n",
    "    #sample the 5 most informative for each\n",
    "    n = 5\n",
    "    # get indices of n most uncertain samples\n",
    "    indices = uncertainty.loc[:,i].sort_values()[:n].index.values\n",
    "    sample_indices[i] = indices\n",
    "\n",
    "indices = list(set([item for sublist in sample_indices.values() for item in sublist]))\n",
    "\n",
    "for i in [(0.3,6), (0.25,4), (0.18,3), (0.02,2)]:\n",
    "    extra = list(uncertainty[(uncertainty <i[0]).sum(axis=1) >= i[1]].index.values)\n",
    "    indices.extend(extra)  \n",
    "\n",
    "indices = list(set(indices))\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "725dcc7b-f259-49ba-854f-547af7223c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataset to annotate\n",
    "\n",
    "samples_to_annotate = {new_sample_keys[i]:new_samples[i] for i in indices}\n",
    "\n",
    "\n",
    "output_file_path = r'C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\fourthTry\\active_learning_samples1.json'\n",
    "\n",
    "\n",
    "# Write the random key-value pairs to the output file\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for key, text in samples_to_annotate.items():\n",
    "        entry = {\n",
    "            \"id\": key,\n",
    "            \"text\": text,\n",
    "            \"annotations\": []\n",
    "        }\n",
    "        f.write(json.dumps(entry) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
