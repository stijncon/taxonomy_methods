{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b36ff1-6837-40bc-a541-a56e1ed37219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import pickle\n",
    "from math import ceil\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    ParameterGrid,\n",
    "    RandomizedSearchCV,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    hamming_loss,\n",
    "    make_scorer,\n",
    "    precision_recall_fscore_support,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# Other ML libraries\n",
    "from lightgbm import LGBMClassifier\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from iterstrat.ml_stratifiers import (\n",
    "    MultilabelStratifiedKFold,\n",
    "    MultilabelStratifiedShuffleSplit,\n",
    ")\n",
    "\n",
    "# Sentence transformers (e.g., SciBERT)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Joblib\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d015ce81-428c-476e-8e31-2d31211f55c4",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15497ef0-a358-4408-a900-93a8bed2c158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phenotype</th>\n",
       "      <th>phen_data</th>\n",
       "      <th>morph</th>\n",
       "      <th>quant_morph</th>\n",
       "      <th>qual_morph</th>\n",
       "      <th>color_pattern</th>\n",
       "      <th>shape</th>\n",
       "      <th>texture</th>\n",
       "      <th>ultrastruct</th>\n",
       "      <th>interbr_morph</th>\n",
       "      <th>...</th>\n",
       "      <th>abbrev_terms</th>\n",
       "      <th>nomenclat_history</th>\n",
       "      <th>biogeo</th>\n",
       "      <th>id</th>\n",
       "      <th>displayed_text</th>\n",
       "      <th>checked</th>\n",
       "      <th>sequencing</th>\n",
       "      <th>batch</th>\n",
       "      <th>not important</th>\n",
       "      <th>is_random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/EJT/10_5852_ejt_2021_735_1243.json_0</td>\n",
       "      <td>The material examined was collected in fragmen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>./Corpus/Zootaxa/1/zootaxa_1920_1_5.json_0</td>\n",
       "      <td>Invertebrate samples\\nwere collected using a h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>batch1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   phenotype  phen_data  morph  quant_morph  qual_morph  color_pattern  shape  \\\n",
       "0          1          1      1            1           0              0      0   \n",
       "1          1          0      0            0           0              0      0   \n",
       "\n",
       "   texture  ultrastruct  interbr_morph  ...  abbrev_terms  nomenclat_history  \\\n",
       "0        0            0              0  ...             1                  0   \n",
       "1        0            0              0  ...             0                  0   \n",
       "\n",
       "   biogeo                                             id  \\\n",
       "0       0  ./Corpus/EJT/10_5852_ejt_2021_735_1243.json_0   \n",
       "1       0     ./Corpus/Zootaxa/1/zootaxa_1920_1_5.json_0   \n",
       "\n",
       "                                      displayed_text  checked  sequencing  \\\n",
       "0  The material examined was collected in fragmen...        0           0   \n",
       "1  Invertebrate samples\\nwere collected using a h...        0           0   \n",
       "\n",
       "    batch  not important  is_random  \n",
       "0  batch1            NaN          1  \n",
       "1  batch1            NaN          1  \n",
       "\n",
       "[2 rows x 71 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\testData13052025.csv\")\n",
    "df = df.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "\n",
    "# add a column for which data was really randomly sampled\n",
    "# this avoid testing on oversampled sparse classes\n",
    "\n",
    "df['is_random'] = np.where(df.batch.isin(['batch1', 'batch2_m', 'batch2_s', 'batch2_l', 'batch2_j',\n",
    "       'batch3_l']),1,0)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c3868a-3304-4ee0-9369-96bc7520ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list with categories of interest\n",
    "filename = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\data\\categories_of_interest.txt\"\n",
    "with open(filename, 'r') as file:\n",
    "    cats = json.load(file)\n",
    "\n",
    "# load the dictionary to map category column names to gpt names\n",
    "filename = r'C:\\\\Users\\\\conix\\\\Dropbox\\\\FNRS project taxonomy\\\\methods in taxonomy\\\\data\\\\category_names_dict.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    category_names = json.load(file)\n",
    "\n",
    "# descriptions of the various categories, used in the first try with gpt4omini\n",
    "filename = r'C:\\\\Users\\\\conix\\\\Dropbox\\\\FNRS project taxonomy\\\\methods in taxonomy\\\\data\\\\methods_description_gpt4omini_first_try.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    methods_old = json.load(file)\n",
    "\n",
    "# descriptions of the various categories, used in the second try with gpt4omini\n",
    "filename = r'C:\\\\Users\\\\conix\\\\Dropbox\\\\FNRS project taxonomy\\\\methods in taxonomy\\\\data\\\\methods_description_gpt4omini_second_try.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    methods = json.load(file)\n",
    "\n",
    "# hierarchical classification of the categories\n",
    "filename = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\data\\classification_categories.txt\"\n",
    "with open(filename, \"r\") as file:\n",
    "    classif = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1050dd-3780-44f6-8bba-5d419e3106b8",
   "metadata": {},
   "source": [
    "# Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13579001-28ba-4c06-a644-48906e517ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics for reporting and validation\n",
    "def fold_metrics(y_true, y_pred):\n",
    "    \"\"\"Return overall + per-label metrics in a flat dict.\"\"\"\n",
    "    out = {\n",
    "        'macro_f1'  : f1_score(y_true, y_pred, average='macro'),\n",
    "        'micro_f1'  : f1_score(y_true, y_pred, average='micro'),\n",
    "        'macro_prec': precision_score(y_true, y_pred, average='macro'),\n",
    "        'micro_prec': precision_score(y_true, y_pred, average='micro'),\n",
    "        'macro_rec' : recall_score(y_true, y_pred, average='macro'),\n",
    "        'micro_rec' : recall_score(y_true, y_pred, average='micro'),\n",
    "        'hamming'   : hamming_loss(y_true, y_pred),\n",
    "        'subset_acc': accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "    p, r, f, s = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0)\n",
    "    for i, lab in enumerate(y_true.columns):\n",
    "        out[f'{lab}_prec'] = p[i]\n",
    "        out[f'{lab}_rec']  = r[i]\n",
    "        out[f'{lab}_f1']   = f[i]\n",
    "        out[f'{lab}_sup']  = s[i]\n",
    "    return out\n",
    "    \n",
    "\n",
    "# function to propagate the hierarchy through the classifier results\n",
    "# this at least partially captures label relations in this binary-relevance strategy\n",
    "def propagate_hierarchy(pred_df, classif):\n",
    "    \"\"\"Return a copy where every parent/cross-link label is made 1\n",
    "       if any of its children are 1.\"\"\"\n",
    "    out = pred_df.copy()\n",
    "\n",
    "    def recurse(node):\n",
    "        if isinstance(node, dict):\n",
    "            for parent, children in node.items():\n",
    "                recurse(children)\n",
    "                child_keys = []\n",
    "                for c in children:\n",
    "                    if isinstance(c, str):\n",
    "                        child_keys.append(c)\n",
    "                    elif isinstance(c, dict):\n",
    "                        child_keys.extend(c.keys())\n",
    "                if child_keys:\n",
    "                    out.loc[out[child_keys].eq(1).any(axis=1), parent] = 1\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                recurse(item)\n",
    "\n",
    "    recurse(classif)\n",
    "\n",
    "    # cross-links outside the hierarchical classification\n",
    "    if {'interbr_morph','gen_interbr','interbreeding'}.issubset(out):\n",
    "        mask = (out.interbr_morph==1)|(out.gen_interbr==1)\n",
    "        out.loc[mask, 'interbreeding'] = 1\n",
    "\n",
    "    if {'distance_based','distance'}.issubset(out):\n",
    "        out.loc[out.distance_based==1, 'distance'] = 1\n",
    "\n",
    "    if {'phen_pylo','phylo_sd','phylo_tree','phylogenetic'}.issubset(out):\n",
    "        mask = (out.phen_pylo==1)|(out.phylo_sd==1)|(out.phylo_tree==1)\n",
    "        out.loc[mask, 'phylogenetic'] = 1\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b70bf6-3c97-4278-b4e4-6d02dd2307a6",
   "metadata": {},
   "source": [
    "# Processing text and feature configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c128e2-1cb7-46be-b201-18ef257e53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df   = df[df['is_random'] == 1]      # unbiased rows\n",
    "biased_df = df[df['is_random'] == 0]      # rows from biased sampling (to increase sparse categories)\n",
    "\n",
    "X_rand, y_rand = rand_df['displayed_text'], rand_df[cats]\n",
    "X_bias, y_bias = biased_df['displayed_text'], biased_df[cats]\n",
    "\n",
    "# take 80/20 split from the unbiased part\n",
    "# use this for training, not for validation or testing\n",
    "# the biased samples are only used for training, i.e. appended to the training data within each fold's training set\n",
    "# at the final retraining, the biased samples are also used for training\n",
    "msss = MultilabelStratifiedShuffleSplit(test_size=0.20, random_state=42)\n",
    "train_idx, test_idx = next(msss.split(X_rand, y_rand))\n",
    "\n",
    "X_train_raw, y_train_raw = X_rand.iloc[train_idx], y_rand.iloc[train_idx]\n",
    "X_test,      y_test      = X_rand.iloc[test_idx],  y_rand.iloc[test_idx]\n",
    "\n",
    "# features & pipeline skeleton—unchanged\n",
    "tfidf  = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.95,\n",
    "                         sublinear_tf=True, norm='l2', dtype=np.float32,\n",
    "                         lowercase=True, strip_accents='unicode')\n",
    "\n",
    "# for estimators that don't accept sparse matrices (rf, knn, gb)\n",
    "densify = FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)\n",
    "\n",
    "# we choose macro_F1 as our classes are very imbalanced, and many are very sparse\n",
    "score   = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# use multilabelstratifiedkfold to keep the label proportions roughly equal\n",
    "# 5 folds in the outer loop (for model evaluation)\n",
    "outer = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# 3 folds in the inner loop (for hyperparameter tuning)\n",
    "inner = MultilabelStratifiedKFold(n_splits=3, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe99e9f5-12ff-42e5-9125-436c7e73a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer\n",
    "# the one that came out as the best one in the binary relevance approach\n",
    "plain_tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    norm='l2',\n",
    "    sublinear_tf=True,\n",
    "    max_df=0.95,          \n",
    "    min_df=5,             \n",
    "    ngram_range=(1, 1),   \n",
    "    stop_words=None       \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a73a2-8561-4b20-8fbb-baeb437499c9",
   "metadata": {},
   "source": [
    "# Classifier chain orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f344b-7eaa-4868-8033-ad2048ee5524",
   "metadata": {},
   "source": [
    "We have information about the relations between labels, as well as information on how well LR can classify each. Using this information, a classifier chain might be better overall. The general strategy is to put children before parents, and within children, put the ones that are easiest to classify first. In addition to that, also try ordering them by correlation (if two labels correlate, put the one that is easiest to classify first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15aa6639-942f-4b03-bec4-85f13f397509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hier_corr_f1_order(hierarchy, y_df, mean_f1, alpha=0.7):\n",
    "    \"\"\"\n",
    "    hierarchy : nested dict/list structure (your `classif`)\n",
    "    y_df      : DataFrame of shape (n, L) with 0/1\n",
    "    mean_f1   : pd.Series  index = label_name,  value = mean F1 from BR CV\n",
    "    alpha     : weight for F1 in composite score  (0..1)\n",
    "\n",
    "    returns   idx_order  (list[int]), name_order (list[str])\n",
    "    \"\"\"\n",
    "    labels = list(y_df.columns)\n",
    "    L      = len(labels)\n",
    "\n",
    "    # 1. correlation matrix (φ-coefficient)\n",
    "    corr = y_df.corr().abs()\n",
    "\n",
    "    # 2. helper: composite score for child w.r.t. parent\n",
    "    def score(child, parent):\n",
    "        f1   = mean_f1.get(child, 0.0)\n",
    "        rho  = corr.loc[parent, child] if (parent in corr and child in corr) else 0.0\n",
    "        return alpha * f1 + (1 - alpha) * rho\n",
    "\n",
    "    order, seen = [], set()\n",
    "\n",
    "    def dfs(node):\n",
    "        if isinstance(node, dict):\n",
    "            for parent, children in node.items():\n",
    "                # collect direct children names\n",
    "                direct = []\n",
    "                if isinstance(children, dict):\n",
    "                    direct += list(children.keys())\n",
    "                elif isinstance(children, list):\n",
    "                    for c in children:\n",
    "                        direct += (list(c.keys()) if isinstance(c, dict) else [c])\n",
    "                # sort those children by composite score\n",
    "                sorted_children = sorted(\n",
    "                    direct,\n",
    "                    key=lambda c: score(c, parent),\n",
    "                    reverse=True\n",
    "                )\n",
    "                # recurse down each child-subtree in that order\n",
    "                for c in sorted_children:\n",
    "                    sub = ({c: children[c]} if isinstance(children, dict) and c in children\n",
    "                           else next((d for d in children if isinstance(d, dict) and c in d), c))\n",
    "                    dfs(sub)\n",
    "                # finally emit parent\n",
    "                if parent in labels and parent not in seen:\n",
    "                    order.append(parent); seen.add(parent)\n",
    "                # make sure to recurse into grandchildren\n",
    "                dfs(children)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                dfs(item)\n",
    "        elif isinstance(node, str):\n",
    "            if node in labels and node not in seen:\n",
    "                order.append(node); seen.add(node)\n",
    "\n",
    "    dfs(hierarchy)\n",
    "\n",
    "    # 3. append singletons by descending F1\n",
    "    for lab in mean_f1.sort_values(ascending=False).index:\n",
    "        if lab not in seen:\n",
    "            order.append(lab); seen.add(lab)\n",
    "\n",
    "    idx_order = [labels.index(l) for l in order]\n",
    "    return idx_order, order\n",
    "\n",
    "def compute_label_correlations(y_df):\n",
    "    \"\"\"\n",
    "    Given y_df: a DataFrame of shape (n_samples, n_labels), with 0/1 entries,\n",
    "    compute the L×L Pearson correlation matrix between columns. Then return:\n",
    "      - corr_matrix: a DataFrame of shape (L, L) where corr_matrix.loc[i, j]\n",
    "                     is Pearson( y_df.iloc[:, i], y_df.iloc[:, j] ).\n",
    "      - total_abs_corr: a Series of length L where total_abs_corr[i] = \n",
    "                        sum_j | corr_matrix[i, j] | (i.e. how “connected” label i is).\n",
    "    \"\"\"\n",
    "    # 1) .corr() on a binary DataFrame gives Pearson (which is equivalent to φ‐coefficient\n",
    "    #    for 0/1 variables). This is symmetric, with ones on the diagonal.\n",
    "    corr_matrix = y_df.corr()\n",
    "\n",
    "    # 2) Compute sum of absolute correlations (excluding self-correlation if you like):\n",
    "    #    We’ll exclude the diagonal to avoid counting corr=1 with itself.\n",
    "    abs_corr = corr_matrix.abs().copy()\n",
    "    np.fill_diagonal(abs_corr.values, 0.0)\n",
    "\n",
    "    total_abs_corr = abs_corr.sum(axis=1)\n",
    "    return corr_matrix, total_abs_corr\n",
    "    \n",
    "\n",
    "def correlation_greedy_order(y_df):\n",
    "    \"\"\"\n",
    "    Return a permutation of [0..L-1] based on a greedy walk over the correlation graph:\n",
    "     - Start with the label whose total abs-corr is largest.\n",
    "     - Then repeatedly pick the as‐yet‐unused label that is most correlated\n",
    "       (by absolute Pearson) with the last‐selected label.\n",
    "    This ignores any hierarchical constraints.\n",
    "    \"\"\"\n",
    "    # Compute correlation matrix and total absolute correlations:\n",
    "    corr_df, total_abs_corr = compute_label_correlations(y_df)\n",
    "    labels = list(y_df.columns)\n",
    "    L = len(labels)\n",
    "\n",
    "    # (A) Identify the first label: argmax of total_abs_corr\n",
    "    start_label = total_abs_corr.idxmax()  # e.g. 'Cat'\n",
    "    order_names = [start_label]\n",
    "    used = {start_label}\n",
    "\n",
    "    # (B) Greedy walk: at each step, pick the unused label most correlated with the last one\n",
    "    while len(order_names) < L:\n",
    "        last = order_names[-1]\n",
    "        # Among the unused labels, find which one has max |corr(last, candidate)|.\n",
    "        # corr_df[last] is a Series: correlation of `last` with every other label.\n",
    "        candidates = [\n",
    "            lbl for lbl in labels\n",
    "            if lbl not in used\n",
    "        ]\n",
    "        # Compute absolute correlation to `last` for each candidate:\n",
    "        best_next = max(candidates, key=lambda lbl: abs(corr_df.loc[last, lbl]))\n",
    "        order_names.append(best_next)\n",
    "        used.add(best_next)\n",
    "\n",
    "    # Convert label‐names to indices in y_df.columns\n",
    "    idx_order = [labels.index(lbl) for lbl in order_names]\n",
    "    return idx_order, order_names\n",
    "\n",
    "def build_parent_first_order(classif, cats):\n",
    "\n",
    "    # STEP 1: build a “pre-order” list of all labels in parent→child order\n",
    "    name_ord = []\n",
    "\n",
    "    def _recurse_node(parent_name, children_list):\n",
    "        # Emit the parent itself first\n",
    "        name_ord.append(parent_name)\n",
    "\n",
    "        # Then process its children (if any)\n",
    "        for child in children_list:\n",
    "            if isinstance(child, str):\n",
    "                # plain‐string → leaf, just emit it\n",
    "                name_ord.append(child)\n",
    "\n",
    "            elif isinstance(child, dict):\n",
    "                # dict → nested subtree.  Each dict should contain exactly one key→list.\n",
    "                for k, v in child.items():\n",
    "                    _recurse_node(k, v)\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    f\"Expected str or dict, but got {type(child)} inside children of {parent_name}\"\n",
    "                )\n",
    "\n",
    "    # Walk every top‐level group in `classif`:\n",
    "    for top_key, top_children in classif.items():\n",
    "        if top_key == \"singletons\":\n",
    "            # “singletons” is just a bucket of leaf‐names → emit each leaf directly\n",
    "            for leaf in top_children:\n",
    "                if not isinstance(leaf, str):\n",
    "                    raise TypeError(\n",
    "                        \"Inside 'singletons' we expect only plain strings, got: \"\n",
    "                        f\"{type(leaf)}\"\n",
    "                    )\n",
    "                name_ord.append(leaf)\n",
    "\n",
    "        else:\n",
    "            # a “real” parent node → recurse in pre-order\n",
    "            _recurse_node(top_key, top_children)\n",
    "\n",
    "    # STEP 2: look up each name’s column‐index in y_train_raw\n",
    "    # We assume y_train_raw is a pandas DataFrame:\n",
    "    try:\n",
    "        cols = cats\n",
    "    except AttributeError:\n",
    "        raise ValueError(\n",
    "            \"build_parent_first_order expects y_train_raw to be a pandas DataFrame with .columns set to all label names\"\n",
    "        )\n",
    "\n",
    "    # Verify that every name actually exists in y_train_raw.columns\n",
    "    missing = [n for n in name_ord if n not in cols]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            \"Some labels in the hierarchy were not found among y_train_raw.columns:\\n\"\n",
    "            + \", \".join(missing)\n",
    "        )\n",
    "\n",
    "    idx_ord = [cols.index(n) for n in name_ord]\n",
    "    return {'idx':idx_ord, 'name':name_ord}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49861db7-68f8-43bb-b45a-b003ae78631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results from LR\n",
    "\n",
    "with open(\"methods_paper_files/best_lr_stats.pkl\", \"rb\") as f:\n",
    "    lr_results = pickle.load(f)\n",
    "\n",
    "lr_results = pd.DataFrame(lr_results['mean']).reset_index()\n",
    "lr_f1 = lr_results.loc[lr_results['index'].str.contains('_f1')].iloc[2:]\n",
    "lr_f1['cats'] = cats\n",
    "lr_f1.columns = ['name','F1','labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7292fa52-ce6a-4ac7-9216-ee3a5948aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f1 = lr_f1['F1']\n",
    "alpha = 0.7\n",
    "mean_f1.index = y_train_raw.columns   # strip suffix\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  Decide which “principled” orders you want to try.\n",
    "#     – vary α  (F1-vs-corr weight)        →  6 permutations\n",
    "#     – optionally add a pure bottom-up    →  +1\n",
    "#     – (optionally) add a pure corr walk  →  +1\n",
    "# ------------------------------------------------------------\n",
    "alphas = [0.0, 0.3, 0.7, 0.9]        # 0 = corr-only, 1 = F1-only\n",
    "\n",
    "orders_info = {}                                # key → dict{idx_order, name_order}\n",
    "\n",
    "for a in alphas:\n",
    "    idx_ord, name_ord = build_hier_corr_f1_order(classif, y_train_raw,\n",
    "                                                 mean_f1, alpha=a)\n",
    "    key = f'alpha_{a:.1f}'\n",
    "    orders_info[key] = {'idx': idx_ord, 'name': name_ord}\n",
    "\n",
    "\n",
    "\n",
    "# Optional-2: pure correlation greedy walk (ignores hierarchy)\n",
    "idx_corr, name_corr = correlation_greedy_order(y_train_raw)\n",
    "orders_info['pure_corr'] = {'idx': idx_corr, 'name': name_corr}\n",
    "\n",
    "# try parents first as well\n",
    "orders_info['parents_first'] = build_parent_first_order(classif,cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f338c50-c149-4bc4-b122-7c3c4a3be5fe",
   "metadata": {},
   "source": [
    "# Classifier chain Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b6c0c3d-9329-4d3c-aa08-b39df7c81945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to evaluate a chain with a particular order\n",
    "\n",
    "def evaluate_chain_with_order(idx_order):\n",
    "    \"\"\"\n",
    "    Run a 5-fold *outer* CV for a ClassifierChain that uses the exact\n",
    "    integer permutation `idx_order`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    idx_order : list[int]\n",
    "        A permutation of [0, 1, …, L-1] where L = number of labels.\n",
    "        Each value is the column-index (in y_train_raw.columns) that should\n",
    "        be predicted at that position in the chain.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        {\n",
    "          'mean'    : pd.Series   (mean of every metric across 5 folds)\n",
    "          'std'     : pd.Series   (sample std-dev  ddof=1)\n",
    "          'ci_95'   : pd.DataFrame   (rows 0.025 & 0.975 quantiles)\n",
    "          'fold_df' : pd.DataFrame   (one row per outer fold with all metrics)\n",
    "        }\n",
    "    \"\"\"\n",
    "    # ---------- sanity-check ------------------------------------------------\n",
    "    n_labels = y_train_raw.shape[1]\n",
    "    if sorted(idx_order) != list(range(n_labels)):\n",
    "        raise ValueError(\"idx_order must be a permutation of 0..L-1\")\n",
    "\n",
    "    # ---------- model -------------------------------------------------------\n",
    "    # use the parameters from the binary relevance approach\n",
    "    base_lr = LogisticRegression(\n",
    "            C=10,\n",
    "            class_weight='balanced',\n",
    "            max_iter=2000,\n",
    "            solver='saga',\n",
    "            penalty = 'elasticnet',\n",
    "            l1_ratio = 0.9\n",
    "        )\n",
    "    chain = ClassifierChain(\n",
    "        base_estimator=base_lr,\n",
    "        order=idx_order,\n",
    "        random_state=42\n",
    "    )\n",
    "    pipe = Pipeline([\n",
    "        ('vect',  vect),\n",
    "        ('dense', densify),   \n",
    "        ('clf',   chain)\n",
    "    ])\n",
    "\n",
    "    # ---------- outer-CV loop ----------------------------------------------\n",
    "    fold_dicts = []\n",
    "\n",
    "    for train_idx, val_idx in tqdm(\n",
    "            outer.split(X_train_raw, y_train_raw),\n",
    "            total=outer.get_n_splits(),\n",
    "            desc=\"Outer CV (fixed chain order)\"\n",
    "    ):\n",
    "        # training = unbiased subset + biased rows\n",
    "        X_tr = pd.concat([X_train_raw.iloc[train_idx], X_bias], axis=0)\n",
    "        y_tr = pd.concat([y_train_raw.iloc[train_idx], y_bias], axis=0)\n",
    "\n",
    "        # validation = held-out slice of unbiased part\n",
    "        X_val = X_train_raw.iloc[val_idx]\n",
    "        y_val = y_train_raw.iloc[val_idx]\n",
    "\n",
    "        # fit and predict\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        y_pred = pipe.predict(X_val)          \n",
    "\n",
    "        # convert → DataFrame so hierarchy code can use column names\n",
    "        pred_df = pd.DataFrame(\n",
    "            y_pred,\n",
    "            columns=y_val.columns,\n",
    "            index=X_val.index\n",
    "        )\n",
    "\n",
    "        # enforce hierarchy / cross-links\n",
    "        pred_df = propagate_hierarchy(pred_df, classif)\n",
    "        pred_df = (\n",
    "            pred_df\n",
    "            .fillna(0)\n",
    "            .astype(int)\n",
    "            .reindex(columns=y_val.columns, fill_value=0)\n",
    "        )\n",
    "\n",
    "        # metrics for this fold\n",
    "        fold_metrics_dict = fold_metrics(y_val, pred_df.values)\n",
    "        fold_dicts.append(fold_metrics_dict)\n",
    "\n",
    "    # ---------- aggregate results ------------------------------------------\n",
    "    df_scores = pd.DataFrame(fold_dicts)\n",
    "\n",
    "    return {\n",
    "        'mean':    df_scores.mean(),\n",
    "        'std':     df_scores.std(ddof=1),\n",
    "        'ci_95':   df_scores.quantile([0.025, 0.975]),\n",
    "        'fold_df': df_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c7bd04-f612-4b42-bae7-c4dbf8d9a69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶ Evaluating order: alpha_0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Outer CV (fixed chain order): 100%|█████████████████████████████████████████████████████| 5/5 [18:37<00:00, 223.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   macro-F1 mean ± std : 0.6490 ± 0.0213\n",
      "\n",
      "▶ Evaluating order: alpha_0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Outer CV (fixed chain order): 100%|█████████████████████████████████████████████████████| 5/5 [17:29<00:00, 209.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   macro-F1 mean ± std : 0.6486 ± 0.0213\n",
      "\n",
      "▶ Evaluating order: alpha_0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Outer CV (fixed chain order): 100%|█████████████████████████████████████████████████████| 5/5 [14:58<00:00, 179.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   macro-F1 mean ± std : 0.6525 ± 0.0193\n",
      "\n",
      "▶ Evaluating order: alpha_0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Outer CV (fixed chain order): 100%|█████████████████████████████████████████████████████| 5/5 [09:44<00:00, 116.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   macro-F1 mean ± std : 0.6542 ± 0.0172\n",
      "\n",
      "▶ Evaluating order: pure_corr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Outer CV (fixed chain order): 100%|█████████████████████████████████████████████████████| 5/5 [13:48<00:00, 165.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   macro-F1 mean ± std : 0.6307 ± 0.0161\n",
      "\n",
      "▶ Evaluating order: parents_first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Outer CV (fixed chain order): 100%|█████████████████████████████████████████████████████| 5/5 [10:06<00:00, 121.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   macro-F1 mean ± std : 0.6201 ± 0.0132\n",
      "\n",
      "*** Best chain order (by mean macro-F1 across 5 folds) ***\n",
      "order key : alpha_0.9\n",
      "macro-F1  : 0.654244045126967\n",
      "name order: ['quant_morph', 'interbr_morph', 'color_pattern', 'shape', 'qual_morph', 'morph', 'ecology', 'acoustic', 'behav', 'phen_data', 'imaging', 'storage', 'sampling', 'phen_proc', 'phen_nonphylo', 'phen_pylo', 'phen_analysis', 'phenotype', 'organellar', 'nuclear', 'gen_data', 'gen1', 'sequencing', 'gen_proc', 'character_based', 'distance_based', 'phylo_tree', 'phylo_sd', 'distance', 'gen_non_phylo', 'gen_analysis', 'genotype', 'phylogenetic', 'specimen_storage_loc', 'interbreeding', 'abbrev_terms', 'rank_just', 'biogeo']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cv_results = {}\n",
    "for key, info in orders_info.items():\n",
    "    print(f'\\nEvaluating order: {key}')\n",
    "    res = evaluate_chain_with_order(info['idx'])\n",
    "    cv_results[key] = res\n",
    "    print(f\"   macro-F1 mean ± std : {res['mean']['macro_f1']:.4f}\"\n",
    "          f\" ± {res['std']['macro_f1']:.4f}\")\n",
    "\n",
    "# Pick the winner on macro-F1 (or any metric)\n",
    "\n",
    "best_key = max(cv_results,\n",
    "               key=lambda k: cv_results[k]['mean']['macro_f1'])\n",
    "print('\\n*** Best chain order (by mean macro-F1 across 5 folds) ***')\n",
    "print('order key :', best_key)\n",
    "print('macro-F1  :', cv_results[best_key]['mean']['macro_f1'])\n",
    "print('name order:', orders_info[best_key]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5976a-3863-4057-aee9-f8695aba7029",
   "metadata": {},
   "source": [
    "# run chosen model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f824a6c5-40b4-49b1-b846-2c127ba3a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) Build the “final” base‐estimator using best_params ─────────────────\n",
    "base_lr_final = LogisticRegression(\n",
    "            C=10,\n",
    "            class_weight='balanced',\n",
    "            max_iter=2e4,\n",
    "            solver='saga',\n",
    "            penalty = 'elasticnet',\n",
    "            l1_ratio = 0.9\n",
    "        )\n",
    "\n",
    "final_chain = ClassifierChain(\n",
    "    base_estimator=base_lr_final,\n",
    "    order=orders_info['alpha_0.9']['idx'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_pipe = Pipeline([\n",
    "    ('vect', vect),\n",
    "    ('dense', densify),\n",
    "    ('clf',  final_chain)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "216769a5-4d36-4a95-bb29-c3a4a5cf45f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test‐set metrics:\n",
      "  macro_f1: 0.6967\n",
      "  micro_f1: 0.7910\n",
      "  macro_prec: 0.7643\n",
      "  micro_prec: 0.8591\n",
      "  macro_rec: 0.6651\n",
      "  micro_rec: 0.7329\n",
      "  hamming: 0.0799\n",
      "  subset_acc: 0.1683\n",
      "  phenotype_prec: 0.9912\n",
      "  phenotype_rec: 0.8396\n",
      "  phenotype_f1: 0.9091\n",
      "  phenotype_sup: 268.0000\n",
      "  phen_data_prec: 0.9665\n",
      "  phen_data_rec: 0.7457\n",
      "  phen_data_f1: 0.8418\n",
      "  phen_data_sup: 232.0000\n",
      "  morph_prec: 0.9756\n",
      "  morph_rec: 0.7583\n",
      "  morph_f1: 0.8533\n",
      "  morph_sup: 211.0000\n",
      "  biogeo_prec: 0.6364\n",
      "  biogeo_rec: 0.2917\n",
      "  biogeo_f1: 0.4000\n",
      "  biogeo_sup: 24.0000\n",
      "  color_pattern_prec: 0.6923\n",
      "  color_pattern_rec: 0.3913\n",
      "  color_pattern_f1: 0.5000\n",
      "  color_pattern_sup: 23.0000\n",
      "  phen_proc_prec: 0.9286\n",
      "  phen_proc_rec: 0.8535\n",
      "  phen_proc_f1: 0.8895\n",
      "  phen_proc_sup: 198.0000\n",
      "  imaging_prec: 0.9008\n",
      "  imaging_rec: 0.9291\n",
      "  imaging_f1: 0.9147\n",
      "  imaging_sup: 127.0000\n",
      "  quant_morph_prec: 0.8403\n",
      "  quant_morph_rec: 0.8264\n",
      "  quant_morph_f1: 0.8333\n",
      "  quant_morph_sup: 121.0000\n",
      "  storage_prec: 0.8158\n",
      "  storage_rec: 0.7686\n",
      "  storage_f1: 0.7915\n",
      "  storage_sup: 121.0000\n",
      "  sampling_prec: 0.7125\n",
      "  sampling_rec: 0.6706\n",
      "  sampling_f1: 0.6909\n",
      "  sampling_sup: 85.0000\n",
      "  genotype_prec: 1.0000\n",
      "  genotype_rec: 0.7188\n",
      "  genotype_f1: 0.8364\n",
      "  genotype_sup: 64.0000\n",
      "  interbr_morph_prec: 0.8667\n",
      "  interbr_morph_rec: 0.6500\n",
      "  interbr_morph_f1: 0.7429\n",
      "  interbr_morph_sup: 60.0000\n",
      "  gen_data_prec: 0.9318\n",
      "  gen_data_rec: 0.7736\n",
      "  gen_data_f1: 0.8454\n",
      "  gen_data_sup: 53.0000\n",
      "  gen_analysis_prec: 0.8788\n",
      "  gen_analysis_rec: 0.7250\n",
      "  gen_analysis_f1: 0.7945\n",
      "  gen_analysis_sup: 40.0000\n",
      "  gen_proc_prec: 0.9000\n",
      "  gen_proc_rec: 0.8182\n",
      "  gen_proc_f1: 0.8571\n",
      "  gen_proc_sup: 44.0000\n",
      "  sequencing_prec: 0.9250\n",
      "  sequencing_rec: 0.8605\n",
      "  sequencing_f1: 0.8916\n",
      "  sequencing_sup: 43.0000\n",
      "  gen1_prec: 0.7838\n",
      "  gen1_rec: 0.9355\n",
      "  gen1_f1: 0.8529\n",
      "  gen1_sup: 31.0000\n",
      "  organellar_prec: 0.8205\n",
      "  organellar_rec: 0.8421\n",
      "  organellar_f1: 0.8312\n",
      "  organellar_sup: 38.0000\n",
      "  phylo_tree_prec: 0.7742\n",
      "  phylo_tree_rec: 0.8889\n",
      "  phylo_tree_f1: 0.8276\n",
      "  phylo_tree_sup: 27.0000\n",
      "  phen_analysis_prec: 0.7778\n",
      "  phen_analysis_rec: 0.5600\n",
      "  phen_analysis_f1: 0.6512\n",
      "  phen_analysis_sup: 25.0000\n",
      "  character_based_prec: 0.6538\n",
      "  character_based_rec: 0.9444\n",
      "  character_based_f1: 0.7727\n",
      "  character_based_sup: 18.0000\n",
      "  qual_morph_prec: 0.7742\n",
      "  qual_morph_rec: 0.4706\n",
      "  qual_morph_f1: 0.5854\n",
      "  qual_morph_sup: 102.0000\n",
      "  shape_prec: 0.6333\n",
      "  shape_rec: 0.4222\n",
      "  shape_f1: 0.5067\n",
      "  shape_sup: 45.0000\n",
      "  phen_nonphylo_prec: 0.6923\n",
      "  phen_nonphylo_rec: 0.6429\n",
      "  phen_nonphylo_f1: 0.6667\n",
      "  phen_nonphylo_sup: 14.0000\n",
      "  gen_non_phylo_prec: 0.6250\n",
      "  gen_non_phylo_rec: 0.5263\n",
      "  gen_non_phylo_f1: 0.5714\n",
      "  gen_non_phylo_sup: 19.0000\n",
      "  ecology_prec: 0.7500\n",
      "  ecology_rec: 0.4545\n",
      "  ecology_f1: 0.5660\n",
      "  ecology_sup: 33.0000\n",
      "  distance_prec: 0.5625\n",
      "  distance_rec: 0.6000\n",
      "  distance_f1: 0.5806\n",
      "  distance_sup: 15.0000\n",
      "  nuclear_prec: 0.5217\n",
      "  nuclear_rec: 0.8571\n",
      "  nuclear_f1: 0.6486\n",
      "  nuclear_sup: 14.0000\n",
      "  behav_prec: 1.0000\n",
      "  behav_rec: 0.3333\n",
      "  behav_f1: 0.5000\n",
      "  behav_sup: 12.0000\n",
      "  phylogenetic_prec: 0.8108\n",
      "  phylogenetic_rec: 0.6818\n",
      "  phylogenetic_f1: 0.7407\n",
      "  phylogenetic_sup: 44.0000\n",
      "  rank_just_prec: 0.1111\n",
      "  rank_just_rec: 0.0667\n",
      "  rank_just_f1: 0.0833\n",
      "  rank_just_sup: 15.0000\n",
      "  phen_pylo_prec: 0.6000\n",
      "  phen_pylo_rec: 0.3750\n",
      "  phen_pylo_f1: 0.4615\n",
      "  phen_pylo_sup: 8.0000\n",
      "  distance_based_prec: 0.5000\n",
      "  distance_based_rec: 0.8000\n",
      "  distance_based_f1: 0.6154\n",
      "  distance_based_sup: 5.0000\n",
      "  acoustic_prec: 0.7500\n",
      "  acoustic_rec: 0.6000\n",
      "  acoustic_f1: 0.6667\n",
      "  acoustic_sup: 5.0000\n",
      "  phylo_sd_prec: 0.6000\n",
      "  phylo_sd_rec: 0.7500\n",
      "  phylo_sd_f1: 0.6667\n",
      "  phylo_sd_sup: 4.0000\n",
      "  interbreeding_prec: 0.8889\n",
      "  interbreeding_rec: 0.5970\n",
      "  interbreeding_f1: 0.7143\n",
      "  interbreeding_sup: 67.0000\n",
      "  specimen_storage_loc_prec: 0.8148\n",
      "  specimen_storage_loc_rec: 0.7097\n",
      "  specimen_storage_loc_f1: 0.7586\n",
      "  specimen_storage_loc_sup: 93.0000\n",
      "  abbrev_terms_prec: 0.6377\n",
      "  abbrev_terms_rec: 0.5946\n",
      "  abbrev_terms_f1: 0.6154\n",
      "  abbrev_terms_sup: 74.0000\n"
     ]
    }
   ],
   "source": [
    "X_full_train = pd.concat([X_train_raw, X_bias], axis=0)\n",
    "y_full_train = pd.concat([y_train_raw, y_bias], axis=0)\n",
    "\n",
    "final_pipe.fit(X_full_train, y_full_train)\n",
    "\n",
    "# ─── 5) Predict on X_test and enforce the hierarchy ────────────────────────\n",
    "y_pred_test = final_pipe.predict(X_test)   # shape = (n_test, L)\n",
    "\n",
    "# Re‐wrap into a DataFrame so propagate_hierarchy can use column‐names\n",
    "pred_df_test = pd.DataFrame(\n",
    "    y_pred_test,\n",
    "    columns=y_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Enforce your hierarchy (fills with NaN where needed), then binarize/fill\n",
    "pred_df_test = propagate_hierarchy(pred_df_test, classif)\n",
    "pred_df_test = (\n",
    "    pred_df_test\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    "    .reindex(columns=y_test.columns, fill_value=0)\n",
    ")\n",
    "\n",
    "# ─── 6) Compute final metrics on the held‐out fold ─────────────────────────\n",
    "# Assume fold_metrics(y_true_df, y_pred_array) → dict of metrics\n",
    "final_metrics = fold_metrics(y_test, pred_df_test.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348351d-ca03-405e-aa53-bf02c3361a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "with open(\"methods_paper_files/classifier_chain_tune_stats.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cv_results, f)\n",
    "\n",
    "with open(\"methods_paper_files/results/classifier_chain_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pred_df_test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
