{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b33f2900-03db-4d87-a744-49b2ecdf4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Core libraries\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import pickle\n",
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.metrics import log_loss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data handling\n",
    "from torch.utils.data import Sampler, DataLoader\n",
    "import random\n",
    "\n",
    "# Hugging Face libraries\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    precision_recall_fscore_support,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# Multi-label stratified splitting\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, accuracy_score, roc_auc_score\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69efa14-afc5-4383-8ee6-3cfdd82139b7",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a0baf-f8c6-4f78-9f38-72908f3397ad",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We load a dataset with two types of samples:\n",
    "- **Random samples**: Unbiased data collection\n",
    "- **Biased samples**: Oversampled rare classes to help with imbalance\n",
    "\n",
    "The biased samples are only used for training, never for validation or testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4df3752-9d78-415e-b806-9e34431dcfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1845, 72)\n",
      "Number of categories: 38\n",
      "Random samples: 1497\n",
      "Biased samples: 348\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\coding_trial\\testData13052025.csv\"\n",
    "CATEGORIES_PATH = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\data\\categories_of_interest.txt\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "# Mark which samples were randomly collected vs. oversampled\n",
    "# This is crucial to avoid testing on biased data\n",
    "df['is_random'] = np.where(\n",
    "    df.batch.isin(['batch1', 'batch2_m', 'batch2_s', 'batch2_l', 'batch2_j', 'batch3_l']),\n",
    "    1,  # Random sample\n",
    "    0   # Biased sample\n",
    ")\n",
    "\n",
    "df['paper_id'] = [i.split(\".json\")[0] for i in df.id.values]\n",
    "\n",
    "\n",
    "with open(CATEGORIES_PATH, 'r') as file:\n",
    "    cats = json.load(file)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of categories: {len(cats)}\")\n",
    "print(f\"Random samples: {(df['is_random'] == 1).sum()}\")\n",
    "print(f\"Biased samples: {(df['is_random'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d825c67-ba6e-4a4f-b29c-76a54eee9acc",
   "metadata": {},
   "source": [
    "## Create Train/Validation/Test Split\n",
    "\n",
    "We use a careful splitting strategy:\n",
    "- **Test set (20%)**: Only random samples - for unbiased evaluation\n",
    "- **Validation set (10%)**: Only random samples - for model selection\n",
    "- **Training set (70% + biased)**: Random samples + ALL biased samples\n",
    "\n",
    "This ensures our evaluation is unbiased while maximizing training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddba4c66-ab85-42ef-88af-e413bd51bdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset split:\n",
      "- Training: 1390 samples (including 348 biased)\n",
      "- Validation: 151 samples (all unbiased)\n",
      "- Test: 304 samples (all unbiased)\n",
      "\n",
      "Class frequencies in training set:\n",
      "- Min: 40\n",
      "- Max: 1165\n",
      "- Mean: 317.7\n"
     ]
    }
   ],
   "source": [
    "# Separate random and biased data\n",
    "rand_df = df[df['is_random'] == 1]\n",
    "biased_df = df[df['is_random'] == 0]\n",
    "\n",
    "X_rand, y_rand = rand_df['displayed_text'], rand_df[cats]\n",
    "X_bias, y_bias = biased_df['displayed_text'], biased_df[cats]\n",
    "\n",
    "# First split: 80/20 for train+val/test from random data\n",
    "msss_test = MultilabelStratifiedShuffleSplit(test_size=0.20, random_state=42, n_splits=1)\n",
    "train_val_idx, test_idx = next(msss_test.split(X_rand, y_rand))\n",
    "\n",
    "X_test, y_test = X_rand.iloc[test_idx], y_rand.iloc[test_idx]\n",
    "\n",
    "# Second split: 87.5/12.5 for train/val from remaining (gives us 70/10/20 overall)\n",
    "X_train_val, y_train_val = X_rand.iloc[train_val_idx], y_rand.iloc[train_val_idx]\n",
    "msss_val = MultilabelStratifiedShuffleSplit(test_size=0.125, random_state=42, n_splits=1)\n",
    "train_idx, val_idx = next(msss_val.split(X_train_val, y_train_val))\n",
    "\n",
    "X_train_rand, y_train_rand = X_train_val.iloc[train_idx], y_train_val.iloc[train_idx]\n",
    "X_val, y_val = X_train_val.iloc[val_idx], y_train_val.iloc[val_idx]\n",
    "\n",
    "# Combine random training data with ALL biased data\n",
    "X_train = pd.concat([X_train_rand, X_bias])\n",
    "y_train = pd.concat([y_train_rand, y_bias])\n",
    "\n",
    "# Calculate class frequencies (crucial for DB loss)\n",
    "class_frequencies = y_train.sum(axis=0).values\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"- Training: {len(X_train)} samples (including {len(X_bias)} biased)\")\n",
    "print(f\"- Validation: {len(X_val)} samples (all unbiased)\")\n",
    "print(f\"- Test: {len(X_test)} samples (all unbiased)\")\n",
    "print(f\"\\nClass frequencies in training set:\")\n",
    "print(f\"- Min: {class_frequencies.min()}\")\n",
    "print(f\"- Max: {class_frequencies.max()}\")\n",
    "print(f\"- Mean: {class_frequencies.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8ac1119-3ff3-4ffc-abf6-89ec4a5ab584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the frequencies\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(class_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47cdf4cb-6081-4c72-8433-4a2b46ad361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data for upload\n",
    "# use this for LR and GPT as well\n",
    "\n",
    "upload_data_scibert = {'training data':{'X_train': X_train, 'y_train':y_train, 'X_train_rand':X_train_rand, 'X_bias':X_bias, 'y_train_rand':y_train_rand, 'y_bias':y_bias, 'train_idx':train_idx},\n",
    "              'validation data': {'X_val':X_val, 'y_val':y_val, 'val_idx':val_idx},\n",
    "              'test data':{'X_test':X_test, 'y_test':y_test, 'test_idx':test_idx}, \n",
    "              'og_data': df,\n",
    "              'class_frequencies':class_frequencies,\n",
    "               'labels':cats\n",
    "              }\n",
    "\n",
    "with open(\"methods_paper_files/upload_data_scibert.pkl\", \"wb\") as f:\n",
    "    pickle.dump(upload_data_scibert, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33bf405-8bad-4785-9dfc-0b0ba7f087ce",
   "metadata": {},
   "source": [
    "## Create Label Mappings and HuggingFace Datasets\n",
    "\n",
    "Convert our pandas DataFrames to HuggingFace Dataset format for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29be818f-f91a-4e47-8cb9-ab5ec3ee5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mappings\n",
    "labels = cats\n",
    "id2label = {idx: label for idx, label in enumerate(labels)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "train_data = {'text': X_train.tolist(), 'labels': y_train.values.tolist()}\n",
    "val_data = {'text': X_val.tolist(), 'labels': y_val.values.tolist()}\n",
    "test_data = {'text': X_test.tolist(), 'labels': y_test.values.tolist()}\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict(train_data),\n",
    "    'validation': Dataset.from_dict(val_data),\n",
    "    'test': Dataset.from_dict(test_data)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a771a20-129b-4d11-aeba-5716c1823689",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We tokenize the text using the appropriate tokenizer for our mod. elWtest both scibert and distilbert. .\n",
    "SciBERT is optimized for scientific text, while DistilBERT is faster and smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9118278-7dcf-4819-bca6-4f9d8406b5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for allenai/scibert_scivocab_uncased...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16bf18c25a34037b71779073f38ad2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d355b6d80c9b4955a88b9776dd97c901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fdf53e526d44beae6d77de8cc8535b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose model - SciBERT for scientific text or DistilBERT for general/faster training\n",
    "MODEL_NAME = \"allenai/scibert_scivocab_uncased\"  # or \"distilbert-base-uncased\" for the lighter model\n",
    "\n",
    "print(f\"Loading tokenizer for {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "\n",
    "    # Tokenize text\n",
    "    encoding = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",    \n",
    "        truncation=True,          # Truncate if longer than max_length\n",
    "        max_length=512           # or lower for efficiency\n",
    "    )\n",
    "    \n",
    "    encoding[\"labels\"] = examples[\"labels\"]\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "# Apply preprocessing\n",
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=['text'])\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a7b9e2-bab0-459d-8f5f-cf815d806084",
   "metadata": {},
   "source": [
    "# 3. Sampling Strategy\n",
    "\n",
    "Standard random sampling can lead to batches with only common classes.\n",
    "Our custom sampler ensures each batch contains diverse classes, giving\n",
    "rare classes more exposure during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4987cfed-4dd1-4726-b1fc-4f7abbf8c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassAwareSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Custom sampler that ensures balanced class representation in each batch.\n",
    "    \n",
    "    How it works:\n",
    "    1. For each epoch, shuffle all classes\n",
    "    2. Go through classes in groups of batch_size\n",
    "    3. For each class, randomly sample one instance that has that label\n",
    "    4. This ensures every batch sees diverse classes, not just common ones\n",
    "    \n",
    "    Based on Huang et al. (2021) for multi-label classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, label_matrix, batch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            label_matrix: numpy array [N, C] with binary labels\n",
    "            batch_size: number of samples per batch\n",
    "        \"\"\"\n",
    "        self.label_matrix = label_matrix\n",
    "        self.batch_size = batch_size\n",
    "        self.n_samples, self.n_classes = label_matrix.shape\n",
    "        \n",
    "        # Build mapping: class_id -> [sample indices that have this class]\n",
    "        self.cls2idx = {}\n",
    "        for c in range(self.n_classes):\n",
    "            indices = np.where(label_matrix[:, c] == 1)[0].tolist()\n",
    "            if indices:  # Only include classes that have at least one sample\n",
    "                self.cls2idx[c] = indices\n",
    "        \n",
    "        self.classes = list(self.cls2idx.keys())\n",
    "        print(f\"ClassAwareSampler initialized:\")\n",
    "        print(f\"  - {len(self.classes)} classes with samples\")\n",
    "        print(f\"  - Batch size: {batch_size}\")\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate indices for one epoch.\"\"\"\n",
    "        indices = []\n",
    "        \n",
    "        # Shuffle classes for this epoch\n",
    "        shuffled_classes = self.classes.copy()\n",
    "        random.shuffle(shuffled_classes)\n",
    "        \n",
    "        # Process classes in chunks of batch_size\n",
    "        for i in range(0, len(shuffled_classes), self.batch_size):\n",
    "            batch_classes = shuffled_classes[i:i + self.batch_size]\n",
    "            \n",
    "            # For each class in this batch, sample one instance\n",
    "            for cls in batch_classes:\n",
    "                idx = random.choice(self.cls2idx[cls])\n",
    "                indices.append(idx)\n",
    "        \n",
    "        # Continue sampling until we have enough for full epoch\n",
    "        epoch_size = self.n_samples\n",
    "        while len(indices) < epoch_size:\n",
    "            random.shuffle(shuffled_classes)\n",
    "            for cls in shuffled_classes:\n",
    "                if len(indices) >= epoch_size:\n",
    "                    break\n",
    "                idx = random.choice(self.cls2idx[cls])\n",
    "                indices.append(idx)\n",
    "                \n",
    "        return iter(indices[:epoch_size])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples in one epoch.\"\"\"\n",
    "        return self.n_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012dc78d-72ff-4256-9ada-52b9cecb71cc",
   "metadata": {},
   "source": [
    "# 4. Distribution Balanced Loss Implementation\n",
    "\n",
    "Inspired by mostly [this paper](https://arxiv.org/abs/2109.04712), but also [this paper](https://link.springer.com/chapter/10.1007/978-3-030-58548-8_10) and [this post](https://www.kdnuggets.com/2023/03/multilabel-nlp-analysis-class-imbalance-loss-function-approaches.html). Just the custom loss is probably better than oversampling, because when you oversample sparse labels you also oversample common labels, which messes up that balance as well. We do use class aware sampling, such that each bach has at least one positive of each label (if necessary with oversampling), just like in Huang et al.\n",
    "\n",
    "This is the core innovation for handling imbalanced multi-label classification.\n",
    "The DB loss combines three techniques:\n",
    "\n",
    "1. **Rebalanced Weighting**: Addresses label co-occurrence\n",
    "2. **Focal Loss**: Focuses on hard-to-classify examples\n",
    "3. **Negative-Tolerant Regularization**: Handles class ince. We experimented with label smoothing, but decided to stick to how the paper does itasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd8208cc-278b-493a-974e-efef4dc3d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionBalancedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Distribution-Balanced (DB) loss from Wu & Huang (2020) and Huang et al. (2021)\n",
    "    with backward-compatible keyword aliases:\n",
    "        negative_tolerant_lambda → negative_lambda\n",
    "        negative_tolerant_kappa  → negative_kappa\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 class_freq,\n",
    "                 rebalance_alpha=0.1, rebalance_beta=10.0, rebalance_mu=0.7,\n",
    "                 focal_gamma=2.0,\n",
    "                 negative_lambda=2.0, negative_kappa=0.05,\n",
    "                 label_smoothing=0.0,\n",
    "                 **kwargs):                         # <- catch legacy names\n",
    "        # --- map legacy keywords, if present ---\n",
    "        if \"negative_tolerant_lambda\" in kwargs:\n",
    "            negative_lambda = kwargs.pop(\"negative_tolerant_lambda\")\n",
    "        if \"negative_tolerant_kappa\" in kwargs:\n",
    "            negative_kappa = kwargs.pop(\"negative_tolerant_kappa\")\n",
    "\n",
    "        # raise on any truly unknown kwarg\n",
    "        if kwargs:\n",
    "            unknown = \", \".join(kwargs.keys())\n",
    "            raise TypeError(f\"Unexpected keyword argument(s): {unknown}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # store class frequencies\n",
    "        self.register_buffer(\"class_freq\",\n",
    "                             torch.tensor(class_freq, dtype=torch.float32))\n",
    "        self.m = len(class_freq)\n",
    "\n",
    "        # hyper-parameters\n",
    "        self.alpha  = rebalance_alpha\n",
    "        self.beta   = rebalance_beta\n",
    "        self.mu     = rebalance_mu\n",
    "        self.gamma  = focal_gamma\n",
    "        self.lmbda  = negative_lambda\n",
    "        self.kappa  = negative_kappa\n",
    "        self.eps    = 1e-7\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "        # negative-tolerant logit shift\n",
    "        p_c = torch.clamp(self.class_freq / self.class_freq.sum(), 1e-7, 1-1e-7)\n",
    "        self.register_buffer(\"v_i\", -self.kappa * torch.log(p_c / (1 - p_c)))\n",
    "\n",
    "    # ------------- helper functions (unchanged) -------------\n",
    "    def _smooth(self, y):\n",
    "        if self.label_smoothing <= 0:\n",
    "            return y\n",
    "        return y * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "\n",
    "    def _rebalanced_weights(self, y):\n",
    "        inv_n   = (1.0 / self.class_freq).unsqueeze(0)\n",
    "        pos_m   = y > 0\n",
    "        P_I     = (pos_m * inv_n / self.m).sum(1, keepdim=True) + self.eps\n",
    "        P_C     = inv_n / self.m\n",
    "        r_db    = P_C / P_I\n",
    "        w       = self.alpha + torch.sigmoid(self.beta * (r_db - self.mu))\n",
    "        return torch.where(pos_m, w, torch.ones_like(w))\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        y        = y.float()\n",
    "        v_i      = self.v_i.to(logits.device)\n",
    "        inv_l    = 1.0 / self.lmbda\n",
    "        y_smooth = self._smooth(y)\n",
    "        w        = self._rebalanced_weights(y).to(logits.device)\n",
    "\n",
    "        # positives\n",
    "        q_pos    = torch.sigmoid(logits - v_i)\n",
    "        loss_pos = -w * ((1 - q_pos)**self.gamma) * (\n",
    "                     y_smooth * torch.log(q_pos + self.eps) +\n",
    "                     (1 - y_smooth) * torch.log(1 - q_pos + self.eps))\n",
    "\n",
    "        # negatives\n",
    "        q_neg    = torch.sigmoid(self.lmbda * (logits - v_i))\n",
    "        loss_neg = -inv_l * (q_neg**self.gamma) * (\n",
    "                     y_smooth * torch.log(q_neg + self.eps) +\n",
    "                     (1 - y_smooth) * torch.log(1 - q_neg + self.eps))\n",
    "\n",
    "        loss = torch.where(y > 0, loss_pos, loss_neg)\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c1023-4c1d-447e-9364-b87930a04492",
   "metadata": {},
   "source": [
    "# 5. Model Architecture\n",
    "\n",
    "We wrap the BERT model with our custom loss function.\n",
    "The model can use either DB loss or standard BCE for compariso.n If compute allows, try out the standard BCE for compariso.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa206e1b-9ab9-4f3a-86ae-eaba68524a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelBERTClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT-based model for multi-label classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - BERT encoder (frozen initially)\n",
    "    - Linear classifier head\n",
    "    - Custom loss function (DB loss or BCE)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_labels: int,\n",
    "        class_frequencies,\n",
    "        id2label: dict,\n",
    "        label2id: dict,\n",
    "        use_db_loss: bool = True,\n",
    "        pos_weight=None  # For weighted BCE baseline\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            problem_type=\"multi_label_classification\",\n",
    "        )\n",
    "        \n",
    "        # Choose loss function\n",
    "        if use_db_loss:\n",
    "            print(\"Using Distribution Balanced Loss\")\n",
    "            self.loss_fn = DistributionBalancedLoss(\n",
    "                class_freq=class_frequencies,\n",
    "                label_smoothing=0.05,      # 5% label smoothing\n",
    "                rebalance_alpha=0.1,       # Conservative rebalancing\n",
    "                rebalance_beta=10,         # Moderate sharpness\n",
    "                rebalance_mu=0.7,          # Slightly favor common classes\n",
    "                focal_gamma=2.0,           # Standard focal loss\n",
    "                negative_tolerant_lambda=2.0,\n",
    "                negative_tolerant_kappa=0.05,\n",
    "            )\n",
    "        else:\n",
    "            print(\"Using Binary Cross Entropy Loss\")\n",
    "            if pos_weight is not None:\n",
    "                pos_weight = torch.tensor(pos_weight, dtype=torch.float32)\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None, **extra):\n",
    "        \"\"\"Forward pass through BERT and calculate loss.\"\"\"\n",
    "        # Get BERT outputs\n",
    "        logits = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=None,  # We calculate loss ourselves\n",
    "            **extra\n",
    "        ).logits\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels.float())\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb212d0a-b23c-4bb3-87df-6f3a09f09fbf",
   "metadata": {},
   "source": [
    "# 6. Custom Trainer\n",
    "\n",
    "We need custom trainers to:\n",
    "1. Handle our custom model outputs\n",
    "2. Implement gradual unfreezing\n",
    "3. Use class-aware sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68e7d5c8-0557-4a58-80e0-02fa313fdeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _contiguous_state_dict(model):\n",
    "    \"\"\"Ensure all tensors are contiguous (required for PyTorch >= 2.1).\"\"\"\n",
    "    sd = model.state_dict()\n",
    "    for k, v in sd.items():\n",
    "        if isinstance(v, torch.Tensor) and not v.is_contiguous():\n",
    "            sd[k] = v.contiguous()\n",
    "    return sd\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Base trainer that handles our custom model format.\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Extract labels and compute loss using our custom loss function.\"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # Handle gradient accumulation scaling if needed\n",
    "        if 'num_items_in_batch' in kwargs and kwargs['num_items_in_batch'] is not None:\n",
    "            actual_batch_size = inputs['input_ids'].size(0)\n",
    "            if actual_batch_size != kwargs['num_items_in_batch']:\n",
    "                loss = loss * (actual_batch_size / kwargs['num_items_in_batch'])\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def save_model(self, output_dir: str = None, _internal_call=False):\n",
    "        \"\"\"Save model with proper handling of our custom architecture.\"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        state_dict = _contiguous_state_dict(self.model)\n",
    "        \n",
    "        # Save model weights\n",
    "        torch.save(state_dict, os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        \n",
    "        # Save config from underlying BERT model\n",
    "        if hasattr(self.model, \"bert\") and hasattr(self.model.bert, \"config\"):\n",
    "            self.model.bert.config.save_pretrained(output_dir)\n",
    "        \n",
    "        # Save tokenizer\n",
    "        if self.tokenizer is not None:\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        # Save training args\n",
    "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        \"\"\"Handle prediction step for our custom model.\"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            logits = outputs['logits']\n",
    "        \n",
    "        if prediction_loss_only:\n",
    "            return (loss, None, None)\n",
    "        \n",
    "        return (loss, logits, labels)\n",
    "\n",
    "class UltraConservativeTrainer(CustomTrainer):\n",
    "    \"\"\"\n",
    "    Implements gradual unfreezing strategy:\n",
    "    1. Train only classifier head for N epochs\n",
    "    2. Unfreeze top BERT layers and continue training\n",
    "    \n",
    "    This prevents catastrophic forgetting and improves stability.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args,\n",
    "                 n_frozen_epochs: int = 8,\n",
    "                 n_unfrozen_layers: int = 2,\n",
    "                 **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_frozen_epochs = n_frozen_epochs\n",
    "        self.n_unfrozen_layers = n_unfrozen_layers\n",
    "        self._initialised = False\n",
    "    \n",
    "    def _encoder_layers(self):\n",
    "        \"\"\"Get encoder layers (works for both BERT and DistilBERT).\"\"\"\n",
    "        m = self.model.bert\n",
    "        \n",
    "        if hasattr(m, \"distilbert\"):  # DistilBERT\n",
    "            return m.distilbert.transformer.layer\n",
    "        elif hasattr(m, \"encoder\"):  # BERT/SciBERT\n",
    "            return m.encoder.layer\n",
    "        elif hasattr(m, \"bert\") and hasattr(m.bert, \"encoder\"):\n",
    "            return m.bert.encoder.layer\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoder structure for {type(m)}\")\n",
    "    \n",
    "    def _freeze_all_encoder(self):\n",
    "        \"\"\"Freeze all encoder layers, keep only classifier trainable.\"\"\"\n",
    "        # Freeze encoder\n",
    "        for param in self._encoder_layers().parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Keep classifier trainable\n",
    "        for param in self.model.bert.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        # Print statistics\n",
    "        total = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"[FREEZE] Training {trainable:,}/{total:,} parameters \"\n",
    "              f\"({100*trainable/total:.1f}%) for first {self.n_frozen_epochs} epochs\")\n",
    "    \n",
    "    def _unfreeze_top_layers(self):\n",
    "        \"\"\"Unfreeze top N encoder layers for fine-tuning.\"\"\"\n",
    "        layers = self._encoder_layers()\n",
    "        start = len(layers) - self.n_unfrozen_layers\n",
    "        \n",
    "        for i in range(start, len(layers)):\n",
    "            for p in layers[i].parameters():\n",
    "                p.requires_grad = True\n",
    "        \n",
    "        # Print statistics\n",
    "        total = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"[UNFREEZE] Top {self.n_unfrozen_layers} layers now trainable → \"\n",
    "              f\"{trainable:,}/{total:,} parameters ({100*trainable/total:.1f}%)\")\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\"Freeze encoder at start of training.\"\"\"\n",
    "        if not self._initialised:\n",
    "            self._freeze_all_encoder()\n",
    "            self._initialised = True\n",
    "        return super().on_train_begin(args, state, control, **kwargs)\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Unfreeze layers after N epochs.\"\"\"\n",
    "        epoch = int(state.epoch)\n",
    "        if epoch == self.n_frozen_epochs:\n",
    "            self._unfreeze_top_layers()\n",
    "            # Reduce learning rate when unfreezing\n",
    "            for pg in self.optimizer.param_groups:\n",
    "                pg[\"lr\"] *= 0.1\n",
    "                print(f\"[LR] Reduced learning rate to {pg['lr']:.2e}\")\n",
    "        return super().on_epoch_end(args, state, control, **kwargs)\n",
    "\n",
    "class UltraConservativeTrainerWithSampler(UltraConservativeTrainer):\n",
    "    \"\"\"\n",
    "    Combines gradual unfreezing with class-aware sampling.\n",
    "    \"\"\"\n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"Override to use our ClassAwareSampler.\"\"\"\n",
    "        # Extract label matrix\n",
    "        train_labels = []\n",
    "        for i in range(len(self.train_dataset)):\n",
    "            labels = self.train_dataset[i]['labels']\n",
    "            if isinstance(labels, torch.Tensor):\n",
    "                train_labels.append(labels.numpy())\n",
    "            else:\n",
    "                train_labels.append(labels)\n",
    "        \n",
    "        train_labels = np.array(train_labels)\n",
    "        \n",
    "        # Create sampler\n",
    "        train_sampler = ClassAwareSampler(\n",
    "            label_matrix=train_labels,\n",
    "            batch_size=self.args.per_device_train_batch_size\n",
    "        )\n",
    "        \n",
    "        # Return custom DataLoader\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ab7d199-9087-4db4-ae01-3c827f9071f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callbacks for learning rate scheduling: we use a different lr for the first two epochs\n",
    "\n",
    "class LRDropCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Drops learning rate to a fixed value at a specific epoch.\n",
    "    Used when unfreezing BERT layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, epoch_drop: int, new_lr: float):\n",
    "        super().__init__()\n",
    "        self.epoch_drop = epoch_drop\n",
    "        self.new_lr = new_lr\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        if int(state.epoch) == self.epoch_drop:\n",
    "            optimizer = kwargs[\"optimizer\"]\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] = self.new_lr\n",
    "            print(f\"[LRDropCallback] Epoch {self.epoch_drop}: Set LR → {self.new_lr:.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2f9f7-0730-4e72-a4e9-c4561c5cc284",
   "metadata": {},
   "source": [
    "# 7. Evaluation Metrics and Threshold Tuning\n",
    "\n",
    "For multi-label classification, we need:\n",
    "1. Per-class optimal thresholds (not just 0.5)\n",
    "2. Separate metrics for head/medium/tail class\n",
    "\n",
    "\n",
    "We experimented with tuning thresholds, but instead opt for temperature scaling. This is more conservative, and less likely to overfit on our small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7522ac67-4e4e-427e-827f-72a8206eece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(\n",
    "    predictions,\n",
    "    labels,\n",
    "    threshold=0.5,          \n",
    "    label_names=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate global and per-label metrics with a fixed decision threshold.\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(torch.as_tensor(predictions)).numpy()\n",
    "\n",
    "    # a single scalar → broadcast to shape (n_labels,)\n",
    "    y_pred = (probs >= threshold).astype(int)\n",
    "    y_true = labels\n",
    "\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    # head / medium / tail as before …\n",
    "    idx_sorted = np.argsort(class_frequencies)[::-1]\n",
    "    n = len(class_frequencies)\n",
    "    head_f1   = f1[idx_sorted[: n // 3]].mean()\n",
    "    medium_f1 = f1[idx_sorted[n // 3 : 2 * n // 3]].mean()\n",
    "    tail_f1   = f1[idx_sorted[2 * n // 3 :]].mean()\n",
    "\n",
    "    per_label = {}\n",
    "    for i in range(len(f1)):\n",
    "        name = label_names[i] if label_names is not None else str(i)\n",
    "        per_label[name] = {\n",
    "            \"precision\": float(prec[i]),\n",
    "            \"recall\":    float(rec[i]),\n",
    "            \"f1\":        float(f1[i]),\n",
    "            \"threshold\": float(threshold),      # always 0.5\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"macro_f1\":  macro_f1,\n",
    "        \"micro_f1\":  micro_f1,\n",
    "        \"head_f1\":   head_f1,\n",
    "        \"medium_f1\": medium_f1,\n",
    "        \"tail_f1\":   tail_f1,\n",
    "        \"threshold\": threshold,\n",
    "        \"per_label\": per_label,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    result = multi_label_metrics(predictions, labels, threshold=0.5)\n",
    "    return {\n",
    "        'macro_f1':  result['macro_f1'],\n",
    "        'micro_f1':  result['micro_f1'],\n",
    "        'head_f1':   result['head_f1'],\n",
    "        'medium_f1': result['medium_f1'],\n",
    "        'tail_f1':   result['tail_f1'],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# function for the temperature scaling\n",
    "def fit_temperature(val_logits, val_labels, clip=1e-7):\n",
    "    \"\"\"\n",
    "    Scalar temperature T   (T > 0)   via NLL minimisation.\n",
    "    \"\"\"\n",
    "    logits = torch.as_tensor(val_logits, dtype=torch.float32)\n",
    "    y_true = val_labels.astype(np.float32)\n",
    "\n",
    "    # manual BCE\n",
    "    def nll(T):\n",
    "        probs = torch.sigmoid(logits / T).numpy()\n",
    "        probs = np.clip(probs, clip, 1.0 - clip)          # numerical safety\n",
    "        bce   = -(y_true * np.log(probs) +\n",
    "                  (1.0 - y_true) * np.log(1.0 - probs))\n",
    "        return bce.mean()\n",
    "\n",
    "    res = minimize_scalar(nll, bounds=(0.5, 5.0), method=\"bounded\")\n",
    "    return res.x\n",
    "\n",
    "#apply T at inference\n",
    "def make_hard_preds(logits, T, thresh=0.5):\n",
    "    \"\"\"\n",
    "    logits ➜ probs(T‑scaled) ➜ binary mask\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(torch.as_tensor(logits) / T).numpy()\n",
    "    return (probs >= thresh).astype(int), probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6290383-0389-4344-a419-7821ea928940",
   "metadata": {},
   "source": [
    "# 8. Training & small search for best learning rates\n",
    "\n",
    "We don't have the compute for an extensive hyperparameter grid search. Hence, we try a 2x2 grid of learning rates for epochs 0 & 1 (lr1) and all the remaining epochs (lr2).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ba0a0ee-6049-4c21-98d7-5a7eafd64760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 2 x 2 = 4 configurations\n",
      "\n",
      "============================================================\n",
      "Configuration: Frozen LR=5e-05, Unfrozen LR=1e-05\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Distribution Balanced Loss\n",
      "\n",
      "Starting training...\n",
      "ClassAwareSampler initialized:\n",
      "  - 38 classes with samples\n",
      "  - Batch size: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='860' max='860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [860/860 8:17:24, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Head F1</th>\n",
       "      <th>Medium F1</th>\n",
       "      <th>Tail F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.010341</td>\n",
       "      <td>0.269122</td>\n",
       "      <td>0.561850</td>\n",
       "      <td>0.375689</td>\n",
       "      <td>0.439875</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>0.009962</td>\n",
       "      <td>0.570086</td>\n",
       "      <td>0.766682</td>\n",
       "      <td>0.655145</td>\n",
       "      <td>0.738323</td>\n",
       "      <td>0.323332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.663775</td>\n",
       "      <td>0.788973</td>\n",
       "      <td>0.711935</td>\n",
       "      <td>0.766396</td>\n",
       "      <td>0.516697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>0.691125</td>\n",
       "      <td>0.797416</td>\n",
       "      <td>0.741452</td>\n",
       "      <td>0.754928</td>\n",
       "      <td>0.580867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>0.719455</td>\n",
       "      <td>0.808118</td>\n",
       "      <td>0.746586</td>\n",
       "      <td>0.774302</td>\n",
       "      <td>0.639564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.750397</td>\n",
       "      <td>0.834653</td>\n",
       "      <td>0.795996</td>\n",
       "      <td>0.801083</td>\n",
       "      <td>0.657620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>0.746061</td>\n",
       "      <td>0.844560</td>\n",
       "      <td>0.827958</td>\n",
       "      <td>0.808202</td>\n",
       "      <td>0.608322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>0.753038</td>\n",
       "      <td>0.844813</td>\n",
       "      <td>0.816458</td>\n",
       "      <td>0.808381</td>\n",
       "      <td>0.639154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>0.745229</td>\n",
       "      <td>0.840491</td>\n",
       "      <td>0.813993</td>\n",
       "      <td>0.787541</td>\n",
       "      <td>0.639442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>0.010267</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.841194</td>\n",
       "      <td>0.820075</td>\n",
       "      <td>0.798166</td>\n",
       "      <td>0.660534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LRDropCallback] Epoch 2: Set LR → 1.0e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 01:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Macro F1: 0.7580\n",
      "Training completed in 29893 seconds\n",
      "\n",
      "============================================================\n",
      "Configuration: Frozen LR=5e-05, Unfrozen LR=2e-05\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Distribution Balanced Loss\n",
      "\n",
      "Starting training...\n",
      "ClassAwareSampler initialized:\n",
      "  - 38 classes with samples\n",
      "  - Batch size: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='860' max='860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [860/860 7:46:06, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Head F1</th>\n",
       "      <th>Medium F1</th>\n",
       "      <th>Tail F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>0.313163</td>\n",
       "      <td>0.617965</td>\n",
       "      <td>0.446514</td>\n",
       "      <td>0.490412</td>\n",
       "      <td>0.012821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>0.010429</td>\n",
       "      <td>0.599548</td>\n",
       "      <td>0.777932</td>\n",
       "      <td>0.699039</td>\n",
       "      <td>0.710786</td>\n",
       "      <td>0.396473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>0.008009</td>\n",
       "      <td>0.651647</td>\n",
       "      <td>0.803730</td>\n",
       "      <td>0.742832</td>\n",
       "      <td>0.775761</td>\n",
       "      <td>0.443362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.678156</td>\n",
       "      <td>0.819804</td>\n",
       "      <td>0.775146</td>\n",
       "      <td>0.772868</td>\n",
       "      <td>0.493915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.008957</td>\n",
       "      <td>0.694312</td>\n",
       "      <td>0.813147</td>\n",
       "      <td>0.764896</td>\n",
       "      <td>0.751155</td>\n",
       "      <td>0.572313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.009874</td>\n",
       "      <td>0.727509</td>\n",
       "      <td>0.836813</td>\n",
       "      <td>0.813556</td>\n",
       "      <td>0.792336</td>\n",
       "      <td>0.583256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.009206</td>\n",
       "      <td>0.747097</td>\n",
       "      <td>0.845550</td>\n",
       "      <td>0.816324</td>\n",
       "      <td>0.816604</td>\n",
       "      <td>0.613687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.009719</td>\n",
       "      <td>0.744336</td>\n",
       "      <td>0.846620</td>\n",
       "      <td>0.824346</td>\n",
       "      <td>0.819504</td>\n",
       "      <td>0.595313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.760824</td>\n",
       "      <td>0.851375</td>\n",
       "      <td>0.825856</td>\n",
       "      <td>0.821032</td>\n",
       "      <td>0.640587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.010364</td>\n",
       "      <td>0.759028</td>\n",
       "      <td>0.848976</td>\n",
       "      <td>0.827779</td>\n",
       "      <td>0.810996</td>\n",
       "      <td>0.643598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LRDropCallback] Epoch 2: Set LR → 2.0e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 01:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Macro F1: 0.7608\n",
      "Training completed in 27999 seconds\n",
      "\n",
      "============================================================\n",
      "Configuration: Frozen LR=3e-05, Unfrozen LR=1e-05\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Distribution Balanced Loss\n",
      "\n",
      "Starting training...\n",
      "ClassAwareSampler initialized:\n",
      "  - 38 classes with samples\n",
      "  - Batch size: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='860' max='860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [860/860 7:56:25, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Head F1</th>\n",
       "      <th>Medium F1</th>\n",
       "      <th>Tail F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.011283</td>\n",
       "      <td>0.279658</td>\n",
       "      <td>0.574492</td>\n",
       "      <td>0.367969</td>\n",
       "      <td>0.477797</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.009529</td>\n",
       "      <td>0.495395</td>\n",
       "      <td>0.737255</td>\n",
       "      <td>0.669278</td>\n",
       "      <td>0.601191</td>\n",
       "      <td>0.229093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.008841</td>\n",
       "      <td>0.626201</td>\n",
       "      <td>0.789963</td>\n",
       "      <td>0.723329</td>\n",
       "      <td>0.762221</td>\n",
       "      <td>0.400523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073500</td>\n",
       "      <td>0.008156</td>\n",
       "      <td>0.645292</td>\n",
       "      <td>0.790436</td>\n",
       "      <td>0.724833</td>\n",
       "      <td>0.750686</td>\n",
       "      <td>0.466474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>0.665773</td>\n",
       "      <td>0.807356</td>\n",
       "      <td>0.768149</td>\n",
       "      <td>0.747788</td>\n",
       "      <td>0.489257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>0.712143</td>\n",
       "      <td>0.826948</td>\n",
       "      <td>0.799174</td>\n",
       "      <td>0.777737</td>\n",
       "      <td>0.566214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.008704</td>\n",
       "      <td>0.696395</td>\n",
       "      <td>0.827647</td>\n",
       "      <td>0.795647</td>\n",
       "      <td>0.773407</td>\n",
       "      <td>0.527765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.009338</td>\n",
       "      <td>0.698915</td>\n",
       "      <td>0.826678</td>\n",
       "      <td>0.801320</td>\n",
       "      <td>0.761064</td>\n",
       "      <td>0.542237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>0.718850</td>\n",
       "      <td>0.834061</td>\n",
       "      <td>0.812394</td>\n",
       "      <td>0.777364</td>\n",
       "      <td>0.573987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.720181</td>\n",
       "      <td>0.835289</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>0.779537</td>\n",
       "      <td>0.572238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LRDropCallback] Epoch 2: Set LR → 1.0e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 01:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Macro F1: 0.7202\n",
      "Training completed in 28618 seconds\n",
      "\n",
      "============================================================\n",
      "Configuration: Frozen LR=3e-05, Unfrozen LR=2e-05\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Distribution Balanced Loss\n",
      "\n",
      "Starting training...\n",
      "ClassAwareSampler initialized:\n",
      "  - 38 classes with samples\n",
      "  - Batch size: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='860' max='860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [860/860 8:04:28, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Head F1</th>\n",
       "      <th>Medium F1</th>\n",
       "      <th>Tail F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>0.011283</td>\n",
       "      <td>0.279658</td>\n",
       "      <td>0.574492</td>\n",
       "      <td>0.367969</td>\n",
       "      <td>0.477797</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.009529</td>\n",
       "      <td>0.495395</td>\n",
       "      <td>0.737255</td>\n",
       "      <td>0.669278</td>\n",
       "      <td>0.601191</td>\n",
       "      <td>0.229093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.008841</td>\n",
       "      <td>0.626201</td>\n",
       "      <td>0.789963</td>\n",
       "      <td>0.723329</td>\n",
       "      <td>0.762221</td>\n",
       "      <td>0.400523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.008162</td>\n",
       "      <td>0.646648</td>\n",
       "      <td>0.791569</td>\n",
       "      <td>0.726375</td>\n",
       "      <td>0.753227</td>\n",
       "      <td>0.466474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.665773</td>\n",
       "      <td>0.807356</td>\n",
       "      <td>0.768149</td>\n",
       "      <td>0.747788</td>\n",
       "      <td>0.489257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.008922</td>\n",
       "      <td>0.708444</td>\n",
       "      <td>0.826948</td>\n",
       "      <td>0.798453</td>\n",
       "      <td>0.781147</td>\n",
       "      <td>0.552656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.008692</td>\n",
       "      <td>0.696722</td>\n",
       "      <td>0.827128</td>\n",
       "      <td>0.794205</td>\n",
       "      <td>0.768645</td>\n",
       "      <td>0.534816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.009336</td>\n",
       "      <td>0.698187</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.801320</td>\n",
       "      <td>0.761064</td>\n",
       "      <td>0.540109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.718517</td>\n",
       "      <td>0.833697</td>\n",
       "      <td>0.811340</td>\n",
       "      <td>0.777364</td>\n",
       "      <td>0.573987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.720299</td>\n",
       "      <td>0.835795</td>\n",
       "      <td>0.816524</td>\n",
       "      <td>0.779537</td>\n",
       "      <td>0.572238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LRDropCallback] Epoch 2: Set LR → 2.0e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Macro F1: 0.7203\n",
      "Training completed in 29103 seconds\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter configurations\n",
    "# should also try a couple of seeds\n",
    "FROZEN_LRS = [5e-5,3e-5]    # Learning rates for frozen phase\n",
    "UNFROZEN_LRS = [1e-5,2e-5]  # Learning rates for unfrozen phase\n",
    "\n",
    "# Store results\n",
    "results = defaultdict(list)\n",
    "\n",
    "print(f\"Testing {len(FROZEN_LRS)} x {len(UNFROZEN_LRS)} = {len(FROZEN_LRS) * len(UNFROZEN_LRS)} configurations\")\n",
    "\n",
    "for lr_frozen, lr_unfrozen in product(FROZEN_LRS, UNFROZEN_LRS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Configuration: Frozen LR={lr_frozen:.0e}, Unfrozen LR={lr_unfrozen:.0e}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultiLabelBERTClassification(\n",
    "        model_name=MODEL_NAME,\n",
    "        num_labels=len(labels),\n",
    "        class_frequencies=class_frequencies,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        use_db_loss=True,\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./methods_paper_files/runs/{MODEL_NAME.split('/')[-1]}_lr{lr_frozen:.0e}_lrU{lr_unfrozen:.0e}\",\n",
    "        \n",
    "        # Training schedule\n",
    "        num_train_epochs=10,\n",
    "        learning_rate=lr_frozen,  # Initial learning rate\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        \n",
    "        # Batch sizes\n",
    "        per_device_train_batch_size=2,    \n",
    "        per_device_eval_batch_size=32,    \n",
    "        gradient_accumulation_steps=8,     # Effective batch size = 16\n",
    "        \n",
    "        # Evaluation and checkpointing\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        greater_is_better=True,\n",
    "        \n",
    "        # Other settings\n",
    "        logging_steps=50,\n",
    "        seed=42,\n",
    "        fp16=False,  # Set True if using GPU with mixed precision support\n",
    "        dataloader_num_workers=0,\n",
    "        no_cuda=not torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    # Freezing schedule\n",
    "    n_frozen_epochs = 2    # Freeze for first 2 epochs\n",
    "    n_unfrozen_layers = 4  # Unfreeze top 4 layers\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = UltraConservativeTrainerWithSampler(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        n_frozen_epochs=n_frozen_epochs,\n",
    "        n_unfrozen_layers=n_unfrozen_layers,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=5,\n",
    "                early_stopping_threshold=0.001\n",
    "            ),\n",
    "            LRDropCallback(\n",
    "                epoch_drop=n_frozen_epochs,\n",
    "                new_lr=lr_unfrozen\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    \n",
    "    # Store results\n",
    "    results[(lr_frozen, lr_unfrozen)].append(eval_metrics[\"eval_macro_f1\"])\n",
    "    \n",
    "    print(f\"\\nValidation Macro F1: {eval_metrics['eval_macro_f1']:.4f}\")\n",
    "    print(f\"Training completed in {train_result.metrics['train_runtime']:.0f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74b9f298-3680-48e2-8eba-52d53bba72a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best configuration:\n",
      "  Frozen LR: 5e-05\n",
      "  Unfrozen LR: 2e-05\n",
      "  Validation Macro F1: 0.7608\n"
     ]
    }
   ],
   "source": [
    "# Find best configuration\n",
    "best_config = max(results.items(), key=lambda x: np.mean(x[1]))\n",
    "best_lr_frozen, best_lr_unfrozen = best_config[0]\n",
    "best_val_f1 = np.mean(best_config[1])\n",
    "\n",
    "print(f\"\\nBest configuration:\")\n",
    "print(f\"  Frozen LR: {best_lr_frozen:.0e}\")\n",
    "print(f\"  Unfrozen LR: {best_lr_unfrozen:.0e}\")\n",
    "print(f\"  Validation Macro F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa72d0-55aa-46b1-9327-7bf0ffce5dd1",
   "metadata": {},
   "source": [
    "# 9. Select Best Model and Final Evaluation\n",
    "\n",
    "Based on validation results, we select the best hyperparameters\n",
    "and evaluate on the test set with threshold tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "638da8bd-0dc5-415d-a721-fe874066e6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training final model with best configuration...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Distribution Balanced Loss\n",
      "ClassAwareSampler initialized:\n",
      "  - 38 classes with samples\n",
      "  - Batch size: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='774' max='774' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [774/774 7:04:02, Epoch 8/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.030600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LRDropCallback] Epoch 2: Set LR → 2.0e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training final model with best configuration...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Initialize final model\n",
    "final_model = MultiLabelBERTClassification(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    class_frequencies=class_frequencies,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    use_db_loss=True,\n",
    ")\n",
    "\n",
    "# Same training args but with best learning rates\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=f\".methods_paper_files/results/final_model_{MODEL_NAME.split('/')[-1]}\",\n",
    "    num_train_epochs=9,\n",
    "    learning_rate=best_lr_frozen,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=8,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_steps=50,\n",
    "    seed=42,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Final trainer\n",
    "final_trainer = UltraConservativeTrainerWithSampler(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    n_frozen_epochs=2,\n",
    "    n_unfrozen_layers=4,\n",
    "    callbacks=[\n",
    "        LRDropCallback(epoch_drop=2, new_lr=best_lr_unfrozen),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "final_train_result = final_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2de754-3c6a-417a-bd21-b97b413e24fd",
   "metadata": {},
   "source": [
    "# 10. Save and test the model on the test set\n",
    "\n",
    "Now we apply temperature scaling and evaluate on the test set. Temperature scaling is a post‑training trick that multiplies the model’s logits by 1 / T so the sigmoid probabilities become better calibrated. This shouldn't change F1 much, but will make the probabilities more useful for analysis.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b09057ec-8e14-4af7-8ef0-3d1190763f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal global temperature: T = 0.500\n",
      "Validation macro‑F1 after scaling: 0.7299\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST RESULTS (temperature‑scaled) ===\n",
      "Macro‑F1       : 0.7627\n",
      "Micro‑F1       : 0.8568\n",
      "Exact match %  : 18.42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# collect validation logits once\n",
    "val_predictions = final_trainer.predict(encoded_dataset[\"validation\"])\n",
    "val_logits  = val_predictions.predictions          # (n_val, n_labels)\n",
    "val_labels  = val_predictions.label_ids            # (n_val, n_labels)\n",
    "\n",
    "T_global = fit_temperature(val_logits, val_labels)\n",
    "print(f\"\\nOptimal global temperature: T = {T_global:.3f}\")\n",
    "\n",
    "# re‑evaluate on validation (sanity check)\n",
    "val_pred_mask, _ = make_hard_preds(val_logits, T_global)\n",
    "val_macro_f1 = f1_score(val_labels, val_pred_mask, average=\"macro\", zero_division=0)\n",
    "print(f\"Validation macro‑F1 after scaling: {val_macro_f1:.4f}\")\n",
    "\n",
    "# final evaluation on the test split\n",
    "test_predictions = final_trainer.predict(encoded_dataset[\"test\"])\n",
    "test_logits  = test_predictions.predictions\n",
    "test_labels  = test_predictions.label_ids\n",
    "\n",
    "test_pred_mask, test_probs = make_hard_preds(test_logits, T_global)\n",
    "\n",
    "macro_f1  = f1_score(test_labels, test_pred_mask, average=\"macro\", zero_division=0)\n",
    "micro_f1  = f1_score(test_labels, test_pred_mask, average=\"micro\", zero_division=0)\n",
    "exact_acc = ((test_pred_mask == test_labels).all(axis=1)).mean()\n",
    "\n",
    "print(\"\\n=== TEST RESULTS (temperature‑scaled) ===\")\n",
    "print(f\"Macro‑F1       : {macro_f1:.4f}\")\n",
    "print(f\"Micro‑F1       : {micro_f1:.4f}\")\n",
    "print(f\"Exact match %  : {exact_acc*100:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba10d979-a439-4ae3-82bb-892f148db7fb",
   "metadata": {},
   "source": [
    "USE THIS FOR 50% THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46f9ac80-74e7-4888-aa96-b31f6c9b82d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 79)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store for error analysis and model comparison somewhere else\n",
    "\n",
    "scaled_logits = test_logits / T_global                    # divide by fitted T\n",
    "probs = 1 / (1 + np.exp(-scaled_logits))\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "logit_df = pd.DataFrame(scaled_logits, columns = [f\"{i}_logit\" for i in cats])\n",
    "label_df = pd.DataFrame(preds, columns = [f\"{i}\" for i in cats])\n",
    "id_df = df.iloc[test_idx][[\"id\",\"paper_id\",\"displayed_text\"]].reset_index(drop=True)\n",
    "\n",
    "scibert_df = logit_df.join(label_df, how='inner')\n",
    "scibert_df = scibert_df.join(id_df, how='inner')\n",
    "\n",
    "\n",
    "with open(\"methods_paper_files/results/scibert_results_df_NO_SCALING.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scibert_df, f)\n",
    "\n",
    "\n",
    "scibert_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "875a89d6-9eb2-44c0-86ba-5c7de95329d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n",
      "  - Model weights: .methods_paper_files/results/final_scibert_model/final_model_scibert_scivocab_uncased/pytorch_model.bin\n",
      "  - Tokenizer: .methods_paper_files/results/final_scibert_model/final_model_scibert_scivocab_uncased/tokenizer_config.json\n",
      "  - Class frequencies: .methods_paper_files/results/final_scibert_model/final_model_scibert_scivocab_uncased\\class_frequencies.json\n"
     ]
    }
   ],
   "source": [
    "#save the final model for inference\n",
    "\n",
    "\n",
    "model_save_path = f\".methods_paper_files/results/final_scibert_model/final_model_{MODEL_NAME.split('/')[-1]}\"\n",
    "\n",
    "# dumps the *whole* model (incl. classification head) into model_save_path\n",
    "final_trainer.save_model(model_save_path)\n",
    "\n",
    "# one‑liner for the tokenizer if you really want it in the same dir\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "\n",
    "# Save class frequencies (needed for DB loss if loading model later)\n",
    "freq_dict = {label: int(freq) for label, freq in zip(labels, class_frequencies)}\n",
    "freq_path = os.path.join(model_save_path, \"class_frequencies.json\")\n",
    "with open(freq_path, \"w\") as f:\n",
    "    json.dump(freq_dict, f, indent=2)\n",
    "\n",
    "print(f\"Model saved successfully!\")\n",
    "print(f\"  - Model weights: {model_save_path}/pytorch_model.bin\")\n",
    "print(f\"  - Tokenizer: {model_save_path}/tokenizer_config.json\")\n",
    "print(f\"  - Class frequencies: {freq_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
