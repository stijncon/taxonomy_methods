{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869c74a9-c2c9-47eb-bba2-37263a2f73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score, hamming_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571e166-c7d8-4bf7-b348-0a57eb114049",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469060f7-8d4e-4478-a46c-cec600479a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load list with categories of interest\n",
    "filename = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\data\\categories_of_interest.txt\"\n",
    "with open(filename, 'r') as file:\n",
    "    cats = json.load(file)\n",
    "\n",
    "# load the dictionary to map category column names to gpt names\n",
    "filename = r'C:\\\\Users\\\\conix\\\\Dropbox\\\\FNRS project taxonomy\\\\methods in taxonomy\\\\data\\\\category_names_dict.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    category_names = json.load(file)\n",
    "\n",
    "# descriptions of the various categories, used in the first try with gpt4omini\n",
    "filename = r'C:\\\\Users\\\\conix\\\\Dropbox\\\\FNRS project taxonomy\\\\methods in taxonomy\\\\data\\\\methods_description_gpt4omini_first_try.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    methods_old = json.load(file)\n",
    "\n",
    "# descriptions of the various categories, used in the second try with gpt4omini\n",
    "filename = r'C:\\\\Users\\\\conix\\\\Dropbox\\\\FNRS project taxonomy\\\\methods in taxonomy\\\\data\\\\methods_description_gpt4omini_second_try.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    methods = json.load(file)\n",
    "\n",
    "# hierarchical classification of the categories\n",
    "filename = r\"C:\\Users\\conix\\Dropbox\\FNRS project taxonomy\\methods in taxonomy\\data\\classification_categories.txt\"\n",
    "with open(filename, \"r\") as file:\n",
    "    classif = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa6565a-d262-4889-8d21-34de071edb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from lr\n",
    "\n",
    "with open(\"methods_paper_files/upload_data_lr.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "df = data['og_data']\n",
    "test_idx = data['test data']['test_idx']\n",
    "y_test = data['test data']['y_test']\n",
    "X_test = data['test data']['X_test']\n",
    "\n",
    "# limit to test data\n",
    "df = df.iloc[test_idx]\n",
    "\n",
    "# just make sure the data is correct\n",
    "assert df[cats].equals(y_test)\n",
    "assert X_test.equals(df['displayed_text'])\n",
    "assert len(df) == len(df['id'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5eb97b-6185-4f13-b30d-1a6edcf0e3ba",
   "metadata": {},
   "source": [
    "# Estimate price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4baf4a34-e665-40d8-afd3-3d1500f6f698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price for 304 samples for 38 categories: 5.800101499999999\n"
     ]
    }
   ],
   "source": [
    "def estimate_price(input_cost, output_cost, num_par, tokens_par, tokens_query, cat):\n",
    "    \n",
    "    batch_discount = 0.5\n",
    "    cost = (((tokens_par + tokens_query) * cat * num_par * batch_discount) * input_cost +\n",
    "        (num_par * cat * 1 * output_cost))\n",
    "\n",
    "    print(f\"Price for {num_par} samples for {cat} categories: {cost}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_par = len(df)\n",
    "tokens_par = np.mean([len(i)/4 for i in X_test])\n",
    "tokens_query = 220 # see below for this value\n",
    "cat = len(cats)\n",
    "\n",
    "estimate_price(input_cost = 2 / 1e6, output_cost = 8 / 1e6, num_par = num_par, tokens_par = tokens_par, tokens_query = tokens_query, cat = cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e5d871-460b-4f4b-84ad-db387c4ba886",
   "metadata": {},
   "source": [
    "# set up openAI API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df1d0b61-25c5-4d2c-b546-bdd268dbdf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key linked to our account\n",
    "\n",
    "key = 'YOUR_API_KEY_HERE'\n",
    "client = OpenAI(api_key = key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe933c-5c39-4c9c-bd64-bfec950dbf27",
   "metadata": {},
   "source": [
    "# Cheap base model: gpt 4.1-mini with the batch api\n",
    "\n",
    "We'll use the batch API to get a 50% discount. See [here](https://cookbook.openai.com/examples/batch_processing). Due to rate limits, we have to do this in multiple batches. Check the OpenAI website for this -- rate limits depend on usage tier and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0333a15-40b8-4a74-af47-458a402e2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split up the annotated paragraphs into batches.\n",
    "\n",
    "def create_task(row_id, cat, para, system_prompt):\n",
    "    \"\"\"Create a single task dictionary for the given chunk.\"\"\"\n",
    "    custom_id = f\"{row_id}___{cat}\"\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4.1-mini\",\n",
    "            \"temperature\": 0,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": para}\n",
    "            ],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9c692-783b-4b9a-a8de-5ec49ec9445c",
   "metadata": {},
   "source": [
    "## Create the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96f2dc15-9d99-49f8-b089-8627bca86997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "304it [00:04, 65.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# dictionary that stores all the batches\n",
    "tasks = {}\n",
    "\n",
    "# list that stores the requests within each batch\n",
    "# there is one request for each category for each annotated paragraph\n",
    "tasks_list = []\n",
    "\n",
    "batch_count = 0\n",
    "\n",
    "for _, row in tqdm(df[['id', 'displayed_text']].iterrows()):\n",
    "\n",
    "    # the user prompt is the concrete material that the general task has to be applied to\n",
    "    # in our case the annotated paragraph\n",
    "    para = row[\"displayed_text\"]\n",
    "    for cat, descr in methods.items():\n",
    "\n",
    "        # the system prompt gives general instructions\n",
    "        # in this case: the category name and description as well as the output we want\n",
    "        system_prompt = f'''Imagine you are a professional taxonomist. Your goal is to judge whether a paragraph from the methods section of a taxonomic research paper discusses {cat}. {descr} You will be provided with a paragraph, and you will output a \"1\" if yes, and a \"0\" if not. This is the only output needed. Note, the method may be mentioned either directly or implied by the content. However, please make sure you are conservative. That is, only output \"1\" if you are very sure that this method was applied. If the paragraph is consistent with the method not having been used, please output \"0\".'''\n",
    "\n",
    "        \n",
    "\n",
    "        task = create_task(row_id=row['id'], cat=cat, para=para, system_prompt=system_prompt)\n",
    "        tasks_list.append(task)\n",
    "        \n",
    "        # Check if our accumulated tasks exceeds the 2 million tokens threshold.\n",
    "        # this is also always lower than the 50 000 requests allowed per batch\n",
    "        total_length = sum(len(t['body']['messages'][0]['content']) + len(t['body']['messages'][1]['content'])\n",
    "                           for t in tasks_list)\n",
    "\n",
    "        # assume 4 characters per token on average, and keep some spare room below 2 million the be sure\n",
    "        if ((total_length / 4) > 1850000):\n",
    "            tasks[f\"batch_{batch_count}\"] = tasks_list.copy()\n",
    "            tasks_list = []\n",
    "            batch_count += 1\n",
    "\n",
    "# Save any remaining tasks.\n",
    "if tasks_list:\n",
    "    tasks[f\"batch_{batch_count}\"] = tasks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7b3db82-d71b-42b8-b846-32ce8e32d8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11552\n",
      "All batches within 2 million tokens\n",
      "All tasks have unique custom_ids\n"
     ]
    }
   ],
   "source": [
    "# print total number of requests\n",
    "print(sum(len(v) for v in tasks.values()))\n",
    "\n",
    "# check if all batches are within two million tokens\n",
    "problems = []\n",
    "\n",
    "for key, batch in tasks.items():\n",
    "    tokens = np.array([\n",
    "        len(i['body']['messages'][0]['content']) + len(i['body']['messages'][1]['content'])\n",
    "        for i in batch\n",
    "    ])\n",
    "    total_tokens = tokens.sum() / 4\n",
    "    if total_tokens > 2000000:\n",
    "        problems.append(key)\n",
    "\n",
    "assert not problems, f\"Batches {problems} exceed 2 million tokens.\"\n",
    "print(\"All batches within 2 million tokens\")\n",
    "\n",
    "# check if all custom ids are unique, otherwise the api throws an error\n",
    "problems = []\n",
    "\n",
    "for task in tasks.keys():\n",
    "    custom_ids = [i['custom_id'] for i in tasks[task]]\n",
    "    if len(custom_ids) != len(set(custom_ids)):\n",
    "        problems.append(task)\n",
    "\n",
    "assert not problems, f\"Duplicate custom_ids found in tasks: {problems}\"\n",
    "print(\"All tasks have unique custom_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19dd6ec2-4beb-4f9b-b2de-068f359dacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the tasks to json files to upload to the batch api\n",
    "\n",
    "for batch in tasks.keys():\n",
    "    file_name = f'methods_paper_files/batches/batch_tasks_taxonomy_41mini_{batch}.jsonl'\n",
    "\n",
    "    with open(file_name, 'w') as file:\n",
    "        for obj in tasks[batch]:\n",
    "            file.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a550b62-3ba3-45d7-b201-a96940b971da",
   "metadata": {},
   "source": [
    "## Make the API calls and collect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c2953d2-959f-415e-abfe-35d9cbb06caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started:\n",
      "Batch(id='batch_6877a44b46248190856448c3b2810add', completion_window='24h', created_at=1752671307, endpoint='/v1/chat/completions', input_file_id='file-AsAiGRU78HhpcsGyEenEnJ', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1752757707, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "completed:\n",
      "Batch(id='batch_6877a44b46248190856448c3b2810add', completion_window='24h', created_at=1752671307, endpoint='/v1/chat/completions', input_file_id='file-AsAiGRU78HhpcsGyEenEnJ', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1752672921, error_file_id=None, errors=None, expired_at=None, expires_at=1752757707, failed_at=None, finalizing_at=1752672454, in_progress_at=1752671310, metadata=None, output_file_id='file-9cjJrXXsRwKb7TRqoPYLXB', request_counts=BatchRequestCounts(completed=3372, failed=0, total=3372))\n",
      "saved!\n",
      "---\n",
      "started:\n",
      "Batch(id='batch_6877aafdbbbc8190b43089329678debb', completion_window='24h', created_at=1752673021, endpoint='/v1/chat/completions', input_file_id='file-K1sud5MySVGt7REGDgbavu', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1752759421, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "completed:\n",
      "Batch(id='batch_6877aafdbbbc8190b43089329678debb', completion_window='24h', created_at=1752673021, endpoint='/v1/chat/completions', input_file_id='file-K1sud5MySVGt7REGDgbavu', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1752674782, error_file_id=None, errors=None, expired_at=None, expires_at=1752759421, failed_at=None, finalizing_at=1752674408, in_progress_at=1752673025, metadata=None, output_file_id='file-56JMzSWuVd4N3MQp9dvk8q', request_counts=BatchRequestCounts(completed=3809, failed=0, total=3809))\n",
      "saved!\n",
      "---\n",
      "started:\n",
      "Batch(id='batch_6877b224cac88190b187c860419c2900', completion_window='24h', created_at=1752674852, endpoint='/v1/chat/completions', input_file_id='file-JwKK6E7XCkesDbmbg4xefY', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1752761252, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "completed:\n",
      "Batch(id='batch_6877b224cac88190b187c860419c2900', completion_window='24h', created_at=1752674852, endpoint='/v1/chat/completions', input_file_id='file-JwKK6E7XCkesDbmbg4xefY', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1752677956, error_file_id=None, errors=None, expired_at=None, expires_at=1752761252, failed_at=None, finalizing_at=1752677383, in_progress_at=1752674856, metadata=None, output_file_id='file-5RJeQC2YtaKfMuTt3PgTq9', request_counts=BatchRequestCounts(completed=4031, failed=0, total=4031))\n",
      "saved!\n",
      "---\n",
      "started:\n",
      "Batch(id='batch_6877be642ad48190aad472af5ea8e0ab', completion_window='24h', created_at=1752677988, endpoint='/v1/chat/completions', input_file_id='file-DCQXxTV9AUJmKyWasiU9Pq', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1752764388, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "completed:\n",
      "Batch(id='batch_6877be642ad48190aad472af5ea8e0ab', completion_window='24h', created_at=1752677988, endpoint='/v1/chat/completions', input_file_id='file-DCQXxTV9AUJmKyWasiU9Pq', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1752680364, error_file_id=None, errors=None, expired_at=None, expires_at=1752764388, failed_at=None, finalizing_at=1752680281, in_progress_at=1752677990, metadata=None, output_file_id='file-C7fJBSCrXu33nQBEzD2BKJ', request_counts=BatchRequestCounts(completed=340, failed=0, total=340))\n",
      "saved!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# loop through batches, and save the results as they are returned and as a df\n",
    "\n",
    "batches = list(tasks.keys())\n",
    "\n",
    "for batch in batches:  \n",
    "    filename = f'methods_paper_files/batches/batch_tasks_taxonomy_41mini_{batch}.jsonl'\n",
    "\n",
    "    # upload the file\n",
    "    with open(filename, \"rb\") as f:\n",
    "        batch_file = client.files.create(\n",
    "            file=f,\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "\n",
    "    # start the job\n",
    "    batch_job = client.batches.create(\n",
    "        input_file_id=batch_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "\n",
    "    # get the id to check progress\n",
    "    job_id = batch_job.id \n",
    "    status = client.batches.retrieve(job_id).status\n",
    "    print('started:')\n",
    "    print(client.batches.retrieve(job_id))\n",
    "    print('')\n",
    "    \n",
    "    # Wait until the batch job is finished.\n",
    "    while status in ('in_progress', 'finalizing', 'validating'):\n",
    "        time.sleep(120)\n",
    "        status = client.batches.retrieve(job_id).status\n",
    "\n",
    "    # notify if the hob is completed, and get the result\n",
    "    if status == 'completed':\n",
    "        print('completed:')\n",
    "        print(client.batches.retrieve(job_id))\n",
    "        result_file_id = client.batches.retrieve(job_id).output_file_id\n",
    "        result = client.files.content(result_file_id).content\n",
    "\n",
    "    # indicate if there was an error for a batch\n",
    "    # typically because of rate limits\n",
    "    else:\n",
    "        # Optionally handle other statuses or errors\n",
    "        print('status was:')\n",
    "        print(status)\n",
    "        print(batch)\n",
    "        print(client.batches.retrieve(job_id))\n",
    "        print('---')\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    # Write the result to a file.\n",
    "    result_file_name = f'methods_paper_files/results/batch_tasks_taxonomy_41mini_{batch}.jsonl'\n",
    "    with open(result_file_name, 'wb') as file:\n",
    "        file.write(result)\n",
    "\n",
    "    # Read and parse the JSON lines of the result\n",
    "    results = []\n",
    "    with open(result_file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            results.append(json_object)\n",
    "\n",
    "    # for each paragraph, make a dict entry with a dict for each category\n",
    "    para_ids = list(set([i['custom_id'].split('___')[0] for i in tasks[batch]]))\n",
    "    result_dct = {para_id:{method:0 for method in methods.keys()} for para_id in para_ids}\n",
    "    \n",
    "    # store the results in the dictionaries\n",
    "    for i in results:\n",
    "        cat = i['custom_id'].split('___')[1]\n",
    "        doc_id = i['custom_id'].split('___')[0]\n",
    "        responded_value = int(i['response']['body']['choices'][0]['message']['content'])\n",
    "        if responded_value != 0:\n",
    "            result_dct[doc_id][cat] = responded_value\n",
    "\n",
    "    # Save to a pickle file.\n",
    "    pickle_filename = f'methods_paper_files/results/results_41mini_{batch}.pkl'\n",
    "    with open(pickle_filename, \"wb\") as file:\n",
    "        pickle.dump(result_dct, file)\n",
    "    print('saved!')\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec36c93-6e4f-4987-b699-c20930920f7d",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b79cb71b-5c31-4cd2-b388-838fc821519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = {}\n",
    "\n",
    "count = 0\n",
    "for batch in batches:\n",
    "    with open(f\"methods_paper_files/results/results_41mini_{batch}.pkl\", \"rb\") as file:\n",
    "        dct = pickle.load(file)\n",
    "        count += len(dct)\n",
    "        all_batches.update(dct)\n",
    "\n",
    "\n",
    "# store the results\n",
    "df_results = pd.DataFrame(all_batches).T.astype('int').sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e9bfad8-98a2-43d4-ab33-04caedbef772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classification is hierarchical and leaf categories should have all parent categories\n",
    "# update the gpt results accordingly\n",
    "\n",
    "def recursive_update(df, classif):\n",
    "    # Go through each item in the classification\n",
    "    for key, value in classif.items():\n",
    "        if isinstance(value, list):\n",
    "            # First process subcategories (go deeper into the hierarchy)\n",
    "            for v in value:\n",
    "                if isinstance(v, dict):\n",
    "                    # Recursively update subcategories first\n",
    "                    recursive_update(df, v)\n",
    "            \n",
    "            # After processing subcategories, update the current category\n",
    "            sub_categories = [v if isinstance(v, str) else list(v.keys())[0] for v in value]\n",
    "            df.loc[df[sub_categories].eq(1).any(axis=1), key] = 1\n",
    "    \n",
    "    # Return the DataFrame to allow for chaining if needed\n",
    "    return df\n",
    "\n",
    "# for that, we have to update the category names in the classification dictionary\n",
    "def replace_terms(obj, mapping):\n",
    "    \"\"\"\n",
    "    Recursively replaces strings in a nested structure (dicts/lists)\n",
    "    using the provided mapping dictionary.\n",
    "    \"\"\"\n",
    "    # If the object is a dictionary, iterate over its keys and values.\n",
    "    if isinstance(obj, dict):\n",
    "        new_dict = {}\n",
    "        for key, value in obj.items():\n",
    "            # Replace key if it's a string.\n",
    "            new_key = mapping.get(key, key) if isinstance(key, str) else key\n",
    "            # Recursively replace strings in the value.\n",
    "            new_dict[new_key] = replace_terms(value, mapping)\n",
    "        return new_dict\n",
    "    \n",
    "    # If the object is a list, process each item.\n",
    "    elif isinstance(obj, list):\n",
    "        return [replace_terms(item, mapping) for item in obj]\n",
    "    \n",
    "    # If the object is a string, replace it if it exists in the mapping.\n",
    "    elif isinstance(obj, str):\n",
    "        return mapping.get(obj, obj)\n",
    "    \n",
    "    # For other types, return the object unchanged.\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# update using the classification\n",
    "classif = replace_terms(classif, category_names)\n",
    "df_results = recursive_update(df_results, classif)\n",
    "\n",
    "# update categories that are not hierarchically related\n",
    "df_results['interbreeding or reproduction'] = np.where(df_results['reproductive morphology'] == 1, 1, df_results['interbreeding or reproduction'])\n",
    "df_results['phylogenetic methods'] = np.where(((df_results['phylogenetic species delimitation'] == 1) | (df_results['phylogenetic tree reconstruction methods'] == 1) | (df_results['distance based phylogenetic tree inference'] == 1) | (df_results['character based phylogenetic tree inference'] == 1) | (df_results['phylogenetic analysis and reasoning with phenotypic data'] == 1)), 1, df_results['phylogenetic methods'])\n",
    "\n",
    "# drop singletons, as that's a meaningless hierarchy level\n",
    "df_results = df_results.drop('singletons', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5dda2edb-44df-4492-8459-6bcc920be870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to analyze them in a different notebook\n",
    "\n",
    "\n",
    "with open(\"methods_paper_files/results/gpt_41mini_results_df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3fd6a-1f25-44c6-b0c0-b8859a434d8e",
   "metadata": {},
   "source": [
    "# Expensive reasoning model: o3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691f756-0b91-41db-bcea-1d966affbd8b",
   "metadata": {},
   "source": [
    "## Get the data through the api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e767f41-796f-47d4-bab2-e61b6bf40880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split up the annotated paragraphs into batches.\n",
    "# had to remove temperature and add max completion tokens\n",
    "def create_task(row_id, cat, para, system_prompt):\n",
    "    \"\"\"Create a single task dictionary for the given chunk.\"\"\"\n",
    "    custom_id = f\"{row_id}___{cat}\"\n",
    "    return {\n",
    "        \"custom_id\": custom_id,\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4.1-2025-04-14\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": para}\n",
    "            ],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae33fc0-3f8a-41ad-8f68-57ced5099b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "enc                 = tiktoken.encoding_for_model(\"o3\")   # needs tiktoken ≥0.9.0\n",
    "TOKENS_PER_MESSAGE  = 3    # chat markup  (<|role|>, etc.)\n",
    "ASSISTANT_PRIMER    = 3    # final <|assistant|> header\n",
    "COMPLETION_TOKENS   = 2    # you only want “0”/“1”\n",
    "MAX_BATCH_TOKENS    = 85000  # org‑level hard limit for o‑family\n",
    "\n",
    "def cost_in_tokens(task: dict) -> int:\n",
    "    \"\"\"Exact token cost of one chat request, incl. output budget.\"\"\"\n",
    "    msgs = task[\"body\"][\"messages\"]\n",
    "    prompt = sum(len(enc.encode(m[\"content\"])) for m in msgs)\n",
    "    overhead = TOKENS_PER_MESSAGE * len(msgs) + ASSISTANT_PRIMER\n",
    "    return prompt + overhead + COMPLETION_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c709976c-93d6-4e23-b587-d09f3273cb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "304it [00:03, 82.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# dictionary that stores all the batches\n",
    "tasks = {}\n",
    "\n",
    "# list that stores the requests within each batch\n",
    "# there is one request for each category for each annotated paragraph\n",
    "tasks_list = []\n",
    "\n",
    "used_tokens  = 0\n",
    "batch_count  = 0\n",
    "\n",
    "for _, row in tqdm(df[['id', 'displayed_text']].iterrows()):\n",
    "    para = row[\"displayed_text\"]\n",
    "    for cat, descr in methods.items():\n",
    "\n",
    "        system_prompt = (\n",
    "            f\"Imagine you are a professional taxonomist. Your goal is to judge whether a paragraph \"\n",
    "            f\"from the methods section of a taxonomic research paper discusses {cat}. {descr} \"\n",
    "            'You will be provided with a paragraph, and you will output a \"1\" if yes, and a \"0\" if not. '\n",
    "            \"This is the only output needed. Note, the method may be mentioned either directly or implied by \"\n",
    "            \"the content. However, please make sure you are conservative. That is, only output \\\"1\\\" if you are \"\n",
    "            \"very sure that this method was applied. If the paragraph is consistent with the method not having \"\n",
    "            \"been used, please output \\\"0\\\".\"\n",
    "        )\n",
    "\n",
    "        task = create_task(\n",
    "            row_id=row['id'],\n",
    "            cat=cat,\n",
    "            para=para,\n",
    "            system_prompt=system_prompt\n",
    "        )\n",
    "\n",
    "        t_cost = cost_in_tokens(task)\n",
    "\n",
    "        # flush if adding this task would blow the 90 k quota\n",
    "        if used_tokens + t_cost > MAX_BATCH_TOKENS:\n",
    "            tasks[f\"batch_{batch_count}\"] = tasks_list\n",
    "            tasks_list   = []\n",
    "            used_tokens  = 0\n",
    "            batch_count += 1\n",
    "\n",
    "        tasks_list.append(task)\n",
    "        used_tokens += t_cost\n",
    "\n",
    "# Save any remaining tasks.\n",
    "if tasks_list:\n",
    "    tasks[f\"batch_{batch_count}\"] = tasks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f8433a5-2149-4406-bc02-62ac4f9c7847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11552\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Batches ['batch_0', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6', 'batch_7', 'batch_8', 'batch_9', 'batch_10', 'batch_11', 'batch_13', 'batch_14', 'batch_15', 'batch_16', 'batch_17', 'batch_18', 'batch_21', 'batch_23', 'batch_24', 'batch_25', 'batch_26', 'batch_27', 'batch_28', 'batch_29', 'batch_30', 'batch_31', 'batch_33', 'batch_35', 'batch_36', 'batch_37', 'batch_38', 'batch_39', 'batch_40', 'batch_41', 'batch_42', 'batch_43', 'batch_45', 'batch_47', 'batch_48', 'batch_49', 'batch_50', 'batch_51', 'batch_52', 'batch_53', 'batch_54', 'batch_55', 'batch_56', 'batch_57', 'batch_59', 'batch_60', 'batch_61'] exceed 90 k tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m90000\u001b[39m:\n\u001b[0;32m     14\u001b[0m         problems\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m problems, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblems\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exceed 90 k tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll batches within the token limit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# check if all custom ids are unique, otherwise the api throws an error\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Batches ['batch_0', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6', 'batch_7', 'batch_8', 'batch_9', 'batch_10', 'batch_11', 'batch_13', 'batch_14', 'batch_15', 'batch_16', 'batch_17', 'batch_18', 'batch_21', 'batch_23', 'batch_24', 'batch_25', 'batch_26', 'batch_27', 'batch_28', 'batch_29', 'batch_30', 'batch_31', 'batch_33', 'batch_35', 'batch_36', 'batch_37', 'batch_38', 'batch_39', 'batch_40', 'batch_41', 'batch_42', 'batch_43', 'batch_45', 'batch_47', 'batch_48', 'batch_49', 'batch_50', 'batch_51', 'batch_52', 'batch_53', 'batch_54', 'batch_55', 'batch_56', 'batch_57', 'batch_59', 'batch_60', 'batch_61'] exceed 90 k tokens."
     ]
    }
   ],
   "source": [
    "# print total number of requests\n",
    "print(sum(len(v) for v in tasks.values()))\n",
    "\n",
    "# check if all batches are within two million tokens\n",
    "problems = []\n",
    "\n",
    "for key, batch in tasks.items():\n",
    "    tokens = np.array([\n",
    "        len(i['body']['messages'][0]['content']) + len(i['body']['messages'][1]['content'])\n",
    "        for i in batch\n",
    "    ])\n",
    "    total_tokens = tokens.sum() / 4\n",
    "    if total_tokens > 90000:\n",
    "        problems.append(key)\n",
    "\n",
    "assert not problems, f\"Batches {problems} exceed 90 k tokens.\"\n",
    "print(\"All batches within the token limit\")\n",
    "\n",
    "# check if all custom ids are unique, otherwise the api throws an error\n",
    "problems = []\n",
    "\n",
    "for task in tasks.keys():\n",
    "    custom_ids = [i['custom_id'] for i in tasks[task]]\n",
    "    if len(custom_ids) != len(set(custom_ids)):\n",
    "        problems.append(task)\n",
    "\n",
    "assert not problems, f\"Duplicate custom_ids found in tasks: {problems}\"\n",
    "print(\"All tasks have unique custom_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26bbc180-d40e-48b5-9d91-7cb1732a7f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "# write the tasks to json files to upload to the batch api\n",
    "\n",
    "for batch in tasks.keys():\n",
    "    file_name = f'methods_paper_files/batches/batch_tasks_taxonomy_41_{batch}.jsonl'\n",
    "\n",
    "    with open(file_name, 'w') as file:\n",
    "        for obj in tasks[batch]:\n",
    "            file.write(json.dumps(obj) + '\\n')\n",
    "\n",
    "print(len(tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5d653-a595-4c6d-b93c-d74e15ea3b00",
   "metadata": {},
   "source": [
    "## Make the api calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c9c3a58-5c01-4e77-bf68-7ba5f48a2aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started:\n",
      "Batch(id='batch_687bc736e7308190bb55b7ed5c692ff8', completion_window='24h', created_at=1752942390, endpoint='/v1/chat/completions', input_file_id='file-RMpSi8WjHXAKgadhTYbmYF', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1753028790, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "status was:\n",
      "failed\n",
      "batch_0\n",
      "Batch(id='batch_687bc736e7308190bb55b7ed5c692ff8', completion_window='24h', created_at=1752942390, endpoint='/v1/chat/completions', input_file_id='file-RMpSi8WjHXAKgadhTYbmYF', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4.1-2025-04-14 in organization org-H9yaeVFAhw5Mg1OR4p0HYUUd. Limit: 900,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1753028790, failed_at=1752942392, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "---\n",
      "started:\n",
      "Batch(id='batch_687bc7b18640819082aa361f78bf8d15', completion_window='24h', created_at=1752942513, endpoint='/v1/chat/completions', input_file_id='file-LnebKgr4CCrC6Fie3JFyYE', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1753028913, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "status was:\n",
      "failed\n",
      "batch_0\n",
      "Batch(id='batch_687bc7b18640819082aa361f78bf8d15', completion_window='24h', created_at=1752942513, endpoint='/v1/chat/completions', input_file_id='file-LnebKgr4CCrC6Fie3JFyYE', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4.1-2025-04-14 in organization org-H9yaeVFAhw5Mg1OR4p0HYUUd. Limit: 900,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1753028913, failed_at=1752942515, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "---\n",
      "started:\n",
      "Batch(id='batch_687bc82bf8088190a1bc7f4afcf99336', completion_window='24h', created_at=1752942635, endpoint='/v1/chat/completions', input_file_id='file-UAu8G6warZXAdsLZz2Z834', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1753029035, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "status was:\n",
      "failed\n",
      "batch_0\n",
      "Batch(id='batch_687bc82bf8088190a1bc7f4afcf99336', completion_window='24h', created_at=1752942635, endpoint='/v1/chat/completions', input_file_id='file-UAu8G6warZXAdsLZz2Z834', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4.1-2025-04-14 in organization org-H9yaeVFAhw5Mg1OR4p0HYUUd. Limit: 900,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1753029035, failed_at=1752942636, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "---\n",
      "started:\n",
      "Batch(id='batch_687bc8a6aae4819081aa199b9484c032', completion_window='24h', created_at=1752942758, endpoint='/v1/chat/completions', input_file_id='file-W1FCzq2vhMAQzEmdh5p5fP', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1753029158, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "status was:\n",
      "failed\n",
      "batch_0\n",
      "Batch(id='batch_687bc8a6aae4819081aa199b9484c032', completion_window='24h', created_at=1752942758, endpoint='/v1/chat/completions', input_file_id='file-W1FCzq2vhMAQzEmdh5p5fP', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4.1-2025-04-14 in organization org-H9yaeVFAhw5Mg1OR4p0HYUUd. Limit: 900,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1753029158, failed_at=1752942759, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "---\n",
      "started:\n",
      "Batch(id='batch_687bc9214f088190a782778ed583f209', completion_window='24h', created_at=1752942881, endpoint='/v1/chat/completions', input_file_id='file-KJhKxGTAeH3ZisyQJp3cko', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1753029281, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "status was:\n",
      "failed\n",
      "batch_0\n",
      "Batch(id='batch_687bc9214f088190a782778ed583f209', completion_window='24h', created_at=1752942881, endpoint='/v1/chat/completions', input_file_id='file-KJhKxGTAeH3ZisyQJp3cko', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4.1-2025-04-14 in organization org-H9yaeVFAhw5Mg1OR4p0HYUUd. Limit: 900,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1753029281, failed_at=1752942882, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "---\n",
      "started:\n",
      "Batch(id='batch_687bc99c145c819088f2031699afdf14', completion_window='24h', created_at=1752943004, endpoint='/v1/chat/completions', input_file_id='file-JzJhp3YzNpjyc9ZzKPtUex', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1753029404, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "status was:\n",
      "failed\n",
      "batch_0\n",
      "Batch(id='batch_687bc99c145c819088f2031699afdf14', completion_window='24h', created_at=1752943004, endpoint='/v1/chat/completions', input_file_id='file-JzJhp3YzNpjyc9ZzKPtUex', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4.1-2025-04-14 in organization org-H9yaeVFAhw5Mg1OR4p0HYUUd. Limit: 900,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1753029404, failed_at=1752943005, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "---\n",
      "started:\n",
      "Batch(id='batch_687bca17539081908005ab13a400e35b', completion_window='24h', created_at=1752943127, endpoint='/v1/chat/completions', input_file_id='file-8scVkpduHTvu8R3rsEtzKU', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1753029527, failed_at=None, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n",
      "\n",
      "completed:\n",
      "Batch(id='batch_687bca17539081908005ab13a400e35b', completion_window='24h', created_at=1752943127, endpoint='/v1/chat/completions', input_file_id='file-8scVkpduHTvu8R3rsEtzKU', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1752943250, error_file_id=None, errors=None, expired_at=None, expires_at=1753029527, failed_at=None, finalizing_at=1752943220, in_progress_at=1752943128, metadata=None, output_file_id='file-LHzZvnQyaUmCQZv2v5Aqq2', request_counts=BatchRequestCounts(completed=237, failed=0, total=237))\n",
      "saved!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# loop through batches, and save the results as they are returned and as a df\n",
    "\n",
    "batches = list(tasks.keys())\n",
    "for batch in batches:\n",
    "\n",
    "    filename = f'methods_paper_files/batches/batch_tasks_taxonomy_41_{batch}.jsonl'\n",
    "\n",
    "    # upload the file\n",
    "    with open(filename, \"rb\") as f:\n",
    "        batch_file = client.files.create(\n",
    "            file=f,\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "\n",
    "    # start the job\n",
    "    batch_job = client.batches.create(\n",
    "        input_file_id=batch_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "\n",
    "    # get the id to check progress\n",
    "    job_id = batch_job.id \n",
    "    status = client.batches.retrieve(job_id).status\n",
    "    print('started:')\n",
    "    print(client.batches.retrieve(job_id))\n",
    "    print('')\n",
    "    \n",
    "    # Wait until the batch job is finished.\n",
    "    while status in ('in_progress', 'finalizing', 'validating'):\n",
    "        time.sleep(120)\n",
    "        status = client.batches.retrieve(job_id).status\n",
    "\n",
    "    # notify if the job is completed, and get the result\n",
    "    if status == 'completed':\n",
    "        print('completed:')\n",
    "        print(client.batches.retrieve(job_id))\n",
    "        result_file_id = client.batches.retrieve(job_id).output_file_id\n",
    "        result = client.files.content(result_file_id).content\n",
    "        done.append(batch)\n",
    "\n",
    "    # indicate if there was an error for a batch\n",
    "    # typically because of rate limits\n",
    "    else:\n",
    "        # Optionally handle other statuses or errors\n",
    "        print('status was:')\n",
    "        print(status)\n",
    "        print(batch)\n",
    "        print(client.batches.retrieve(job_id))\n",
    "        print('---')\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    # Write the result to a file.\n",
    "    result_file_name = f'methods_paper_files/results/batch_tasks_taxonomy_41_{batch}.jsonl'\n",
    "    with open(result_file_name, 'wb') as file:\n",
    "        file.write(result)\n",
    "\n",
    "    # Read and parse the JSON lines of the result\n",
    "    results = []\n",
    "    with open(result_file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            results.append(json_object)\n",
    "\n",
    "    # for each paragraph, make a dict entry with a dict for each category\n",
    "    para_ids = list(set([i['custom_id'].split('___')[0] for i in tasks[batch]]))\n",
    "    result_dct = {para_id:{method:0 for method in methods.keys()} for para_id in para_ids}\n",
    "    \n",
    "    # store the results in the dictionaries\n",
    "    for i in results:\n",
    "        cat = i['custom_id'].split('___')[1]\n",
    "        doc_id = i['custom_id'].split('___')[0]\n",
    "        responded_value = int(i['response']['body']['choices'][0]['message']['content'])\n",
    "        if responded_value != 0:\n",
    "            result_dct[doc_id][cat] = responded_value\n",
    "\n",
    "    # Save to a pickle file.\n",
    "    pickle_filename = f'methods_paper_files/results/results_41_{batch}.pkl'\n",
    "    with open(pickle_filename, \"wb\") as file:\n",
    "        pickle.dump(result_dct, file)\n",
    "    print('saved!')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33038782-59d3-4d75-b24d-7f94218d0b83",
   "metadata": {},
   "source": [
    "## save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "381311f7-1e7c-43fc-abbd-7e531eefae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = {}\n",
    "\n",
    "count = 0\n",
    "for batch in batches:\n",
    "    with open(f\"methods_paper_files/results/results_41_{batch}.pkl\", \"rb\") as file:\n",
    "        dct = pickle.load(file)\n",
    "        count += len(dct)\n",
    "        all_batches.update(dct)\n",
    "\n",
    "\n",
    "# store the results\n",
    "df_results = pd.DataFrame(all_batches).T.astype('int').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82dd4f9a-f5e3-48c8-8c43-382ace9d2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to analyze them in a different notebook\n",
    "\n",
    "# update using the classification\n",
    "classif = replace_terms(classif, category_names)\n",
    "df_results = recursive_update(df_results, classif)\n",
    "\n",
    "# update categories that are not hierarchically related\n",
    "df_results['interbreeding or reproduction'] = np.where(df_results['reproductive morphology'] == 1, 1, df_results['interbreeding or reproduction'])\n",
    "df_results['phylogenetic methods'] = np.where(((df_results['phylogenetic species delimitation'] == 1) | (df_results['phylogenetic tree reconstruction methods'] == 1) | (df_results['distance based phylogenetic tree inference'] == 1) | (df_results['character based phylogenetic tree inference'] == 1) | (df_results['phylogenetic analysis and reasoning with phenotypic data'] == 1)), 1, df_results['phylogenetic methods'])\n",
    "\n",
    "# drop singletons, as that's a meaningless hierarchy level\n",
    "df_results = df_results.drop('singletons', axis=1)\n",
    "\n",
    "with open(\"methods_paper_files/results/gpt_41_results_df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
